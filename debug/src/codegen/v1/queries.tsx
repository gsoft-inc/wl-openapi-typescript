/** This file has been generated by @workleap/create-schemas (https://github.com/gsoft-inc/wl-openapi-typescript). Do not modify manually. */
import { createContext, useContext } from "react";
import { useQuery, useSuspenseQuery, useMutation, type UseQueryResult, type UseQueryOptions, type UseSuspenseQueryResult, type UseMutationResult, type UseMutationOptions, type QueryKey, type QueryClient, type FetchQueryOptions, type UseSuspenseQueryOptions } from "@tanstack/react-query";
import { OpenAPIClient, internal_fetch } from "@workleap/create-schemas/plugins/client-plugin/base-client";

const OpenAIAPIContext = createContext(new OpenAPIClient());

export function OpenAIAPIContextProvider({ children, client }: {
    children: React.ReactNode;
    client: OpenAPIClient;
}) {
    return <OpenAIAPIContext.Provider value={client}>{children}</OpenAIAPIContext.Provider>;
}

export interface CreateChatCompletionInit {
    body: CreateChatCompletionRequest;
    request?: RequestInit;
}

function createChatCompletion(client: OpenAPIClient, init: CreateChatCompletionInit): Promise<CreateChatCompletionResponse> {
    return client[internal_fetch]("POST", "/chat/completions", init, "application/json");
}

export function createChatCompletionQueryKey(init: CreateChatCompletionInit): QueryKey {
    return ["post", "/chat/completions", init];
}

export interface PrefetchCreateChatCompletionOptions extends CreateChatCompletionInit {
    queryOptions?: Partial<FetchQueryOptions<CreateChatCompletionResponse, unknown>>;
}

export function prefetchCreateChatCompletion(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateChatCompletionOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createChatCompletionQueryKey(init),
        queryFn: () => createChatCompletion(client, init),
        ...queryOptions
    });
}

export interface UseCreateChatCompletionQueryOptions extends CreateChatCompletionInit {
    queryOptions?: Partial<UseQueryOptions<CreateChatCompletionResponse, unknown>>;
}

/**
 * `POST /chat/completions`
 *
 * Creates a model response for the given chat conversation.
 */
export function useCreateChatCompletionQuery(options: UseCreateChatCompletionQueryOptions): UseQueryResult<CreateChatCompletionResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createChatCompletionQueryKey(init),
        queryFn: () => createChatCompletion(client, init),
        ...queryOptions
    });
}

export interface UseCreateChatCompletionSuspenseQueryOptions extends CreateChatCompletionInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<CreateChatCompletionResponse, unknown>>;
}

/**
 * `POST /chat/completions`
 *
 * Creates a model response for the given chat conversation.
 */
export function useCreateChatCompletionSuspenseQuery(options: UseCreateChatCompletionSuspenseQueryOptions): UseSuspenseQueryResult<CreateChatCompletionResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createChatCompletionQueryKey(init),
        queryFn: () => createChatCompletion(client, init),
        ...queryOptions
    });
}

/**
 * `POST /chat/completions`
 *
 * Creates a model response for the given chat conversation.
 */
export function useCreateChatCompletionMutation(options: Partial<UseMutationOptions<CreateChatCompletionResponse, unknown, CreateChatCompletionInit>> = {}): UseMutationResult<CreateChatCompletionResponse, unknown, CreateChatCompletionInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateChatCompletionInit) => createChatCompletion(client, init),
        ...options
    });
}

export interface CreateCompletionInit {
    body: CreateCompletionRequest;
    request?: RequestInit;
}

function createCompletion(client: OpenAPIClient, init: CreateCompletionInit): Promise<CreateCompletionResponse> {
    return client[internal_fetch]("POST", "/completions", init, "application/json");
}

export function createCompletionQueryKey(init: CreateCompletionInit): QueryKey {
    return ["post", "/completions", init];
}

export interface PrefetchCreateCompletionOptions extends CreateCompletionInit {
    queryOptions?: Partial<FetchQueryOptions<CreateCompletionResponse, unknown>>;
}

export function prefetchCreateCompletion(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateCompletionOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createCompletionQueryKey(init),
        queryFn: () => createCompletion(client, init),
        ...queryOptions
    });
}

export interface UseCreateCompletionQueryOptions extends CreateCompletionInit {
    queryOptions?: Partial<UseQueryOptions<CreateCompletionResponse, unknown>>;
}

/**
 * `POST /completions`
 *
 * Creates a completion for the provided prompt and parameters.
 */
export function useCreateCompletionQuery(options: UseCreateCompletionQueryOptions): UseQueryResult<CreateCompletionResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createCompletionQueryKey(init),
        queryFn: () => createCompletion(client, init),
        ...queryOptions
    });
}

export interface UseCreateCompletionSuspenseQueryOptions extends CreateCompletionInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<CreateCompletionResponse, unknown>>;
}

/**
 * `POST /completions`
 *
 * Creates a completion for the provided prompt and parameters.
 */
export function useCreateCompletionSuspenseQuery(options: UseCreateCompletionSuspenseQueryOptions): UseSuspenseQueryResult<CreateCompletionResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createCompletionQueryKey(init),
        queryFn: () => createCompletion(client, init),
        ...queryOptions
    });
}

/**
 * `POST /completions`
 *
 * Creates a completion for the provided prompt and parameters.
 */
export function useCreateCompletionMutation(options: Partial<UseMutationOptions<CreateCompletionResponse, unknown, CreateCompletionInit>> = {}): UseMutationResult<CreateCompletionResponse, unknown, CreateCompletionInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateCompletionInit) => createCompletion(client, init),
        ...options
    });
}

export interface CreateImageInit {
    body: CreateImageRequest;
    request?: RequestInit;
}

function createImage(client: OpenAPIClient, init: CreateImageInit): Promise<ImagesResponse> {
    return client[internal_fetch]("POST", "/images/generations", init, "application/json");
}

export function createImageQueryKey(init: CreateImageInit): QueryKey {
    return ["post", "/images/generations", init];
}

export interface PrefetchCreateImageOptions extends CreateImageInit {
    queryOptions?: Partial<FetchQueryOptions<ImagesResponse, unknown>>;
}

export function prefetchCreateImage(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateImageOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createImageQueryKey(init),
        queryFn: () => createImage(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageQueryOptions extends CreateImageInit {
    queryOptions?: Partial<UseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/generations`
 *
 * Creates an image given a prompt.
 */
export function useCreateImageQuery(options: UseCreateImageQueryOptions): UseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createImageQueryKey(init),
        queryFn: () => createImage(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageSuspenseQueryOptions extends CreateImageInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/generations`
 *
 * Creates an image given a prompt.
 */
export function useCreateImageSuspenseQuery(options: UseCreateImageSuspenseQueryOptions): UseSuspenseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createImageQueryKey(init),
        queryFn: () => createImage(client, init),
        ...queryOptions
    });
}

/**
 * `POST /images/generations`
 *
 * Creates an image given a prompt.
 */
export function useCreateImageMutation(options: Partial<UseMutationOptions<ImagesResponse, unknown, CreateImageInit>> = {}): UseMutationResult<ImagesResponse, unknown, CreateImageInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateImageInit) => createImage(client, init),
        ...options
    });
}

export interface CreateImageEditInit {
    body: CreateImageEditRequest | FormData;
    request?: RequestInit;
}

function createImageEdit(client: OpenAPIClient, init: CreateImageEditInit): Promise<ImagesResponse> {
    return client[internal_fetch]("POST", "/images/edits", init, "multipart/form-data");
}

export function createImageEditQueryKey(init: CreateImageEditInit): QueryKey {
    return ["post", "/images/edits", init];
}

export interface PrefetchCreateImageEditOptions extends CreateImageEditInit {
    queryOptions?: Partial<FetchQueryOptions<ImagesResponse, unknown>>;
}

export function prefetchCreateImageEdit(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateImageEditOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createImageEditQueryKey(init),
        queryFn: () => createImageEdit(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageEditQueryOptions extends CreateImageEditInit {
    queryOptions?: Partial<UseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/edits`
 *
 * Creates an edited or extended image given an original image and a prompt.
 */
export function useCreateImageEditQuery(options: UseCreateImageEditQueryOptions): UseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createImageEditQueryKey(init),
        queryFn: () => createImageEdit(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageEditSuspenseQueryOptions extends CreateImageEditInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/edits`
 *
 * Creates an edited or extended image given an original image and a prompt.
 */
export function useCreateImageEditSuspenseQuery(options: UseCreateImageEditSuspenseQueryOptions): UseSuspenseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createImageEditQueryKey(init),
        queryFn: () => createImageEdit(client, init),
        ...queryOptions
    });
}

/**
 * `POST /images/edits`
 *
 * Creates an edited or extended image given an original image and a prompt.
 */
export function useCreateImageEditMutation(options: Partial<UseMutationOptions<ImagesResponse, unknown, CreateImageEditInit>> = {}): UseMutationResult<ImagesResponse, unknown, CreateImageEditInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateImageEditInit) => createImageEdit(client, init),
        ...options
    });
}

export interface CreateImageVariationInit {
    body: CreateImageVariationRequest | FormData;
    request?: RequestInit;
}

function createImageVariation(client: OpenAPIClient, init: CreateImageVariationInit): Promise<ImagesResponse> {
    return client[internal_fetch]("POST", "/images/variations", init, "multipart/form-data");
}

export function createImageVariationQueryKey(init: CreateImageVariationInit): QueryKey {
    return ["post", "/images/variations", init];
}

export interface PrefetchCreateImageVariationOptions extends CreateImageVariationInit {
    queryOptions?: Partial<FetchQueryOptions<ImagesResponse, unknown>>;
}

export function prefetchCreateImageVariation(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateImageVariationOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createImageVariationQueryKey(init),
        queryFn: () => createImageVariation(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageVariationQueryOptions extends CreateImageVariationInit {
    queryOptions?: Partial<UseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/variations`
 *
 * Creates a variation of a given image.
 */
export function useCreateImageVariationQuery(options: UseCreateImageVariationQueryOptions): UseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createImageVariationQueryKey(init),
        queryFn: () => createImageVariation(client, init),
        ...queryOptions
    });
}

export interface UseCreateImageVariationSuspenseQueryOptions extends CreateImageVariationInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ImagesResponse, unknown>>;
}

/**
 * `POST /images/variations`
 *
 * Creates a variation of a given image.
 */
export function useCreateImageVariationSuspenseQuery(options: UseCreateImageVariationSuspenseQueryOptions): UseSuspenseQueryResult<ImagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createImageVariationQueryKey(init),
        queryFn: () => createImageVariation(client, init),
        ...queryOptions
    });
}

/**
 * `POST /images/variations`
 *
 * Creates a variation of a given image.
 */
export function useCreateImageVariationMutation(options: Partial<UseMutationOptions<ImagesResponse, unknown, CreateImageVariationInit>> = {}): UseMutationResult<ImagesResponse, unknown, CreateImageVariationInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateImageVariationInit) => createImageVariation(client, init),
        ...options
    });
}

export interface CreateEmbeddingInit {
    body: CreateEmbeddingRequest;
    request?: RequestInit;
}

function createEmbedding(client: OpenAPIClient, init: CreateEmbeddingInit): Promise<CreateEmbeddingResponse> {
    return client[internal_fetch]("POST", "/embeddings", init, "application/json");
}

export function createEmbeddingQueryKey(init: CreateEmbeddingInit): QueryKey {
    return ["post", "/embeddings", init];
}

export interface PrefetchCreateEmbeddingOptions extends CreateEmbeddingInit {
    queryOptions?: Partial<FetchQueryOptions<CreateEmbeddingResponse, unknown>>;
}

export function prefetchCreateEmbedding(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateEmbeddingOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createEmbeddingQueryKey(init),
        queryFn: () => createEmbedding(client, init),
        ...queryOptions
    });
}

export interface UseCreateEmbeddingQueryOptions extends CreateEmbeddingInit {
    queryOptions?: Partial<UseQueryOptions<CreateEmbeddingResponse, unknown>>;
}

/**
 * `POST /embeddings`
 *
 * Creates an embedding vector representing the input text.
 */
export function useCreateEmbeddingQuery(options: UseCreateEmbeddingQueryOptions): UseQueryResult<CreateEmbeddingResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createEmbeddingQueryKey(init),
        queryFn: () => createEmbedding(client, init),
        ...queryOptions
    });
}

export interface UseCreateEmbeddingSuspenseQueryOptions extends CreateEmbeddingInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<CreateEmbeddingResponse, unknown>>;
}

/**
 * `POST /embeddings`
 *
 * Creates an embedding vector representing the input text.
 */
export function useCreateEmbeddingSuspenseQuery(options: UseCreateEmbeddingSuspenseQueryOptions): UseSuspenseQueryResult<CreateEmbeddingResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createEmbeddingQueryKey(init),
        queryFn: () => createEmbedding(client, init),
        ...queryOptions
    });
}

/**
 * `POST /embeddings`
 *
 * Creates an embedding vector representing the input text.
 */
export function useCreateEmbeddingMutation(options: Partial<UseMutationOptions<CreateEmbeddingResponse, unknown, CreateEmbeddingInit>> = {}): UseMutationResult<CreateEmbeddingResponse, unknown, CreateEmbeddingInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateEmbeddingInit) => createEmbedding(client, init),
        ...options
    });
}

export interface CreateSpeechInit {
    body: CreateSpeechRequest;
    request?: RequestInit;
}

function createSpeech(client: OpenAPIClient, init: CreateSpeechInit): Promise<unknown> {
    return client[internal_fetch]("POST", "/audio/speech", init, "application/json");
}

export function createSpeechQueryKey(init: CreateSpeechInit): QueryKey {
    return ["post", "/audio/speech", init];
}

export interface PrefetchCreateSpeechOptions extends CreateSpeechInit {
    queryOptions?: Partial<FetchQueryOptions<unknown, unknown>>;
}

export function prefetchCreateSpeech(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateSpeechOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createSpeechQueryKey(init),
        queryFn: () => createSpeech(client, init),
        ...queryOptions
    });
}

export interface UseCreateSpeechQueryOptions extends CreateSpeechInit {
    queryOptions?: Partial<UseQueryOptions<unknown, unknown>>;
}

/**
 * `POST /audio/speech`
 *
 * Generates audio from the input text.
 */
export function useCreateSpeechQuery(options: UseCreateSpeechQueryOptions): UseQueryResult<unknown, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createSpeechQueryKey(init),
        queryFn: () => createSpeech(client, init),
        ...queryOptions
    });
}

export interface UseCreateSpeechSuspenseQueryOptions extends CreateSpeechInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<unknown, unknown>>;
}

/**
 * `POST /audio/speech`
 *
 * Generates audio from the input text.
 */
export function useCreateSpeechSuspenseQuery(options: UseCreateSpeechSuspenseQueryOptions): UseSuspenseQueryResult<unknown, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createSpeechQueryKey(init),
        queryFn: () => createSpeech(client, init),
        ...queryOptions
    });
}

/**
 * `POST /audio/speech`
 *
 * Generates audio from the input text.
 */
export function useCreateSpeechMutation(options: Partial<UseMutationOptions<unknown, unknown, CreateSpeechInit>> = {}): UseMutationResult<unknown, unknown, CreateSpeechInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateSpeechInit) => createSpeech(client, init),
        ...options
    });
}

export interface CreateTranscriptionInit {
    body: CreateTranscriptionRequest | FormData;
    request?: RequestInit;
}

function createTranscription(client: OpenAPIClient, init: CreateTranscriptionInit): Promise<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson)> {
    return client[internal_fetch]("POST", "/audio/transcriptions", init, "multipart/form-data");
}

export function createTranscriptionQueryKey(init: CreateTranscriptionInit): QueryKey {
    return ["post", "/audio/transcriptions", init];
}

export interface PrefetchCreateTranscriptionOptions extends CreateTranscriptionInit {
    queryOptions?: Partial<FetchQueryOptions<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown>>;
}

export function prefetchCreateTranscription(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateTranscriptionOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createTranscriptionQueryKey(init),
        queryFn: () => createTranscription(client, init),
        ...queryOptions
    });
}

export interface UseCreateTranscriptionQueryOptions extends CreateTranscriptionInit {
    queryOptions?: Partial<UseQueryOptions<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown>>;
}

/**
 * `POST /audio/transcriptions`
 *
 * Transcribes audio into the input language.
 */
export function useCreateTranscriptionQuery(options: UseCreateTranscriptionQueryOptions): UseQueryResult<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createTranscriptionQueryKey(init),
        queryFn: () => createTranscription(client, init),
        ...queryOptions
    });
}

export interface UseCreateTranscriptionSuspenseQueryOptions extends CreateTranscriptionInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown>>;
}

/**
 * `POST /audio/transcriptions`
 *
 * Transcribes audio into the input language.
 */
export function useCreateTranscriptionSuspenseQuery(options: UseCreateTranscriptionSuspenseQueryOptions): UseSuspenseQueryResult<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createTranscriptionQueryKey(init),
        queryFn: () => createTranscription(client, init),
        ...queryOptions
    });
}

/**
 * `POST /audio/transcriptions`
 *
 * Transcribes audio into the input language.
 */
export function useCreateTranscriptionMutation(options: Partial<UseMutationOptions<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown, CreateTranscriptionInit>> = {}): UseMutationResult<(CreateTranscriptionResponseJson | CreateTranscriptionResponseVerboseJson), unknown, CreateTranscriptionInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateTranscriptionInit) => createTranscription(client, init),
        ...options
    });
}

export interface CreateTranslationInit {
    body: CreateTranslationRequest | FormData;
    request?: RequestInit;
}

function createTranslation(client: OpenAPIClient, init: CreateTranslationInit): Promise<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson)> {
    return client[internal_fetch]("POST", "/audio/translations", init, "multipart/form-data");
}

export function createTranslationQueryKey(init: CreateTranslationInit): QueryKey {
    return ["post", "/audio/translations", init];
}

export interface PrefetchCreateTranslationOptions extends CreateTranslationInit {
    queryOptions?: Partial<FetchQueryOptions<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown>>;
}

export function prefetchCreateTranslation(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateTranslationOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createTranslationQueryKey(init),
        queryFn: () => createTranslation(client, init),
        ...queryOptions
    });
}

export interface UseCreateTranslationQueryOptions extends CreateTranslationInit {
    queryOptions?: Partial<UseQueryOptions<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown>>;
}

/**
 * `POST /audio/translations`
 *
 * Translates audio into English.
 */
export function useCreateTranslationQuery(options: UseCreateTranslationQueryOptions): UseQueryResult<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createTranslationQueryKey(init),
        queryFn: () => createTranslation(client, init),
        ...queryOptions
    });
}

export interface UseCreateTranslationSuspenseQueryOptions extends CreateTranslationInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown>>;
}

/**
 * `POST /audio/translations`
 *
 * Translates audio into English.
 */
export function useCreateTranslationSuspenseQuery(options: UseCreateTranslationSuspenseQueryOptions): UseSuspenseQueryResult<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createTranslationQueryKey(init),
        queryFn: () => createTranslation(client, init),
        ...queryOptions
    });
}

/**
 * `POST /audio/translations`
 *
 * Translates audio into English.
 */
export function useCreateTranslationMutation(options: Partial<UseMutationOptions<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown, CreateTranslationInit>> = {}): UseMutationResult<(CreateTranslationResponseJson | CreateTranslationResponseVerboseJson), unknown, CreateTranslationInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateTranslationInit) => createTranslation(client, init),
        ...options
    });
}

export interface ListFilesInit {
    query?: {
        /**
         * Only return files with the given purpose.
         */
        purpose?: string;
    };
    request?: RequestInit;
}

function listFiles(client: OpenAPIClient, init: ListFilesInit = {}): Promise<ListFilesResponse> {
    return client[internal_fetch]("GET", "/files", init);
}

export function listFilesQueryKey(init: ListFilesInit = {}): QueryKey {
    return ["get", "/files", init];
}

export interface PrefetchListFilesOptions extends ListFilesInit {
    queryOptions?: Partial<FetchQueryOptions<ListFilesResponse, unknown>>;
}

export function prefetchListFiles(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListFilesOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listFilesQueryKey(init),
        queryFn: () => listFiles(client, init),
        ...queryOptions
    });
}

export interface UseListFilesQueryOptions extends ListFilesInit {
    queryOptions?: Partial<UseQueryOptions<ListFilesResponse, unknown>>;
}

/**
 * `GET /files`
 *
 * Returns a list of files that belong to the user's organization.
 */
export function useListFilesQuery(options: UseListFilesQueryOptions = {}): UseQueryResult<ListFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listFilesQueryKey(init),
        queryFn: () => listFiles(client, init),
        ...queryOptions
    });
}

export interface UseListFilesSuspenseQueryOptions extends ListFilesInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListFilesResponse, unknown>>;
}

/**
 * `GET /files`
 *
 * Returns a list of files that belong to the user's organization.
 */
export function useListFilesSuspenseQuery(options: UseListFilesSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listFilesQueryKey(init),
        queryFn: () => listFiles(client, init),
        ...queryOptions
    });
}

export interface CreateFileInit {
    body: CreateFileRequest | FormData;
    request?: RequestInit;
}

function createFile(client: OpenAPIClient, init: CreateFileInit): Promise<OpenAIFile> {
    return client[internal_fetch]("POST", "/files", init, "multipart/form-data");
}

export function createFileQueryKey(init: CreateFileInit): QueryKey {
    return ["post", "/files", init];
}

export interface PrefetchCreateFileOptions extends CreateFileInit {
    queryOptions?: Partial<FetchQueryOptions<OpenAIFile, unknown>>;
}

export function prefetchCreateFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createFileQueryKey(init),
        queryFn: () => createFile(client, init),
        ...queryOptions
    });
}

export interface UseCreateFileQueryOptions extends CreateFileInit {
    queryOptions?: Partial<UseQueryOptions<OpenAIFile, unknown>>;
}

/**
 * `POST /files`
 *
 * Upload a file that can be used across various endpoints. Individual files can be up to 512 MB, and the size of all files uploaded by one organization can be up to 100 GB.
 *
 * The Assistants API supports files up to 2 million tokens and of specific file types. See the [Assistants Tools guide](/docs/assistants/tools) for details.
 *
 * The Fine-tuning API only supports `.jsonl` files. The input also has certain required formats for fine-tuning [chat](/docs/api-reference/fine-tuning/chat-input) or [completions](/docs/api-reference/fine-tuning/completions-input) models.
 *
 * The Batch API only supports `.jsonl` files up to 100 MB in size. The input also has a specific required [format](/docs/api-reference/batch/request-input).
 *
 * Please [contact us](https://help.openai.com/) if you need to increase these storage limits.
 *
 */
export function useCreateFileQuery(options: UseCreateFileQueryOptions): UseQueryResult<OpenAIFile, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createFileQueryKey(init),
        queryFn: () => createFile(client, init),
        ...queryOptions
    });
}

export interface UseCreateFileSuspenseQueryOptions extends CreateFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<OpenAIFile, unknown>>;
}

/**
 * `POST /files`
 *
 * Upload a file that can be used across various endpoints. Individual files can be up to 512 MB, and the size of all files uploaded by one organization can be up to 100 GB.
 *
 * The Assistants API supports files up to 2 million tokens and of specific file types. See the [Assistants Tools guide](/docs/assistants/tools) for details.
 *
 * The Fine-tuning API only supports `.jsonl` files. The input also has certain required formats for fine-tuning [chat](/docs/api-reference/fine-tuning/chat-input) or [completions](/docs/api-reference/fine-tuning/completions-input) models.
 *
 * The Batch API only supports `.jsonl` files up to 100 MB in size. The input also has a specific required [format](/docs/api-reference/batch/request-input).
 *
 * Please [contact us](https://help.openai.com/) if you need to increase these storage limits.
 *
 */
export function useCreateFileSuspenseQuery(options: UseCreateFileSuspenseQueryOptions): UseSuspenseQueryResult<OpenAIFile, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createFileQueryKey(init),
        queryFn: () => createFile(client, init),
        ...queryOptions
    });
}

/**
 * `POST /files`
 *
 * Upload a file that can be used across various endpoints. Individual files can be up to 512 MB, and the size of all files uploaded by one organization can be up to 100 GB.
 *
 * The Assistants API supports files up to 2 million tokens and of specific file types. See the [Assistants Tools guide](/docs/assistants/tools) for details.
 *
 * The Fine-tuning API only supports `.jsonl` files. The input also has certain required formats for fine-tuning [chat](/docs/api-reference/fine-tuning/chat-input) or [completions](/docs/api-reference/fine-tuning/completions-input) models.
 *
 * The Batch API only supports `.jsonl` files up to 100 MB in size. The input also has a specific required [format](/docs/api-reference/batch/request-input).
 *
 * Please [contact us](https://help.openai.com/) if you need to increase these storage limits.
 *
 */
export function useCreateFileMutation(options: Partial<UseMutationOptions<OpenAIFile, unknown, CreateFileInit>> = {}): UseMutationResult<OpenAIFile, unknown, CreateFileInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateFileInit) => createFile(client, init),
        ...options
    });
}

export interface RetrieveFileInit {
    path: {
        /**
         * The ID of the file to use for this request.
         */
        file_id: string;
    };
    request?: RequestInit;
}

function retrieveFile(client: OpenAPIClient, init: RetrieveFileInit): Promise<OpenAIFile> {
    return client[internal_fetch]("GET", "/files/{file_id}", init);
}

export function retrieveFileQueryKey(init: RetrieveFileInit): QueryKey {
    return ["get", "/files/{file_id}", init];
}

export interface PrefetchRetrieveFileOptions extends RetrieveFileInit {
    queryOptions?: Partial<FetchQueryOptions<OpenAIFile, unknown>>;
}

export function prefetchRetrieveFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveFileQueryKey(init),
        queryFn: () => retrieveFile(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveFileQueryOptions extends RetrieveFileInit {
    queryOptions?: Partial<UseQueryOptions<OpenAIFile, unknown>>;
}

/**
 * `GET /files/{file_id}`
 *
 * Returns information about a specific file.
 */
export function useRetrieveFileQuery(options: UseRetrieveFileQueryOptions): UseQueryResult<OpenAIFile, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveFileQueryKey(init),
        queryFn: () => retrieveFile(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveFileSuspenseQueryOptions extends RetrieveFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<OpenAIFile, unknown>>;
}

/**
 * `GET /files/{file_id}`
 *
 * Returns information about a specific file.
 */
export function useRetrieveFileSuspenseQuery(options: UseRetrieveFileSuspenseQueryOptions): UseSuspenseQueryResult<OpenAIFile, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveFileQueryKey(init),
        queryFn: () => retrieveFile(client, init),
        ...queryOptions
    });
}

export interface DeleteFileInit {
    path: {
        /**
         * The ID of the file to use for this request.
         */
        file_id: string;
    };
    request?: RequestInit;
}

function deleteFile(client: OpenAPIClient, init: DeleteFileInit): Promise<DeleteFileResponse> {
    return client[internal_fetch]("DELETE", "/files/{file_id}", init);
}

export function deleteFileQueryKey(init: DeleteFileInit): QueryKey {
    return ["delete", "/files/{file_id}", init];
}

export interface PrefetchDeleteFileOptions extends DeleteFileInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteFileResponse, unknown>>;
}

export function prefetchDeleteFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteFileQueryKey(init),
        queryFn: () => deleteFile(client, init),
        ...queryOptions
    });
}

export interface UseDeleteFileQueryOptions extends DeleteFileInit {
    queryOptions?: Partial<UseQueryOptions<DeleteFileResponse, unknown>>;
}

/**
 * `DELETE /files/{file_id}`
 *
 * Delete a file.
 */
export function useDeleteFileQuery(options: UseDeleteFileQueryOptions): UseQueryResult<DeleteFileResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteFileQueryKey(init),
        queryFn: () => deleteFile(client, init),
        ...queryOptions
    });
}

export interface UseDeleteFileSuspenseQueryOptions extends DeleteFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteFileResponse, unknown>>;
}

/**
 * `DELETE /files/{file_id}`
 *
 * Delete a file.
 */
export function useDeleteFileSuspenseQuery(options: UseDeleteFileSuspenseQueryOptions): UseSuspenseQueryResult<DeleteFileResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteFileQueryKey(init),
        queryFn: () => deleteFile(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /files/{file_id}`
 *
 * Delete a file.
 */
export function useDeleteFileMutation(options: Partial<UseMutationOptions<DeleteFileResponse, unknown, DeleteFileInit>> = {}): UseMutationResult<DeleteFileResponse, unknown, DeleteFileInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteFileInit) => deleteFile(client, init),
        ...options
    });
}

export interface DownloadFileInit {
    path: {
        /**
         * The ID of the file to use for this request.
         */
        file_id: string;
    };
    request?: RequestInit;
}

function downloadFile(client: OpenAPIClient, init: DownloadFileInit): Promise<string> {
    return client[internal_fetch]("GET", "/files/{file_id}/content", init);
}

export function downloadFileQueryKey(init: DownloadFileInit): QueryKey {
    return ["get", "/files/{file_id}/content", init];
}

export interface PrefetchDownloadFileOptions extends DownloadFileInit {
    queryOptions?: Partial<FetchQueryOptions<string, unknown>>;
}

export function prefetchDownloadFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDownloadFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: downloadFileQueryKey(init),
        queryFn: () => downloadFile(client, init),
        ...queryOptions
    });
}

export interface UseDownloadFileQueryOptions extends DownloadFileInit {
    queryOptions?: Partial<UseQueryOptions<string, unknown>>;
}

/**
 * `GET /files/{file_id}/content`
 *
 * Returns the contents of the specified file.
 */
export function useDownloadFileQuery(options: UseDownloadFileQueryOptions): UseQueryResult<string, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: downloadFileQueryKey(init),
        queryFn: () => downloadFile(client, init),
        ...queryOptions
    });
}

export interface UseDownloadFileSuspenseQueryOptions extends DownloadFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<string, unknown>>;
}

/**
 * `GET /files/{file_id}/content`
 *
 * Returns the contents of the specified file.
 */
export function useDownloadFileSuspenseQuery(options: UseDownloadFileSuspenseQueryOptions): UseSuspenseQueryResult<string, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: downloadFileQueryKey(init),
        queryFn: () => downloadFile(client, init),
        ...queryOptions
    });
}

export interface CreateUploadInit {
    body: CreateUploadRequest;
    request?: RequestInit;
}

function createUpload(client: OpenAPIClient, init: CreateUploadInit): Promise<Upload> {
    return client[internal_fetch]("POST", "/uploads", init, "application/json");
}

export function createUploadQueryKey(init: CreateUploadInit): QueryKey {
    return ["post", "/uploads", init];
}

export interface PrefetchCreateUploadOptions extends CreateUploadInit {
    queryOptions?: Partial<FetchQueryOptions<Upload, unknown>>;
}

export function prefetchCreateUpload(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateUploadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createUploadQueryKey(init),
        queryFn: () => createUpload(client, init),
        ...queryOptions
    });
}

export interface UseCreateUploadQueryOptions extends CreateUploadInit {
    queryOptions?: Partial<UseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads`
 *
 * Creates an intermediate [Upload](/docs/api-reference/uploads/object) object that you can add [Parts](/docs/api-reference/uploads/part-object) to. Currently, an Upload can accept at most 8 GB in total and expires after an hour after you create it.
 *
 * Once you complete the Upload, we will create a [File](/docs/api-reference/files/object) object that contains all the parts you uploaded. This File is usable in the rest of our platform as a regular File object.
 *
 * For certain `purpose`s, the correct `mime_type` must be specified. Please refer to documentation for the supported MIME types for your use case:
 * - [Assistants](/docs/assistants/tools/file-search/supported-files)
 *
 * For guidance on the proper filename extensions for each purpose, please follow the documentation on [creating a File](/docs/api-reference/files/create).
 *
 */
export function useCreateUploadQuery(options: UseCreateUploadQueryOptions): UseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createUploadQueryKey(init),
        queryFn: () => createUpload(client, init),
        ...queryOptions
    });
}

export interface UseCreateUploadSuspenseQueryOptions extends CreateUploadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads`
 *
 * Creates an intermediate [Upload](/docs/api-reference/uploads/object) object that you can add [Parts](/docs/api-reference/uploads/part-object) to. Currently, an Upload can accept at most 8 GB in total and expires after an hour after you create it.
 *
 * Once you complete the Upload, we will create a [File](/docs/api-reference/files/object) object that contains all the parts you uploaded. This File is usable in the rest of our platform as a regular File object.
 *
 * For certain `purpose`s, the correct `mime_type` must be specified. Please refer to documentation for the supported MIME types for your use case:
 * - [Assistants](/docs/assistants/tools/file-search/supported-files)
 *
 * For guidance on the proper filename extensions for each purpose, please follow the documentation on [creating a File](/docs/api-reference/files/create).
 *
 */
export function useCreateUploadSuspenseQuery(options: UseCreateUploadSuspenseQueryOptions): UseSuspenseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createUploadQueryKey(init),
        queryFn: () => createUpload(client, init),
        ...queryOptions
    });
}

/**
 * `POST /uploads`
 *
 * Creates an intermediate [Upload](/docs/api-reference/uploads/object) object that you can add [Parts](/docs/api-reference/uploads/part-object) to. Currently, an Upload can accept at most 8 GB in total and expires after an hour after you create it.
 *
 * Once you complete the Upload, we will create a [File](/docs/api-reference/files/object) object that contains all the parts you uploaded. This File is usable in the rest of our platform as a regular File object.
 *
 * For certain `purpose`s, the correct `mime_type` must be specified. Please refer to documentation for the supported MIME types for your use case:
 * - [Assistants](/docs/assistants/tools/file-search/supported-files)
 *
 * For guidance on the proper filename extensions for each purpose, please follow the documentation on [creating a File](/docs/api-reference/files/create).
 *
 */
export function useCreateUploadMutation(options: Partial<UseMutationOptions<Upload, unknown, CreateUploadInit>> = {}): UseMutationResult<Upload, unknown, CreateUploadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateUploadInit) => createUpload(client, init),
        ...options
    });
}

export interface AddUploadPartInit {
    body: AddUploadPartRequest | FormData;
    path: {
        /**
         * The ID of the Upload.
         *
         */
        upload_id: string;
    };
    request?: RequestInit;
}

function addUploadPart(client: OpenAPIClient, init: AddUploadPartInit): Promise<UploadPart> {
    return client[internal_fetch]("POST", "/uploads/{upload_id}/parts", init, "multipart/form-data");
}

export function addUploadPartQueryKey(init: AddUploadPartInit): QueryKey {
    return ["post", "/uploads/{upload_id}/parts", init];
}

export interface PrefetchAddUploadPartOptions extends AddUploadPartInit {
    queryOptions?: Partial<FetchQueryOptions<UploadPart, unknown>>;
}

export function prefetchAddUploadPart(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchAddUploadPartOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: addUploadPartQueryKey(init),
        queryFn: () => addUploadPart(client, init),
        ...queryOptions
    });
}

export interface UseAddUploadPartQueryOptions extends AddUploadPartInit {
    queryOptions?: Partial<UseQueryOptions<UploadPart, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/parts`
 *
 * Adds a [Part](/docs/api-reference/uploads/part-object) to an [Upload](/docs/api-reference/uploads/object) object. A Part represents a chunk of bytes from the file you are trying to upload.
 *
 * Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.
 *
 * It is possible to add multiple Parts in parallel. You can decide the intended order of the Parts when you [complete the Upload](/docs/api-reference/uploads/complete).
 *
 */
export function useAddUploadPartQuery(options: UseAddUploadPartQueryOptions): UseQueryResult<UploadPart, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: addUploadPartQueryKey(init),
        queryFn: () => addUploadPart(client, init),
        ...queryOptions
    });
}

export interface UseAddUploadPartSuspenseQueryOptions extends AddUploadPartInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<UploadPart, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/parts`
 *
 * Adds a [Part](/docs/api-reference/uploads/part-object) to an [Upload](/docs/api-reference/uploads/object) object. A Part represents a chunk of bytes from the file you are trying to upload.
 *
 * Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.
 *
 * It is possible to add multiple Parts in parallel. You can decide the intended order of the Parts when you [complete the Upload](/docs/api-reference/uploads/complete).
 *
 */
export function useAddUploadPartSuspenseQuery(options: UseAddUploadPartSuspenseQueryOptions): UseSuspenseQueryResult<UploadPart, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: addUploadPartQueryKey(init),
        queryFn: () => addUploadPart(client, init),
        ...queryOptions
    });
}

/**
 * `POST /uploads/{upload_id}/parts`
 *
 * Adds a [Part](/docs/api-reference/uploads/part-object) to an [Upload](/docs/api-reference/uploads/object) object. A Part represents a chunk of bytes from the file you are trying to upload.
 *
 * Each Part can be at most 64 MB, and you can add Parts until you hit the Upload maximum of 8 GB.
 *
 * It is possible to add multiple Parts in parallel. You can decide the intended order of the Parts when you [complete the Upload](/docs/api-reference/uploads/complete).
 *
 */
export function useAddUploadPartMutation(options: Partial<UseMutationOptions<UploadPart, unknown, AddUploadPartInit>> = {}): UseMutationResult<UploadPart, unknown, AddUploadPartInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: AddUploadPartInit) => addUploadPart(client, init),
        ...options
    });
}

export interface CompleteUploadInit {
    body: CompleteUploadRequest;
    path: {
        /**
         * The ID of the Upload.
         *
         */
        upload_id: string;
    };
    request?: RequestInit;
}

function completeUpload(client: OpenAPIClient, init: CompleteUploadInit): Promise<Upload> {
    return client[internal_fetch]("POST", "/uploads/{upload_id}/complete", init, "application/json");
}

export function completeUploadQueryKey(init: CompleteUploadInit): QueryKey {
    return ["post", "/uploads/{upload_id}/complete", init];
}

export interface PrefetchCompleteUploadOptions extends CompleteUploadInit {
    queryOptions?: Partial<FetchQueryOptions<Upload, unknown>>;
}

export function prefetchCompleteUpload(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCompleteUploadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: completeUploadQueryKey(init),
        queryFn: () => completeUpload(client, init),
        ...queryOptions
    });
}

export interface UseCompleteUploadQueryOptions extends CompleteUploadInit {
    queryOptions?: Partial<UseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/complete`
 *
 * Completes the [Upload](/docs/api-reference/uploads/object).
 *
 * Within the returned Upload object, there is a nested [File](/docs/api-reference/files/object) object that is ready to use in the rest of the platform.
 *
 * You can specify the order of the Parts by passing in an ordered list of the Part IDs.
 *
 * The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the Upload object. No Parts may be added after an Upload is completed.
 *
 */
export function useCompleteUploadQuery(options: UseCompleteUploadQueryOptions): UseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: completeUploadQueryKey(init),
        queryFn: () => completeUpload(client, init),
        ...queryOptions
    });
}

export interface UseCompleteUploadSuspenseQueryOptions extends CompleteUploadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/complete`
 *
 * Completes the [Upload](/docs/api-reference/uploads/object).
 *
 * Within the returned Upload object, there is a nested [File](/docs/api-reference/files/object) object that is ready to use in the rest of the platform.
 *
 * You can specify the order of the Parts by passing in an ordered list of the Part IDs.
 *
 * The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the Upload object. No Parts may be added after an Upload is completed.
 *
 */
export function useCompleteUploadSuspenseQuery(options: UseCompleteUploadSuspenseQueryOptions): UseSuspenseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: completeUploadQueryKey(init),
        queryFn: () => completeUpload(client, init),
        ...queryOptions
    });
}

/**
 * `POST /uploads/{upload_id}/complete`
 *
 * Completes the [Upload](/docs/api-reference/uploads/object).
 *
 * Within the returned Upload object, there is a nested [File](/docs/api-reference/files/object) object that is ready to use in the rest of the platform.
 *
 * You can specify the order of the Parts by passing in an ordered list of the Part IDs.
 *
 * The number of bytes uploaded upon completion must match the number of bytes initially specified when creating the Upload object. No Parts may be added after an Upload is completed.
 *
 */
export function useCompleteUploadMutation(options: Partial<UseMutationOptions<Upload, unknown, CompleteUploadInit>> = {}): UseMutationResult<Upload, unknown, CompleteUploadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CompleteUploadInit) => completeUpload(client, init),
        ...options
    });
}

export interface CancelUploadInit {
    path: {
        /**
         * The ID of the Upload.
         *
         */
        upload_id: string;
    };
    request?: RequestInit;
}

function cancelUpload(client: OpenAPIClient, init: CancelUploadInit): Promise<Upload> {
    return client[internal_fetch]("POST", "/uploads/{upload_id}/cancel", init);
}

export function cancelUploadQueryKey(init: CancelUploadInit): QueryKey {
    return ["post", "/uploads/{upload_id}/cancel", init];
}

export interface PrefetchCancelUploadOptions extends CancelUploadInit {
    queryOptions?: Partial<FetchQueryOptions<Upload, unknown>>;
}

export function prefetchCancelUpload(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCancelUploadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: cancelUploadQueryKey(init),
        queryFn: () => cancelUpload(client, init),
        ...queryOptions
    });
}

export interface UseCancelUploadQueryOptions extends CancelUploadInit {
    queryOptions?: Partial<UseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/cancel`
 *
 * Cancels the Upload. No Parts may be added after an Upload is cancelled.
 *
 */
export function useCancelUploadQuery(options: UseCancelUploadQueryOptions): UseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: cancelUploadQueryKey(init),
        queryFn: () => cancelUpload(client, init),
        ...queryOptions
    });
}

export interface UseCancelUploadSuspenseQueryOptions extends CancelUploadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Upload, unknown>>;
}

/**
 * `POST /uploads/{upload_id}/cancel`
 *
 * Cancels the Upload. No Parts may be added after an Upload is cancelled.
 *
 */
export function useCancelUploadSuspenseQuery(options: UseCancelUploadSuspenseQueryOptions): UseSuspenseQueryResult<Upload, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: cancelUploadQueryKey(init),
        queryFn: () => cancelUpload(client, init),
        ...queryOptions
    });
}

/**
 * `POST /uploads/{upload_id}/cancel`
 *
 * Cancels the Upload. No Parts may be added after an Upload is cancelled.
 *
 */
export function useCancelUploadMutation(options: Partial<UseMutationOptions<Upload, unknown, CancelUploadInit>> = {}): UseMutationResult<Upload, unknown, CancelUploadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CancelUploadInit) => cancelUpload(client, init),
        ...options
    });
}

export interface ListPaginatedFineTuningJobsInit {
    query?: {
        /**
         * Identifier for the last job from the previous pagination request.
         */
        after?: string;
        /**
         * Number of fine-tuning jobs to retrieve.
         * @defaultValue `20`
         */
        limit?: number;
    };
    request?: RequestInit;
}

function listPaginatedFineTuningJobs(client: OpenAPIClient, init: ListPaginatedFineTuningJobsInit = {}): Promise<ListPaginatedFineTuningJobsResponse> {
    return client[internal_fetch]("GET", "/fine_tuning/jobs", init);
}

export function listPaginatedFineTuningJobsQueryKey(init: ListPaginatedFineTuningJobsInit = {}): QueryKey {
    return ["get", "/fine_tuning/jobs", init];
}

export interface PrefetchListPaginatedFineTuningJobsOptions extends ListPaginatedFineTuningJobsInit {
    queryOptions?: Partial<FetchQueryOptions<ListPaginatedFineTuningJobsResponse, unknown>>;
}

export function prefetchListPaginatedFineTuningJobs(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListPaginatedFineTuningJobsOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listPaginatedFineTuningJobsQueryKey(init),
        queryFn: () => listPaginatedFineTuningJobs(client, init),
        ...queryOptions
    });
}

export interface UseListPaginatedFineTuningJobsQueryOptions extends ListPaginatedFineTuningJobsInit {
    queryOptions?: Partial<UseQueryOptions<ListPaginatedFineTuningJobsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs`
 *
 * List your organization's fine-tuning jobs
 *
 */
export function useListPaginatedFineTuningJobsQuery(options: UseListPaginatedFineTuningJobsQueryOptions = {}): UseQueryResult<ListPaginatedFineTuningJobsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listPaginatedFineTuningJobsQueryKey(init),
        queryFn: () => listPaginatedFineTuningJobs(client, init),
        ...queryOptions
    });
}

export interface UseListPaginatedFineTuningJobsSuspenseQueryOptions extends ListPaginatedFineTuningJobsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListPaginatedFineTuningJobsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs`
 *
 * List your organization's fine-tuning jobs
 *
 */
export function useListPaginatedFineTuningJobsSuspenseQuery(options: UseListPaginatedFineTuningJobsSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListPaginatedFineTuningJobsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listPaginatedFineTuningJobsQueryKey(init),
        queryFn: () => listPaginatedFineTuningJobs(client, init),
        ...queryOptions
    });
}

export interface CreateFineTuningJobInit {
    body: CreateFineTuningJobRequest;
    request?: RequestInit;
}

function createFineTuningJob(client: OpenAPIClient, init: CreateFineTuningJobInit): Promise<FineTuningJob> {
    return client[internal_fetch]("POST", "/fine_tuning/jobs", init, "application/json");
}

export function createFineTuningJobQueryKey(init: CreateFineTuningJobInit): QueryKey {
    return ["post", "/fine_tuning/jobs", init];
}

export interface PrefetchCreateFineTuningJobOptions extends CreateFineTuningJobInit {
    queryOptions?: Partial<FetchQueryOptions<FineTuningJob, unknown>>;
}

export function prefetchCreateFineTuningJob(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateFineTuningJobOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createFineTuningJobQueryKey(init),
        queryFn: () => createFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseCreateFineTuningJobQueryOptions extends CreateFineTuningJobInit {
    queryOptions?: Partial<UseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `POST /fine_tuning/jobs`
 *
 * Creates a fine-tuning job which begins the process of creating a new model from a given dataset.
 *
 * Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.
 *
 * [Learn more about fine-tuning](/docs/guides/fine-tuning)
 *
 */
export function useCreateFineTuningJobQuery(options: UseCreateFineTuningJobQueryOptions): UseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createFineTuningJobQueryKey(init),
        queryFn: () => createFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseCreateFineTuningJobSuspenseQueryOptions extends CreateFineTuningJobInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `POST /fine_tuning/jobs`
 *
 * Creates a fine-tuning job which begins the process of creating a new model from a given dataset.
 *
 * Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.
 *
 * [Learn more about fine-tuning](/docs/guides/fine-tuning)
 *
 */
export function useCreateFineTuningJobSuspenseQuery(options: UseCreateFineTuningJobSuspenseQueryOptions): UseSuspenseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createFineTuningJobQueryKey(init),
        queryFn: () => createFineTuningJob(client, init),
        ...queryOptions
    });
}

/**
 * `POST /fine_tuning/jobs`
 *
 * Creates a fine-tuning job which begins the process of creating a new model from a given dataset.
 *
 * Response includes details of the enqueued job including job status and the name of the fine-tuned models once complete.
 *
 * [Learn more about fine-tuning](/docs/guides/fine-tuning)
 *
 */
export function useCreateFineTuningJobMutation(options: Partial<UseMutationOptions<FineTuningJob, unknown, CreateFineTuningJobInit>> = {}): UseMutationResult<FineTuningJob, unknown, CreateFineTuningJobInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateFineTuningJobInit) => createFineTuningJob(client, init),
        ...options
    });
}

export interface RetrieveFineTuningJobInit {
    path: {
        /**
         * The ID of the fine-tuning job.
         *
         */
        fine_tuning_job_id: string;
    };
    request?: RequestInit;
}

function retrieveFineTuningJob(client: OpenAPIClient, init: RetrieveFineTuningJobInit): Promise<FineTuningJob> {
    return client[internal_fetch]("GET", "/fine_tuning/jobs/{fine_tuning_job_id}", init);
}

export function retrieveFineTuningJobQueryKey(init: RetrieveFineTuningJobInit): QueryKey {
    return ["get", "/fine_tuning/jobs/{fine_tuning_job_id}", init];
}

export interface PrefetchRetrieveFineTuningJobOptions extends RetrieveFineTuningJobInit {
    queryOptions?: Partial<FetchQueryOptions<FineTuningJob, unknown>>;
}

export function prefetchRetrieveFineTuningJob(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveFineTuningJobOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveFineTuningJobQueryKey(init),
        queryFn: () => retrieveFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveFineTuningJobQueryOptions extends RetrieveFineTuningJobInit {
    queryOptions?: Partial<UseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}`
 *
 * Get info about a fine-tuning job.
 *
 * [Learn more about fine-tuning](/docs/guides/fine-tuning)
 *
 */
export function useRetrieveFineTuningJobQuery(options: UseRetrieveFineTuningJobQueryOptions): UseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveFineTuningJobQueryKey(init),
        queryFn: () => retrieveFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveFineTuningJobSuspenseQueryOptions extends RetrieveFineTuningJobInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}`
 *
 * Get info about a fine-tuning job.
 *
 * [Learn more about fine-tuning](/docs/guides/fine-tuning)
 *
 */
export function useRetrieveFineTuningJobSuspenseQuery(options: UseRetrieveFineTuningJobSuspenseQueryOptions): UseSuspenseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveFineTuningJobQueryKey(init),
        queryFn: () => retrieveFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface ListFineTuningEventsInit {
    path: {
        /**
         * The ID of the fine-tuning job to get events for.
         *
         */
        fine_tuning_job_id: string;
    };
    query?: {
        /**
         * Identifier for the last event from the previous pagination request.
         */
        after?: string;
        /**
         * Number of events to retrieve.
         * @defaultValue `20`
         */
        limit?: number;
    };
    request?: RequestInit;
}

function listFineTuningEvents(client: OpenAPIClient, init: ListFineTuningEventsInit): Promise<ListFineTuningJobEventsResponse> {
    return client[internal_fetch]("GET", "/fine_tuning/jobs/{fine_tuning_job_id}/events", init);
}

export function listFineTuningEventsQueryKey(init: ListFineTuningEventsInit): QueryKey {
    return ["get", "/fine_tuning/jobs/{fine_tuning_job_id}/events", init];
}

export interface PrefetchListFineTuningEventsOptions extends ListFineTuningEventsInit {
    queryOptions?: Partial<FetchQueryOptions<ListFineTuningJobEventsResponse, unknown>>;
}

export function prefetchListFineTuningEvents(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListFineTuningEventsOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listFineTuningEventsQueryKey(init),
        queryFn: () => listFineTuningEvents(client, init),
        ...queryOptions
    });
}

export interface UseListFineTuningEventsQueryOptions extends ListFineTuningEventsInit {
    queryOptions?: Partial<UseQueryOptions<ListFineTuningJobEventsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}/events`
 *
 * Get status updates for a fine-tuning job.
 *
 */
export function useListFineTuningEventsQuery(options: UseListFineTuningEventsQueryOptions): UseQueryResult<ListFineTuningJobEventsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listFineTuningEventsQueryKey(init),
        queryFn: () => listFineTuningEvents(client, init),
        ...queryOptions
    });
}

export interface UseListFineTuningEventsSuspenseQueryOptions extends ListFineTuningEventsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListFineTuningJobEventsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}/events`
 *
 * Get status updates for a fine-tuning job.
 *
 */
export function useListFineTuningEventsSuspenseQuery(options: UseListFineTuningEventsSuspenseQueryOptions): UseSuspenseQueryResult<ListFineTuningJobEventsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listFineTuningEventsQueryKey(init),
        queryFn: () => listFineTuningEvents(client, init),
        ...queryOptions
    });
}

export interface CancelFineTuningJobInit {
    path: {
        /**
         * The ID of the fine-tuning job to cancel.
         *
         */
        fine_tuning_job_id: string;
    };
    request?: RequestInit;
}

function cancelFineTuningJob(client: OpenAPIClient, init: CancelFineTuningJobInit): Promise<FineTuningJob> {
    return client[internal_fetch]("POST", "/fine_tuning/jobs/{fine_tuning_job_id}/cancel", init);
}

export function cancelFineTuningJobQueryKey(init: CancelFineTuningJobInit): QueryKey {
    return ["post", "/fine_tuning/jobs/{fine_tuning_job_id}/cancel", init];
}

export interface PrefetchCancelFineTuningJobOptions extends CancelFineTuningJobInit {
    queryOptions?: Partial<FetchQueryOptions<FineTuningJob, unknown>>;
}

export function prefetchCancelFineTuningJob(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCancelFineTuningJobOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: cancelFineTuningJobQueryKey(init),
        queryFn: () => cancelFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseCancelFineTuningJobQueryOptions extends CancelFineTuningJobInit {
    queryOptions?: Partial<UseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel`
 *
 * Immediately cancel a fine-tune job.
 *
 */
export function useCancelFineTuningJobQuery(options: UseCancelFineTuningJobQueryOptions): UseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: cancelFineTuningJobQueryKey(init),
        queryFn: () => cancelFineTuningJob(client, init),
        ...queryOptions
    });
}

export interface UseCancelFineTuningJobSuspenseQueryOptions extends CancelFineTuningJobInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<FineTuningJob, unknown>>;
}

/**
 * `POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel`
 *
 * Immediately cancel a fine-tune job.
 *
 */
export function useCancelFineTuningJobSuspenseQuery(options: UseCancelFineTuningJobSuspenseQueryOptions): UseSuspenseQueryResult<FineTuningJob, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: cancelFineTuningJobQueryKey(init),
        queryFn: () => cancelFineTuningJob(client, init),
        ...queryOptions
    });
}

/**
 * `POST /fine_tuning/jobs/{fine_tuning_job_id}/cancel`
 *
 * Immediately cancel a fine-tune job.
 *
 */
export function useCancelFineTuningJobMutation(options: Partial<UseMutationOptions<FineTuningJob, unknown, CancelFineTuningJobInit>> = {}): UseMutationResult<FineTuningJob, unknown, CancelFineTuningJobInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CancelFineTuningJobInit) => cancelFineTuningJob(client, init),
        ...options
    });
}

export interface ListFineTuningJobCheckpointsInit {
    path: {
        /**
         * The ID of the fine-tuning job to get checkpoints for.
         *
         */
        fine_tuning_job_id: string;
    };
    query?: {
        /**
         * Identifier for the last checkpoint ID from the previous pagination request.
         */
        after?: string;
        /**
         * Number of checkpoints to retrieve.
         * @defaultValue `10`
         */
        limit?: number;
    };
    request?: RequestInit;
}

function listFineTuningJobCheckpoints(client: OpenAPIClient, init: ListFineTuningJobCheckpointsInit): Promise<ListFineTuningJobCheckpointsResponse> {
    return client[internal_fetch]("GET", "/fine_tuning/jobs/{fine_tuning_job_id}/checkpoints", init);
}

export function listFineTuningJobCheckpointsQueryKey(init: ListFineTuningJobCheckpointsInit): QueryKey {
    return ["get", "/fine_tuning/jobs/{fine_tuning_job_id}/checkpoints", init];
}

export interface PrefetchListFineTuningJobCheckpointsOptions extends ListFineTuningJobCheckpointsInit {
    queryOptions?: Partial<FetchQueryOptions<ListFineTuningJobCheckpointsResponse, unknown>>;
}

export function prefetchListFineTuningJobCheckpoints(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListFineTuningJobCheckpointsOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listFineTuningJobCheckpointsQueryKey(init),
        queryFn: () => listFineTuningJobCheckpoints(client, init),
        ...queryOptions
    });
}

export interface UseListFineTuningJobCheckpointsQueryOptions extends ListFineTuningJobCheckpointsInit {
    queryOptions?: Partial<UseQueryOptions<ListFineTuningJobCheckpointsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}/checkpoints`
 *
 * List checkpoints for a fine-tuning job.
 *
 */
export function useListFineTuningJobCheckpointsQuery(options: UseListFineTuningJobCheckpointsQueryOptions): UseQueryResult<ListFineTuningJobCheckpointsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listFineTuningJobCheckpointsQueryKey(init),
        queryFn: () => listFineTuningJobCheckpoints(client, init),
        ...queryOptions
    });
}

export interface UseListFineTuningJobCheckpointsSuspenseQueryOptions extends ListFineTuningJobCheckpointsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListFineTuningJobCheckpointsResponse, unknown>>;
}

/**
 * `GET /fine_tuning/jobs/{fine_tuning_job_id}/checkpoints`
 *
 * List checkpoints for a fine-tuning job.
 *
 */
export function useListFineTuningJobCheckpointsSuspenseQuery(options: UseListFineTuningJobCheckpointsSuspenseQueryOptions): UseSuspenseQueryResult<ListFineTuningJobCheckpointsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listFineTuningJobCheckpointsQueryKey(init),
        queryFn: () => listFineTuningJobCheckpoints(client, init),
        ...queryOptions
    });
}

export interface ListModelsInit {
    request?: RequestInit;
}

function listModels(client: OpenAPIClient, init: ListModelsInit = {}): Promise<ListModelsResponse> {
    return client[internal_fetch]("GET", "/models", init);
}

export function listModelsQueryKey(init: ListModelsInit = {}): QueryKey {
    return ["get", "/models", init];
}

export interface PrefetchListModelsOptions extends ListModelsInit {
    queryOptions?: Partial<FetchQueryOptions<ListModelsResponse, unknown>>;
}

export function prefetchListModels(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListModelsOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listModelsQueryKey(init),
        queryFn: () => listModels(client, init),
        ...queryOptions
    });
}

export interface UseListModelsQueryOptions extends ListModelsInit {
    queryOptions?: Partial<UseQueryOptions<ListModelsResponse, unknown>>;
}

/**
 * `GET /models`
 *
 * Lists the currently available models, and provides basic information about each one such as the owner and availability.
 */
export function useListModelsQuery(options: UseListModelsQueryOptions = {}): UseQueryResult<ListModelsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listModelsQueryKey(init),
        queryFn: () => listModels(client, init),
        ...queryOptions
    });
}

export interface UseListModelsSuspenseQueryOptions extends ListModelsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListModelsResponse, unknown>>;
}

/**
 * `GET /models`
 *
 * Lists the currently available models, and provides basic information about each one such as the owner and availability.
 */
export function useListModelsSuspenseQuery(options: UseListModelsSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListModelsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listModelsQueryKey(init),
        queryFn: () => listModels(client, init),
        ...queryOptions
    });
}

export interface RetrieveModelInit {
    path: {
        /**
         * The ID of the model to use for this request
         */
        model: string;
    };
    request?: RequestInit;
}

function retrieveModel(client: OpenAPIClient, init: RetrieveModelInit): Promise<Model> {
    return client[internal_fetch]("GET", "/models/{model}", init);
}

export function retrieveModelQueryKey(init: RetrieveModelInit): QueryKey {
    return ["get", "/models/{model}", init];
}

export interface PrefetchRetrieveModelOptions extends RetrieveModelInit {
    queryOptions?: Partial<FetchQueryOptions<Model, unknown>>;
}

export function prefetchRetrieveModel(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveModelOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveModelQueryKey(init),
        queryFn: () => retrieveModel(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveModelQueryOptions extends RetrieveModelInit {
    queryOptions?: Partial<UseQueryOptions<Model, unknown>>;
}

/**
 * `GET /models/{model}`
 *
 * Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
 */
export function useRetrieveModelQuery(options: UseRetrieveModelQueryOptions): UseQueryResult<Model, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveModelQueryKey(init),
        queryFn: () => retrieveModel(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveModelSuspenseQueryOptions extends RetrieveModelInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Model, unknown>>;
}

/**
 * `GET /models/{model}`
 *
 * Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
 */
export function useRetrieveModelSuspenseQuery(options: UseRetrieveModelSuspenseQueryOptions): UseSuspenseQueryResult<Model, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveModelQueryKey(init),
        queryFn: () => retrieveModel(client, init),
        ...queryOptions
    });
}

export interface DeleteModelInit {
    path: {
        /**
         * The model to delete
         */
        model: string;
    };
    request?: RequestInit;
}

function deleteModel(client: OpenAPIClient, init: DeleteModelInit): Promise<DeleteModelResponse> {
    return client[internal_fetch]("DELETE", "/models/{model}", init);
}

export function deleteModelQueryKey(init: DeleteModelInit): QueryKey {
    return ["delete", "/models/{model}", init];
}

export interface PrefetchDeleteModelOptions extends DeleteModelInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteModelResponse, unknown>>;
}

export function prefetchDeleteModel(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteModelOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteModelQueryKey(init),
        queryFn: () => deleteModel(client, init),
        ...queryOptions
    });
}

export interface UseDeleteModelQueryOptions extends DeleteModelInit {
    queryOptions?: Partial<UseQueryOptions<DeleteModelResponse, unknown>>;
}

/**
 * `DELETE /models/{model}`
 *
 * Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.
 */
export function useDeleteModelQuery(options: UseDeleteModelQueryOptions): UseQueryResult<DeleteModelResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteModelQueryKey(init),
        queryFn: () => deleteModel(client, init),
        ...queryOptions
    });
}

export interface UseDeleteModelSuspenseQueryOptions extends DeleteModelInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteModelResponse, unknown>>;
}

/**
 * `DELETE /models/{model}`
 *
 * Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.
 */
export function useDeleteModelSuspenseQuery(options: UseDeleteModelSuspenseQueryOptions): UseSuspenseQueryResult<DeleteModelResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteModelQueryKey(init),
        queryFn: () => deleteModel(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /models/{model}`
 *
 * Delete a fine-tuned model. You must have the Owner role in your organization to delete a model.
 */
export function useDeleteModelMutation(options: Partial<UseMutationOptions<DeleteModelResponse, unknown, DeleteModelInit>> = {}): UseMutationResult<DeleteModelResponse, unknown, DeleteModelInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteModelInit) => deleteModel(client, init),
        ...options
    });
}

export interface CreateModerationInit {
    body: CreateModerationRequest;
    request?: RequestInit;
}

function createModeration(client: OpenAPIClient, init: CreateModerationInit): Promise<CreateModerationResponse> {
    return client[internal_fetch]("POST", "/moderations", init, "application/json");
}

export function createModerationQueryKey(init: CreateModerationInit): QueryKey {
    return ["post", "/moderations", init];
}

export interface PrefetchCreateModerationOptions extends CreateModerationInit {
    queryOptions?: Partial<FetchQueryOptions<CreateModerationResponse, unknown>>;
}

export function prefetchCreateModeration(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateModerationOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createModerationQueryKey(init),
        queryFn: () => createModeration(client, init),
        ...queryOptions
    });
}

export interface UseCreateModerationQueryOptions extends CreateModerationInit {
    queryOptions?: Partial<UseQueryOptions<CreateModerationResponse, unknown>>;
}

/**
 * `POST /moderations`
 *
 * Classifies if text is potentially harmful.
 */
export function useCreateModerationQuery(options: UseCreateModerationQueryOptions): UseQueryResult<CreateModerationResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createModerationQueryKey(init),
        queryFn: () => createModeration(client, init),
        ...queryOptions
    });
}

export interface UseCreateModerationSuspenseQueryOptions extends CreateModerationInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<CreateModerationResponse, unknown>>;
}

/**
 * `POST /moderations`
 *
 * Classifies if text is potentially harmful.
 */
export function useCreateModerationSuspenseQuery(options: UseCreateModerationSuspenseQueryOptions): UseSuspenseQueryResult<CreateModerationResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createModerationQueryKey(init),
        queryFn: () => createModeration(client, init),
        ...queryOptions
    });
}

/**
 * `POST /moderations`
 *
 * Classifies if text is potentially harmful.
 */
export function useCreateModerationMutation(options: Partial<UseMutationOptions<CreateModerationResponse, unknown, CreateModerationInit>> = {}): UseMutationResult<CreateModerationResponse, unknown, CreateModerationInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateModerationInit) => createModeration(client, init),
        ...options
    });
}

export interface ListAssistantsInit {
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
    };
    request?: RequestInit;
}

function listAssistants(client: OpenAPIClient, init: ListAssistantsInit = {}): Promise<ListAssistantsResponse> {
    return client[internal_fetch]("GET", "/assistants", init);
}

export function listAssistantsQueryKey(init: ListAssistantsInit = {}): QueryKey {
    return ["get", "/assistants", init];
}

export interface PrefetchListAssistantsOptions extends ListAssistantsInit {
    queryOptions?: Partial<FetchQueryOptions<ListAssistantsResponse, unknown>>;
}

export function prefetchListAssistants(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListAssistantsOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listAssistantsQueryKey(init),
        queryFn: () => listAssistants(client, init),
        ...queryOptions
    });
}

export interface UseListAssistantsQueryOptions extends ListAssistantsInit {
    queryOptions?: Partial<UseQueryOptions<ListAssistantsResponse, unknown>>;
}

/**
 * `GET /assistants`
 *
 * Returns a list of assistants.
 */
export function useListAssistantsQuery(options: UseListAssistantsQueryOptions = {}): UseQueryResult<ListAssistantsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listAssistantsQueryKey(init),
        queryFn: () => listAssistants(client, init),
        ...queryOptions
    });
}

export interface UseListAssistantsSuspenseQueryOptions extends ListAssistantsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListAssistantsResponse, unknown>>;
}

/**
 * `GET /assistants`
 *
 * Returns a list of assistants.
 */
export function useListAssistantsSuspenseQuery(options: UseListAssistantsSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListAssistantsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listAssistantsQueryKey(init),
        queryFn: () => listAssistants(client, init),
        ...queryOptions
    });
}

export interface CreateAssistantInit {
    body: CreateAssistantRequest;
    request?: RequestInit;
}

function createAssistant(client: OpenAPIClient, init: CreateAssistantInit): Promise<AssistantObject> {
    return client[internal_fetch]("POST", "/assistants", init, "application/json");
}

export function createAssistantQueryKey(init: CreateAssistantInit): QueryKey {
    return ["post", "/assistants", init];
}

export interface PrefetchCreateAssistantOptions extends CreateAssistantInit {
    queryOptions?: Partial<FetchQueryOptions<AssistantObject, unknown>>;
}

export function prefetchCreateAssistant(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateAssistantOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createAssistantQueryKey(init),
        queryFn: () => createAssistant(client, init),
        ...queryOptions
    });
}

export interface UseCreateAssistantQueryOptions extends CreateAssistantInit {
    queryOptions?: Partial<UseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `POST /assistants`
 *
 * Create an assistant with a model and instructions.
 */
export function useCreateAssistantQuery(options: UseCreateAssistantQueryOptions): UseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createAssistantQueryKey(init),
        queryFn: () => createAssistant(client, init),
        ...queryOptions
    });
}

export interface UseCreateAssistantSuspenseQueryOptions extends CreateAssistantInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `POST /assistants`
 *
 * Create an assistant with a model and instructions.
 */
export function useCreateAssistantSuspenseQuery(options: UseCreateAssistantSuspenseQueryOptions): UseSuspenseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createAssistantQueryKey(init),
        queryFn: () => createAssistant(client, init),
        ...queryOptions
    });
}

/**
 * `POST /assistants`
 *
 * Create an assistant with a model and instructions.
 */
export function useCreateAssistantMutation(options: Partial<UseMutationOptions<AssistantObject, unknown, CreateAssistantInit>> = {}): UseMutationResult<AssistantObject, unknown, CreateAssistantInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateAssistantInit) => createAssistant(client, init),
        ...options
    });
}

export interface GetAssistantInit {
    path: {
        /**
         * The ID of the assistant to retrieve.
         */
        assistant_id: string;
    };
    request?: RequestInit;
}

function getAssistant(client: OpenAPIClient, init: GetAssistantInit): Promise<AssistantObject> {
    return client[internal_fetch]("GET", "/assistants/{assistant_id}", init);
}

export function getAssistantQueryKey(init: GetAssistantInit): QueryKey {
    return ["get", "/assistants/{assistant_id}", init];
}

export interface PrefetchGetAssistantOptions extends GetAssistantInit {
    queryOptions?: Partial<FetchQueryOptions<AssistantObject, unknown>>;
}

export function prefetchGetAssistant(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetAssistantOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getAssistantQueryKey(init),
        queryFn: () => getAssistant(client, init),
        ...queryOptions
    });
}

export interface UseGetAssistantQueryOptions extends GetAssistantInit {
    queryOptions?: Partial<UseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `GET /assistants/{assistant_id}`
 *
 * Retrieves an assistant.
 */
export function useGetAssistantQuery(options: UseGetAssistantQueryOptions): UseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getAssistantQueryKey(init),
        queryFn: () => getAssistant(client, init),
        ...queryOptions
    });
}

export interface UseGetAssistantSuspenseQueryOptions extends GetAssistantInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `GET /assistants/{assistant_id}`
 *
 * Retrieves an assistant.
 */
export function useGetAssistantSuspenseQuery(options: UseGetAssistantSuspenseQueryOptions): UseSuspenseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getAssistantQueryKey(init),
        queryFn: () => getAssistant(client, init),
        ...queryOptions
    });
}

export interface ModifyAssistantInit {
    body: ModifyAssistantRequest;
    path: {
        /**
         * The ID of the assistant to modify.
         */
        assistant_id: string;
    };
    request?: RequestInit;
}

function modifyAssistant(client: OpenAPIClient, init: ModifyAssistantInit): Promise<AssistantObject> {
    return client[internal_fetch]("POST", "/assistants/{assistant_id}", init, "application/json");
}

export function modifyAssistantQueryKey(init: ModifyAssistantInit): QueryKey {
    return ["post", "/assistants/{assistant_id}", init];
}

export interface PrefetchModifyAssistantOptions extends ModifyAssistantInit {
    queryOptions?: Partial<FetchQueryOptions<AssistantObject, unknown>>;
}

export function prefetchModifyAssistant(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyAssistantOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyAssistantQueryKey(init),
        queryFn: () => modifyAssistant(client, init),
        ...queryOptions
    });
}

export interface UseModifyAssistantQueryOptions extends ModifyAssistantInit {
    queryOptions?: Partial<UseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `POST /assistants/{assistant_id}`
 *
 * Modifies an assistant.
 */
export function useModifyAssistantQuery(options: UseModifyAssistantQueryOptions): UseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyAssistantQueryKey(init),
        queryFn: () => modifyAssistant(client, init),
        ...queryOptions
    });
}

export interface UseModifyAssistantSuspenseQueryOptions extends ModifyAssistantInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<AssistantObject, unknown>>;
}

/**
 * `POST /assistants/{assistant_id}`
 *
 * Modifies an assistant.
 */
export function useModifyAssistantSuspenseQuery(options: UseModifyAssistantSuspenseQueryOptions): UseSuspenseQueryResult<AssistantObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyAssistantQueryKey(init),
        queryFn: () => modifyAssistant(client, init),
        ...queryOptions
    });
}

/**
 * `POST /assistants/{assistant_id}`
 *
 * Modifies an assistant.
 */
export function useModifyAssistantMutation(options: Partial<UseMutationOptions<AssistantObject, unknown, ModifyAssistantInit>> = {}): UseMutationResult<AssistantObject, unknown, ModifyAssistantInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyAssistantInit) => modifyAssistant(client, init),
        ...options
    });
}

export interface DeleteAssistantInit {
    path: {
        /**
         * The ID of the assistant to delete.
         */
        assistant_id: string;
    };
    request?: RequestInit;
}

function deleteAssistant(client: OpenAPIClient, init: DeleteAssistantInit): Promise<DeleteAssistantResponse> {
    return client[internal_fetch]("DELETE", "/assistants/{assistant_id}", init);
}

export function deleteAssistantQueryKey(init: DeleteAssistantInit): QueryKey {
    return ["delete", "/assistants/{assistant_id}", init];
}

export interface PrefetchDeleteAssistantOptions extends DeleteAssistantInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteAssistantResponse, unknown>>;
}

export function prefetchDeleteAssistant(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteAssistantOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteAssistantQueryKey(init),
        queryFn: () => deleteAssistant(client, init),
        ...queryOptions
    });
}

export interface UseDeleteAssistantQueryOptions extends DeleteAssistantInit {
    queryOptions?: Partial<UseQueryOptions<DeleteAssistantResponse, unknown>>;
}

/**
 * `DELETE /assistants/{assistant_id}`
 *
 * Delete an assistant.
 */
export function useDeleteAssistantQuery(options: UseDeleteAssistantQueryOptions): UseQueryResult<DeleteAssistantResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteAssistantQueryKey(init),
        queryFn: () => deleteAssistant(client, init),
        ...queryOptions
    });
}

export interface UseDeleteAssistantSuspenseQueryOptions extends DeleteAssistantInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteAssistantResponse, unknown>>;
}

/**
 * `DELETE /assistants/{assistant_id}`
 *
 * Delete an assistant.
 */
export function useDeleteAssistantSuspenseQuery(options: UseDeleteAssistantSuspenseQueryOptions): UseSuspenseQueryResult<DeleteAssistantResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteAssistantQueryKey(init),
        queryFn: () => deleteAssistant(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /assistants/{assistant_id}`
 *
 * Delete an assistant.
 */
export function useDeleteAssistantMutation(options: Partial<UseMutationOptions<DeleteAssistantResponse, unknown, DeleteAssistantInit>> = {}): UseMutationResult<DeleteAssistantResponse, unknown, DeleteAssistantInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteAssistantInit) => deleteAssistant(client, init),
        ...options
    });
}

export interface CreateThreadInit {
    body?: CreateThreadRequest;
    request?: RequestInit;
}

function createThread(client: OpenAPIClient, init: CreateThreadInit = {}): Promise<ThreadObject> {
    return client[internal_fetch]("POST", "/threads", init, "application/json");
}

export function createThreadQueryKey(init: CreateThreadInit = {}): QueryKey {
    return ["post", "/threads", init];
}

export interface PrefetchCreateThreadOptions extends CreateThreadInit {
    queryOptions?: Partial<FetchQueryOptions<ThreadObject, unknown>>;
}

export function prefetchCreateThread(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateThreadOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createThreadQueryKey(init),
        queryFn: () => createThread(client, init),
        ...queryOptions
    });
}

export interface UseCreateThreadQueryOptions extends CreateThreadInit {
    queryOptions?: Partial<UseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `POST /threads`
 *
 * Create a thread.
 */
export function useCreateThreadQuery(options: UseCreateThreadQueryOptions = {}): UseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createThreadQueryKey(init),
        queryFn: () => createThread(client, init),
        ...queryOptions
    });
}

export interface UseCreateThreadSuspenseQueryOptions extends CreateThreadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `POST /threads`
 *
 * Create a thread.
 */
export function useCreateThreadSuspenseQuery(options: UseCreateThreadSuspenseQueryOptions = {}): UseSuspenseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createThreadQueryKey(init),
        queryFn: () => createThread(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads`
 *
 * Create a thread.
 */
export function useCreateThreadMutation(options: Partial<UseMutationOptions<ThreadObject, unknown, CreateThreadInit>> = {}): UseMutationResult<ThreadObject, unknown, CreateThreadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateThreadInit = {}) => createThread(client, init),
        ...options
    });
}

export interface GetThreadInit {
    path: {
        /**
         * The ID of the thread to retrieve.
         */
        thread_id: string;
    };
    request?: RequestInit;
}

function getThread(client: OpenAPIClient, init: GetThreadInit): Promise<ThreadObject> {
    return client[internal_fetch]("GET", "/threads/{thread_id}", init);
}

export function getThreadQueryKey(init: GetThreadInit): QueryKey {
    return ["get", "/threads/{thread_id}", init];
}

export interface PrefetchGetThreadOptions extends GetThreadInit {
    queryOptions?: Partial<FetchQueryOptions<ThreadObject, unknown>>;
}

export function prefetchGetThread(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetThreadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getThreadQueryKey(init),
        queryFn: () => getThread(client, init),
        ...queryOptions
    });
}

export interface UseGetThreadQueryOptions extends GetThreadInit {
    queryOptions?: Partial<UseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}`
 *
 * Retrieves a thread.
 */
export function useGetThreadQuery(options: UseGetThreadQueryOptions): UseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getThreadQueryKey(init),
        queryFn: () => getThread(client, init),
        ...queryOptions
    });
}

export interface UseGetThreadSuspenseQueryOptions extends GetThreadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}`
 *
 * Retrieves a thread.
 */
export function useGetThreadSuspenseQuery(options: UseGetThreadSuspenseQueryOptions): UseSuspenseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getThreadQueryKey(init),
        queryFn: () => getThread(client, init),
        ...queryOptions
    });
}

export interface ModifyThreadInit {
    body: ModifyThreadRequest;
    path: {
        /**
         * The ID of the thread to modify. Only the `metadata` can be modified.
         */
        thread_id: string;
    };
    request?: RequestInit;
}

function modifyThread(client: OpenAPIClient, init: ModifyThreadInit): Promise<ThreadObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}", init, "application/json");
}

export function modifyThreadQueryKey(init: ModifyThreadInit): QueryKey {
    return ["post", "/threads/{thread_id}", init];
}

export interface PrefetchModifyThreadOptions extends ModifyThreadInit {
    queryOptions?: Partial<FetchQueryOptions<ThreadObject, unknown>>;
}

export function prefetchModifyThread(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyThreadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyThreadQueryKey(init),
        queryFn: () => modifyThread(client, init),
        ...queryOptions
    });
}

export interface UseModifyThreadQueryOptions extends ModifyThreadInit {
    queryOptions?: Partial<UseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}`
 *
 * Modifies a thread.
 */
export function useModifyThreadQuery(options: UseModifyThreadQueryOptions): UseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyThreadQueryKey(init),
        queryFn: () => modifyThread(client, init),
        ...queryOptions
    });
}

export interface UseModifyThreadSuspenseQueryOptions extends ModifyThreadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ThreadObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}`
 *
 * Modifies a thread.
 */
export function useModifyThreadSuspenseQuery(options: UseModifyThreadSuspenseQueryOptions): UseSuspenseQueryResult<ThreadObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyThreadQueryKey(init),
        queryFn: () => modifyThread(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}`
 *
 * Modifies a thread.
 */
export function useModifyThreadMutation(options: Partial<UseMutationOptions<ThreadObject, unknown, ModifyThreadInit>> = {}): UseMutationResult<ThreadObject, unknown, ModifyThreadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyThreadInit) => modifyThread(client, init),
        ...options
    });
}

export interface DeleteThreadInit {
    path: {
        /**
         * The ID of the thread to delete.
         */
        thread_id: string;
    };
    request?: RequestInit;
}

function deleteThread(client: OpenAPIClient, init: DeleteThreadInit): Promise<DeleteThreadResponse> {
    return client[internal_fetch]("DELETE", "/threads/{thread_id}", init);
}

export function deleteThreadQueryKey(init: DeleteThreadInit): QueryKey {
    return ["delete", "/threads/{thread_id}", init];
}

export interface PrefetchDeleteThreadOptions extends DeleteThreadInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteThreadResponse, unknown>>;
}

export function prefetchDeleteThread(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteThreadOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteThreadQueryKey(init),
        queryFn: () => deleteThread(client, init),
        ...queryOptions
    });
}

export interface UseDeleteThreadQueryOptions extends DeleteThreadInit {
    queryOptions?: Partial<UseQueryOptions<DeleteThreadResponse, unknown>>;
}

/**
 * `DELETE /threads/{thread_id}`
 *
 * Delete a thread.
 */
export function useDeleteThreadQuery(options: UseDeleteThreadQueryOptions): UseQueryResult<DeleteThreadResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteThreadQueryKey(init),
        queryFn: () => deleteThread(client, init),
        ...queryOptions
    });
}

export interface UseDeleteThreadSuspenseQueryOptions extends DeleteThreadInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteThreadResponse, unknown>>;
}

/**
 * `DELETE /threads/{thread_id}`
 *
 * Delete a thread.
 */
export function useDeleteThreadSuspenseQuery(options: UseDeleteThreadSuspenseQueryOptions): UseSuspenseQueryResult<DeleteThreadResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteThreadQueryKey(init),
        queryFn: () => deleteThread(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /threads/{thread_id}`
 *
 * Delete a thread.
 */
export function useDeleteThreadMutation(options: Partial<UseMutationOptions<DeleteThreadResponse, unknown, DeleteThreadInit>> = {}): UseMutationResult<DeleteThreadResponse, unknown, DeleteThreadInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteThreadInit) => deleteThread(client, init),
        ...options
    });
}

export interface ListMessagesInit {
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) the messages belong to.
         */
        thread_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
        /**
         * Filter messages by the run ID that generated them.
         *
         */
        run_id?: string;
    };
    request?: RequestInit;
}

function listMessages(client: OpenAPIClient, init: ListMessagesInit): Promise<ListMessagesResponse> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/messages", init);
}

export function listMessagesQueryKey(init: ListMessagesInit): QueryKey {
    return ["get", "/threads/{thread_id}/messages", init];
}

export interface PrefetchListMessagesOptions extends ListMessagesInit {
    queryOptions?: Partial<FetchQueryOptions<ListMessagesResponse, unknown>>;
}

export function prefetchListMessages(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListMessagesOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listMessagesQueryKey(init),
        queryFn: () => listMessages(client, init),
        ...queryOptions
    });
}

export interface UseListMessagesQueryOptions extends ListMessagesInit {
    queryOptions?: Partial<UseQueryOptions<ListMessagesResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/messages`
 *
 * Returns a list of messages for a given thread.
 */
export function useListMessagesQuery(options: UseListMessagesQueryOptions): UseQueryResult<ListMessagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listMessagesQueryKey(init),
        queryFn: () => listMessages(client, init),
        ...queryOptions
    });
}

export interface UseListMessagesSuspenseQueryOptions extends ListMessagesInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListMessagesResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/messages`
 *
 * Returns a list of messages for a given thread.
 */
export function useListMessagesSuspenseQuery(options: UseListMessagesSuspenseQueryOptions): UseSuspenseQueryResult<ListMessagesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listMessagesQueryKey(init),
        queryFn: () => listMessages(client, init),
        ...queryOptions
    });
}

export interface CreateMessageInit {
    body: CreateMessageRequest;
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) to create a message for.
         */
        thread_id: string;
    };
    request?: RequestInit;
}

function createMessage(client: OpenAPIClient, init: CreateMessageInit): Promise<MessageObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/messages", init, "application/json");
}

export function createMessageQueryKey(init: CreateMessageInit): QueryKey {
    return ["post", "/threads/{thread_id}/messages", init];
}

export interface PrefetchCreateMessageOptions extends CreateMessageInit {
    queryOptions?: Partial<FetchQueryOptions<MessageObject, unknown>>;
}

export function prefetchCreateMessage(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateMessageOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createMessageQueryKey(init),
        queryFn: () => createMessage(client, init),
        ...queryOptions
    });
}

export interface UseCreateMessageQueryOptions extends CreateMessageInit {
    queryOptions?: Partial<UseQueryOptions<MessageObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/messages`
 *
 * Create a message.
 */
export function useCreateMessageQuery(options: UseCreateMessageQueryOptions): UseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createMessageQueryKey(init),
        queryFn: () => createMessage(client, init),
        ...queryOptions
    });
}

export interface UseCreateMessageSuspenseQueryOptions extends CreateMessageInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<MessageObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/messages`
 *
 * Create a message.
 */
export function useCreateMessageSuspenseQuery(options: UseCreateMessageSuspenseQueryOptions): UseSuspenseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createMessageQueryKey(init),
        queryFn: () => createMessage(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/messages`
 *
 * Create a message.
 */
export function useCreateMessageMutation(options: Partial<UseMutationOptions<MessageObject, unknown, CreateMessageInit>> = {}): UseMutationResult<MessageObject, unknown, CreateMessageInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateMessageInit) => createMessage(client, init),
        ...options
    });
}

export interface GetMessageInit {
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) to which this message belongs.
         */
        thread_id: string;
        /**
         * The ID of the message to retrieve.
         */
        message_id: string;
    };
    request?: RequestInit;
}

function getMessage(client: OpenAPIClient, init: GetMessageInit): Promise<MessageObject> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/messages/{message_id}", init);
}

export function getMessageQueryKey(init: GetMessageInit): QueryKey {
    return ["get", "/threads/{thread_id}/messages/{message_id}", init];
}

export interface PrefetchGetMessageOptions extends GetMessageInit {
    queryOptions?: Partial<FetchQueryOptions<MessageObject, unknown>>;
}

export function prefetchGetMessage(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetMessageOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getMessageQueryKey(init),
        queryFn: () => getMessage(client, init),
        ...queryOptions
    });
}

export interface UseGetMessageQueryOptions extends GetMessageInit {
    queryOptions?: Partial<UseQueryOptions<MessageObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/messages/{message_id}`
 *
 * Retrieve a message.
 */
export function useGetMessageQuery(options: UseGetMessageQueryOptions): UseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getMessageQueryKey(init),
        queryFn: () => getMessage(client, init),
        ...queryOptions
    });
}

export interface UseGetMessageSuspenseQueryOptions extends GetMessageInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<MessageObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/messages/{message_id}`
 *
 * Retrieve a message.
 */
export function useGetMessageSuspenseQuery(options: UseGetMessageSuspenseQueryOptions): UseSuspenseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getMessageQueryKey(init),
        queryFn: () => getMessage(client, init),
        ...queryOptions
    });
}

export interface ModifyMessageInit {
    body: ModifyMessageRequest;
    path: {
        /**
         * The ID of the thread to which this message belongs.
         */
        thread_id: string;
        /**
         * The ID of the message to modify.
         */
        message_id: string;
    };
    request?: RequestInit;
}

function modifyMessage(client: OpenAPIClient, init: ModifyMessageInit): Promise<MessageObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/messages/{message_id}", init, "application/json");
}

export function modifyMessageQueryKey(init: ModifyMessageInit): QueryKey {
    return ["post", "/threads/{thread_id}/messages/{message_id}", init];
}

export interface PrefetchModifyMessageOptions extends ModifyMessageInit {
    queryOptions?: Partial<FetchQueryOptions<MessageObject, unknown>>;
}

export function prefetchModifyMessage(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyMessageOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyMessageQueryKey(init),
        queryFn: () => modifyMessage(client, init),
        ...queryOptions
    });
}

export interface UseModifyMessageQueryOptions extends ModifyMessageInit {
    queryOptions?: Partial<UseQueryOptions<MessageObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/messages/{message_id}`
 *
 * Modifies a message.
 */
export function useModifyMessageQuery(options: UseModifyMessageQueryOptions): UseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyMessageQueryKey(init),
        queryFn: () => modifyMessage(client, init),
        ...queryOptions
    });
}

export interface UseModifyMessageSuspenseQueryOptions extends ModifyMessageInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<MessageObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/messages/{message_id}`
 *
 * Modifies a message.
 */
export function useModifyMessageSuspenseQuery(options: UseModifyMessageSuspenseQueryOptions): UseSuspenseQueryResult<MessageObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyMessageQueryKey(init),
        queryFn: () => modifyMessage(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/messages/{message_id}`
 *
 * Modifies a message.
 */
export function useModifyMessageMutation(options: Partial<UseMutationOptions<MessageObject, unknown, ModifyMessageInit>> = {}): UseMutationResult<MessageObject, unknown, ModifyMessageInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyMessageInit) => modifyMessage(client, init),
        ...options
    });
}

export interface DeleteMessageInit {
    path: {
        /**
         * The ID of the thread to which this message belongs.
         */
        thread_id: string;
        /**
         * The ID of the message to delete.
         */
        message_id: string;
    };
    request?: RequestInit;
}

function deleteMessage(client: OpenAPIClient, init: DeleteMessageInit): Promise<DeleteMessageResponse> {
    return client[internal_fetch]("DELETE", "/threads/{thread_id}/messages/{message_id}", init);
}

export function deleteMessageQueryKey(init: DeleteMessageInit): QueryKey {
    return ["delete", "/threads/{thread_id}/messages/{message_id}", init];
}

export interface PrefetchDeleteMessageOptions extends DeleteMessageInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteMessageResponse, unknown>>;
}

export function prefetchDeleteMessage(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteMessageOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteMessageQueryKey(init),
        queryFn: () => deleteMessage(client, init),
        ...queryOptions
    });
}

export interface UseDeleteMessageQueryOptions extends DeleteMessageInit {
    queryOptions?: Partial<UseQueryOptions<DeleteMessageResponse, unknown>>;
}

/**
 * `DELETE /threads/{thread_id}/messages/{message_id}`
 *
 * Deletes a message.
 */
export function useDeleteMessageQuery(options: UseDeleteMessageQueryOptions): UseQueryResult<DeleteMessageResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteMessageQueryKey(init),
        queryFn: () => deleteMessage(client, init),
        ...queryOptions
    });
}

export interface UseDeleteMessageSuspenseQueryOptions extends DeleteMessageInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteMessageResponse, unknown>>;
}

/**
 * `DELETE /threads/{thread_id}/messages/{message_id}`
 *
 * Deletes a message.
 */
export function useDeleteMessageSuspenseQuery(options: UseDeleteMessageSuspenseQueryOptions): UseSuspenseQueryResult<DeleteMessageResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteMessageQueryKey(init),
        queryFn: () => deleteMessage(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /threads/{thread_id}/messages/{message_id}`
 *
 * Deletes a message.
 */
export function useDeleteMessageMutation(options: Partial<UseMutationOptions<DeleteMessageResponse, unknown, DeleteMessageInit>> = {}): UseMutationResult<DeleteMessageResponse, unknown, DeleteMessageInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteMessageInit) => deleteMessage(client, init),
        ...options
    });
}

export interface CreateThreadAndRunInit {
    body: CreateThreadAndRunRequest;
    request?: RequestInit;
}

function createThreadAndRun(client: OpenAPIClient, init: CreateThreadAndRunInit): Promise<RunObject> {
    return client[internal_fetch]("POST", "/threads/runs", init, "application/json");
}

export function createThreadAndRunQueryKey(init: CreateThreadAndRunInit): QueryKey {
    return ["post", "/threads/runs", init];
}

export interface PrefetchCreateThreadAndRunOptions extends CreateThreadAndRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchCreateThreadAndRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateThreadAndRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createThreadAndRunQueryKey(init),
        queryFn: () => createThreadAndRun(client, init),
        ...queryOptions
    });
}

export interface UseCreateThreadAndRunQueryOptions extends CreateThreadAndRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/runs`
 *
 * Create a thread and run it in one request.
 */
export function useCreateThreadAndRunQuery(options: UseCreateThreadAndRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createThreadAndRunQueryKey(init),
        queryFn: () => createThreadAndRun(client, init),
        ...queryOptions
    });
}

export interface UseCreateThreadAndRunSuspenseQueryOptions extends CreateThreadAndRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/runs`
 *
 * Create a thread and run it in one request.
 */
export function useCreateThreadAndRunSuspenseQuery(options: UseCreateThreadAndRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createThreadAndRunQueryKey(init),
        queryFn: () => createThreadAndRun(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/runs`
 *
 * Create a thread and run it in one request.
 */
export function useCreateThreadAndRunMutation(options: Partial<UseMutationOptions<RunObject, unknown, CreateThreadAndRunInit>> = {}): UseMutationResult<RunObject, unknown, CreateThreadAndRunInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateThreadAndRunInit) => createThreadAndRun(client, init),
        ...options
    });
}

export interface ListRunsInit {
    path: {
        /**
         * The ID of the thread the run belongs to.
         */
        thread_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
    };
    request?: RequestInit;
}

function listRuns(client: OpenAPIClient, init: ListRunsInit): Promise<ListRunsResponse> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/runs", init);
}

export function listRunsQueryKey(init: ListRunsInit): QueryKey {
    return ["get", "/threads/{thread_id}/runs", init];
}

export interface PrefetchListRunsOptions extends ListRunsInit {
    queryOptions?: Partial<FetchQueryOptions<ListRunsResponse, unknown>>;
}

export function prefetchListRuns(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListRunsOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listRunsQueryKey(init),
        queryFn: () => listRuns(client, init),
        ...queryOptions
    });
}

export interface UseListRunsQueryOptions extends ListRunsInit {
    queryOptions?: Partial<UseQueryOptions<ListRunsResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs`
 *
 * Returns a list of runs belonging to a thread.
 */
export function useListRunsQuery(options: UseListRunsQueryOptions): UseQueryResult<ListRunsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listRunsQueryKey(init),
        queryFn: () => listRuns(client, init),
        ...queryOptions
    });
}

export interface UseListRunsSuspenseQueryOptions extends ListRunsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListRunsResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs`
 *
 * Returns a list of runs belonging to a thread.
 */
export function useListRunsSuspenseQuery(options: UseListRunsSuspenseQueryOptions): UseSuspenseQueryResult<ListRunsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listRunsQueryKey(init),
        queryFn: () => listRuns(client, init),
        ...queryOptions
    });
}

export interface CreateRunInit {
    body: CreateRunRequest;
    path: {
        /**
         * The ID of the thread to run.
         */
        thread_id: string;
    };
    request?: RequestInit;
}

function createRun(client: OpenAPIClient, init: CreateRunInit): Promise<RunObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/runs", init, "application/json");
}

export function createRunQueryKey(init: CreateRunInit): QueryKey {
    return ["post", "/threads/{thread_id}/runs", init];
}

export interface PrefetchCreateRunOptions extends CreateRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchCreateRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createRunQueryKey(init),
        queryFn: () => createRun(client, init),
        ...queryOptions
    });
}

export interface UseCreateRunQueryOptions extends CreateRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs`
 *
 * Create a run.
 */
export function useCreateRunQuery(options: UseCreateRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createRunQueryKey(init),
        queryFn: () => createRun(client, init),
        ...queryOptions
    });
}

export interface UseCreateRunSuspenseQueryOptions extends CreateRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs`
 *
 * Create a run.
 */
export function useCreateRunSuspenseQuery(options: UseCreateRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createRunQueryKey(init),
        queryFn: () => createRun(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/runs`
 *
 * Create a run.
 */
export function useCreateRunMutation(options: Partial<UseMutationOptions<RunObject, unknown, CreateRunInit>> = {}): UseMutationResult<RunObject, unknown, CreateRunInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateRunInit) => createRun(client, init),
        ...options
    });
}

export interface GetRunInit {
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) that was run.
         */
        thread_id: string;
        /**
         * The ID of the run to retrieve.
         */
        run_id: string;
    };
    request?: RequestInit;
}

function getRun(client: OpenAPIClient, init: GetRunInit): Promise<RunObject> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/runs/{run_id}", init);
}

export function getRunQueryKey(init: GetRunInit): QueryKey {
    return ["get", "/threads/{thread_id}/runs/{run_id}", init];
}

export interface PrefetchGetRunOptions extends GetRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchGetRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getRunQueryKey(init),
        queryFn: () => getRun(client, init),
        ...queryOptions
    });
}

export interface UseGetRunQueryOptions extends GetRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}`
 *
 * Retrieves a run.
 */
export function useGetRunQuery(options: UseGetRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getRunQueryKey(init),
        queryFn: () => getRun(client, init),
        ...queryOptions
    });
}

export interface UseGetRunSuspenseQueryOptions extends GetRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}`
 *
 * Retrieves a run.
 */
export function useGetRunSuspenseQuery(options: UseGetRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getRunQueryKey(init),
        queryFn: () => getRun(client, init),
        ...queryOptions
    });
}

export interface ModifyRunInit {
    body: ModifyRunRequest;
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) that was run.
         */
        thread_id: string;
        /**
         * The ID of the run to modify.
         */
        run_id: string;
    };
    request?: RequestInit;
}

function modifyRun(client: OpenAPIClient, init: ModifyRunInit): Promise<RunObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/runs/{run_id}", init, "application/json");
}

export function modifyRunQueryKey(init: ModifyRunInit): QueryKey {
    return ["post", "/threads/{thread_id}/runs/{run_id}", init];
}

export interface PrefetchModifyRunOptions extends ModifyRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchModifyRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyRunQueryKey(init),
        queryFn: () => modifyRun(client, init),
        ...queryOptions
    });
}

export interface UseModifyRunQueryOptions extends ModifyRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}`
 *
 * Modifies a run.
 */
export function useModifyRunQuery(options: UseModifyRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyRunQueryKey(init),
        queryFn: () => modifyRun(client, init),
        ...queryOptions
    });
}

export interface UseModifyRunSuspenseQueryOptions extends ModifyRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}`
 *
 * Modifies a run.
 */
export function useModifyRunSuspenseQuery(options: UseModifyRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyRunQueryKey(init),
        queryFn: () => modifyRun(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}`
 *
 * Modifies a run.
 */
export function useModifyRunMutation(options: Partial<UseMutationOptions<RunObject, unknown, ModifyRunInit>> = {}): UseMutationResult<RunObject, unknown, ModifyRunInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyRunInit) => modifyRun(client, init),
        ...options
    });
}

export interface SubmitToolOuputsToRunInit {
    body: SubmitToolOutputsRunRequest;
    path: {
        /**
         * The ID of the [thread](/docs/api-reference/threads) to which this run belongs.
         */
        thread_id: string;
        /**
         * The ID of the run that requires the tool output submission.
         */
        run_id: string;
    };
    request?: RequestInit;
}

function submitToolOuputsToRun(client: OpenAPIClient, init: SubmitToolOuputsToRunInit): Promise<RunObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/runs/{run_id}/submit_tool_outputs", init, "application/json");
}

export function submitToolOuputsToRunQueryKey(init: SubmitToolOuputsToRunInit): QueryKey {
    return ["post", "/threads/{thread_id}/runs/{run_id}/submit_tool_outputs", init];
}

export interface PrefetchSubmitToolOuputsToRunOptions extends SubmitToolOuputsToRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchSubmitToolOuputsToRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchSubmitToolOuputsToRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: submitToolOuputsToRunQueryKey(init),
        queryFn: () => submitToolOuputsToRun(client, init),
        ...queryOptions
    });
}

export interface UseSubmitToolOuputsToRunQueryOptions extends SubmitToolOuputsToRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs`
 *
 * When a run has the `status: "requires_action"` and `required_action.type` is `submit_tool_outputs`, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.
 *
 */
export function useSubmitToolOuputsToRunQuery(options: UseSubmitToolOuputsToRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: submitToolOuputsToRunQueryKey(init),
        queryFn: () => submitToolOuputsToRun(client, init),
        ...queryOptions
    });
}

export interface UseSubmitToolOuputsToRunSuspenseQueryOptions extends SubmitToolOuputsToRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs`
 *
 * When a run has the `status: "requires_action"` and `required_action.type` is `submit_tool_outputs`, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.
 *
 */
export function useSubmitToolOuputsToRunSuspenseQuery(options: UseSubmitToolOuputsToRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: submitToolOuputsToRunQueryKey(init),
        queryFn: () => submitToolOuputsToRun(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/submit_tool_outputs`
 *
 * When a run has the `status: "requires_action"` and `required_action.type` is `submit_tool_outputs`, this endpoint can be used to submit the outputs from the tool calls once they're all completed. All outputs must be submitted in a single request.
 *
 */
export function useSubmitToolOuputsToRunMutation(options: Partial<UseMutationOptions<RunObject, unknown, SubmitToolOuputsToRunInit>> = {}): UseMutationResult<RunObject, unknown, SubmitToolOuputsToRunInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: SubmitToolOuputsToRunInit) => submitToolOuputsToRun(client, init),
        ...options
    });
}

export interface CancelRunInit {
    path: {
        /**
         * The ID of the thread to which this run belongs.
         */
        thread_id: string;
        /**
         * The ID of the run to cancel.
         */
        run_id: string;
    };
    request?: RequestInit;
}

function cancelRun(client: OpenAPIClient, init: CancelRunInit): Promise<RunObject> {
    return client[internal_fetch]("POST", "/threads/{thread_id}/runs/{run_id}/cancel", init);
}

export function cancelRunQueryKey(init: CancelRunInit): QueryKey {
    return ["post", "/threads/{thread_id}/runs/{run_id}/cancel", init];
}

export interface PrefetchCancelRunOptions extends CancelRunInit {
    queryOptions?: Partial<FetchQueryOptions<RunObject, unknown>>;
}

export function prefetchCancelRun(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCancelRunOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: cancelRunQueryKey(init),
        queryFn: () => cancelRun(client, init),
        ...queryOptions
    });
}

export interface UseCancelRunQueryOptions extends CancelRunInit {
    queryOptions?: Partial<UseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/cancel`
 *
 * Cancels a run that is `in_progress`.
 */
export function useCancelRunQuery(options: UseCancelRunQueryOptions): UseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: cancelRunQueryKey(init),
        queryFn: () => cancelRun(client, init),
        ...queryOptions
    });
}

export interface UseCancelRunSuspenseQueryOptions extends CancelRunInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunObject, unknown>>;
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/cancel`
 *
 * Cancels a run that is `in_progress`.
 */
export function useCancelRunSuspenseQuery(options: UseCancelRunSuspenseQueryOptions): UseSuspenseQueryResult<RunObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: cancelRunQueryKey(init),
        queryFn: () => cancelRun(client, init),
        ...queryOptions
    });
}

/**
 * `POST /threads/{thread_id}/runs/{run_id}/cancel`
 *
 * Cancels a run that is `in_progress`.
 */
export function useCancelRunMutation(options: Partial<UseMutationOptions<RunObject, unknown, CancelRunInit>> = {}): UseMutationResult<RunObject, unknown, CancelRunInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CancelRunInit) => cancelRun(client, init),
        ...options
    });
}

export interface ListRunStepsInit {
    path: {
        /**
         * The ID of the thread the run and run steps belong to.
         */
        thread_id: string;
        /**
         * The ID of the run the run steps belong to.
         */
        run_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
    };
    request?: RequestInit;
}

function listRunSteps(client: OpenAPIClient, init: ListRunStepsInit): Promise<ListRunStepsResponse> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/runs/{run_id}/steps", init);
}

export function listRunStepsQueryKey(init: ListRunStepsInit): QueryKey {
    return ["get", "/threads/{thread_id}/runs/{run_id}/steps", init];
}

export interface PrefetchListRunStepsOptions extends ListRunStepsInit {
    queryOptions?: Partial<FetchQueryOptions<ListRunStepsResponse, unknown>>;
}

export function prefetchListRunSteps(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListRunStepsOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listRunStepsQueryKey(init),
        queryFn: () => listRunSteps(client, init),
        ...queryOptions
    });
}

export interface UseListRunStepsQueryOptions extends ListRunStepsInit {
    queryOptions?: Partial<UseQueryOptions<ListRunStepsResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}/steps`
 *
 * Returns a list of run steps belonging to a run.
 */
export function useListRunStepsQuery(options: UseListRunStepsQueryOptions): UseQueryResult<ListRunStepsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listRunStepsQueryKey(init),
        queryFn: () => listRunSteps(client, init),
        ...queryOptions
    });
}

export interface UseListRunStepsSuspenseQueryOptions extends ListRunStepsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListRunStepsResponse, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}/steps`
 *
 * Returns a list of run steps belonging to a run.
 */
export function useListRunStepsSuspenseQuery(options: UseListRunStepsSuspenseQueryOptions): UseSuspenseQueryResult<ListRunStepsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listRunStepsQueryKey(init),
        queryFn: () => listRunSteps(client, init),
        ...queryOptions
    });
}

export interface GetRunStepInit {
    path: {
        /**
         * The ID of the thread to which the run and run step belongs.
         */
        thread_id: string;
        /**
         * The ID of the run to which the run step belongs.
         */
        run_id: string;
        /**
         * The ID of the run step to retrieve.
         */
        step_id: string;
    };
    request?: RequestInit;
}

function getRunStep(client: OpenAPIClient, init: GetRunStepInit): Promise<RunStepObject> {
    return client[internal_fetch]("GET", "/threads/{thread_id}/runs/{run_id}/steps/{step_id}", init);
}

export function getRunStepQueryKey(init: GetRunStepInit): QueryKey {
    return ["get", "/threads/{thread_id}/runs/{run_id}/steps/{step_id}", init];
}

export interface PrefetchGetRunStepOptions extends GetRunStepInit {
    queryOptions?: Partial<FetchQueryOptions<RunStepObject, unknown>>;
}

export function prefetchGetRunStep(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetRunStepOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getRunStepQueryKey(init),
        queryFn: () => getRunStep(client, init),
        ...queryOptions
    });
}

export interface UseGetRunStepQueryOptions extends GetRunStepInit {
    queryOptions?: Partial<UseQueryOptions<RunStepObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}/steps/{step_id}`
 *
 * Retrieves a run step.
 */
export function useGetRunStepQuery(options: UseGetRunStepQueryOptions): UseQueryResult<RunStepObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getRunStepQueryKey(init),
        queryFn: () => getRunStep(client, init),
        ...queryOptions
    });
}

export interface UseGetRunStepSuspenseQueryOptions extends GetRunStepInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<RunStepObject, unknown>>;
}

/**
 * `GET /threads/{thread_id}/runs/{run_id}/steps/{step_id}`
 *
 * Retrieves a run step.
 */
export function useGetRunStepSuspenseQuery(options: UseGetRunStepSuspenseQueryOptions): UseSuspenseQueryResult<RunStepObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getRunStepQueryKey(init),
        queryFn: () => getRunStep(client, init),
        ...queryOptions
    });
}

export interface ListVectorStoresInit {
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
    };
    request?: RequestInit;
}

function listVectorStores(client: OpenAPIClient, init: ListVectorStoresInit = {}): Promise<ListVectorStoresResponse> {
    return client[internal_fetch]("GET", "/vector_stores", init);
}

export function listVectorStoresQueryKey(init: ListVectorStoresInit = {}): QueryKey {
    return ["get", "/vector_stores", init];
}

export interface PrefetchListVectorStoresOptions extends ListVectorStoresInit {
    queryOptions?: Partial<FetchQueryOptions<ListVectorStoresResponse, unknown>>;
}

export function prefetchListVectorStores(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListVectorStoresOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listVectorStoresQueryKey(init),
        queryFn: () => listVectorStores(client, init),
        ...queryOptions
    });
}

export interface UseListVectorStoresQueryOptions extends ListVectorStoresInit {
    queryOptions?: Partial<UseQueryOptions<ListVectorStoresResponse, unknown>>;
}

/**
 * `GET /vector_stores`
 *
 * Returns a list of vector stores.
 */
export function useListVectorStoresQuery(options: UseListVectorStoresQueryOptions = {}): UseQueryResult<ListVectorStoresResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listVectorStoresQueryKey(init),
        queryFn: () => listVectorStores(client, init),
        ...queryOptions
    });
}

export interface UseListVectorStoresSuspenseQueryOptions extends ListVectorStoresInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListVectorStoresResponse, unknown>>;
}

/**
 * `GET /vector_stores`
 *
 * Returns a list of vector stores.
 */
export function useListVectorStoresSuspenseQuery(options: UseListVectorStoresSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListVectorStoresResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listVectorStoresQueryKey(init),
        queryFn: () => listVectorStores(client, init),
        ...queryOptions
    });
}

export interface CreateVectorStoreInit {
    body: CreateVectorStoreRequest;
    request?: RequestInit;
}

function createVectorStore(client: OpenAPIClient, init: CreateVectorStoreInit): Promise<VectorStoreObject> {
    return client[internal_fetch]("POST", "/vector_stores", init, "application/json");
}

export function createVectorStoreQueryKey(init: CreateVectorStoreInit): QueryKey {
    return ["post", "/vector_stores", init];
}

export interface PrefetchCreateVectorStoreOptions extends CreateVectorStoreInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreObject, unknown>>;
}

export function prefetchCreateVectorStore(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateVectorStoreOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createVectorStoreQueryKey(init),
        queryFn: () => createVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreQueryOptions extends CreateVectorStoreInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `POST /vector_stores`
 *
 * Create a vector store.
 */
export function useCreateVectorStoreQuery(options: UseCreateVectorStoreQueryOptions): UseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createVectorStoreQueryKey(init),
        queryFn: () => createVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreSuspenseQueryOptions extends CreateVectorStoreInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `POST /vector_stores`
 *
 * Create a vector store.
 */
export function useCreateVectorStoreSuspenseQuery(options: UseCreateVectorStoreSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createVectorStoreQueryKey(init),
        queryFn: () => createVectorStore(client, init),
        ...queryOptions
    });
}

/**
 * `POST /vector_stores`
 *
 * Create a vector store.
 */
export function useCreateVectorStoreMutation(options: Partial<UseMutationOptions<VectorStoreObject, unknown, CreateVectorStoreInit>> = {}): UseMutationResult<VectorStoreObject, unknown, CreateVectorStoreInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateVectorStoreInit) => createVectorStore(client, init),
        ...options
    });
}

export interface GetVectorStoreInit {
    path: {
        /**
         * The ID of the vector store to retrieve.
         */
        vector_store_id: string;
    };
    request?: RequestInit;
}

function getVectorStore(client: OpenAPIClient, init: GetVectorStoreInit): Promise<VectorStoreObject> {
    return client[internal_fetch]("GET", "/vector_stores/{vector_store_id}", init);
}

export function getVectorStoreQueryKey(init: GetVectorStoreInit): QueryKey {
    return ["get", "/vector_stores/{vector_store_id}", init];
}

export interface PrefetchGetVectorStoreOptions extends GetVectorStoreInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreObject, unknown>>;
}

export function prefetchGetVectorStore(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetVectorStoreOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getVectorStoreQueryKey(init),
        queryFn: () => getVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreQueryOptions extends GetVectorStoreInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}`
 *
 * Retrieves a vector store.
 */
export function useGetVectorStoreQuery(options: UseGetVectorStoreQueryOptions): UseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getVectorStoreQueryKey(init),
        queryFn: () => getVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreSuspenseQueryOptions extends GetVectorStoreInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}`
 *
 * Retrieves a vector store.
 */
export function useGetVectorStoreSuspenseQuery(options: UseGetVectorStoreSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getVectorStoreQueryKey(init),
        queryFn: () => getVectorStore(client, init),
        ...queryOptions
    });
}

export interface ModifyVectorStoreInit {
    body: UpdateVectorStoreRequest;
    path: {
        /**
         * The ID of the vector store to modify.
         */
        vector_store_id: string;
    };
    request?: RequestInit;
}

function modifyVectorStore(client: OpenAPIClient, init: ModifyVectorStoreInit): Promise<VectorStoreObject> {
    return client[internal_fetch]("POST", "/vector_stores/{vector_store_id}", init, "application/json");
}

export function modifyVectorStoreQueryKey(init: ModifyVectorStoreInit): QueryKey {
    return ["post", "/vector_stores/{vector_store_id}", init];
}

export interface PrefetchModifyVectorStoreOptions extends ModifyVectorStoreInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreObject, unknown>>;
}

export function prefetchModifyVectorStore(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyVectorStoreOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyVectorStoreQueryKey(init),
        queryFn: () => modifyVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseModifyVectorStoreQueryOptions extends ModifyVectorStoreInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}`
 *
 * Modifies a vector store.
 */
export function useModifyVectorStoreQuery(options: UseModifyVectorStoreQueryOptions): UseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyVectorStoreQueryKey(init),
        queryFn: () => modifyVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseModifyVectorStoreSuspenseQueryOptions extends ModifyVectorStoreInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}`
 *
 * Modifies a vector store.
 */
export function useModifyVectorStoreSuspenseQuery(options: UseModifyVectorStoreSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyVectorStoreQueryKey(init),
        queryFn: () => modifyVectorStore(client, init),
        ...queryOptions
    });
}

/**
 * `POST /vector_stores/{vector_store_id}`
 *
 * Modifies a vector store.
 */
export function useModifyVectorStoreMutation(options: Partial<UseMutationOptions<VectorStoreObject, unknown, ModifyVectorStoreInit>> = {}): UseMutationResult<VectorStoreObject, unknown, ModifyVectorStoreInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyVectorStoreInit) => modifyVectorStore(client, init),
        ...options
    });
}

export interface DeleteVectorStoreInit {
    path: {
        /**
         * The ID of the vector store to delete.
         */
        vector_store_id: string;
    };
    request?: RequestInit;
}

function deleteVectorStore(client: OpenAPIClient, init: DeleteVectorStoreInit): Promise<DeleteVectorStoreResponse> {
    return client[internal_fetch]("DELETE", "/vector_stores/{vector_store_id}", init);
}

export function deleteVectorStoreQueryKey(init: DeleteVectorStoreInit): QueryKey {
    return ["delete", "/vector_stores/{vector_store_id}", init];
}

export interface PrefetchDeleteVectorStoreOptions extends DeleteVectorStoreInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteVectorStoreResponse, unknown>>;
}

export function prefetchDeleteVectorStore(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteVectorStoreOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteVectorStoreQueryKey(init),
        queryFn: () => deleteVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseDeleteVectorStoreQueryOptions extends DeleteVectorStoreInit {
    queryOptions?: Partial<UseQueryOptions<DeleteVectorStoreResponse, unknown>>;
}

/**
 * `DELETE /vector_stores/{vector_store_id}`
 *
 * Delete a vector store.
 */
export function useDeleteVectorStoreQuery(options: UseDeleteVectorStoreQueryOptions): UseQueryResult<DeleteVectorStoreResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteVectorStoreQueryKey(init),
        queryFn: () => deleteVectorStore(client, init),
        ...queryOptions
    });
}

export interface UseDeleteVectorStoreSuspenseQueryOptions extends DeleteVectorStoreInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteVectorStoreResponse, unknown>>;
}

/**
 * `DELETE /vector_stores/{vector_store_id}`
 *
 * Delete a vector store.
 */
export function useDeleteVectorStoreSuspenseQuery(options: UseDeleteVectorStoreSuspenseQueryOptions): UseSuspenseQueryResult<DeleteVectorStoreResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteVectorStoreQueryKey(init),
        queryFn: () => deleteVectorStore(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /vector_stores/{vector_store_id}`
 *
 * Delete a vector store.
 */
export function useDeleteVectorStoreMutation(options: Partial<UseMutationOptions<DeleteVectorStoreResponse, unknown, DeleteVectorStoreInit>> = {}): UseMutationResult<DeleteVectorStoreResponse, unknown, DeleteVectorStoreInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteVectorStoreInit) => deleteVectorStore(client, init),
        ...options
    });
}

export interface ListVectorStoreFilesInit {
    path: {
        /**
         * The ID of the vector store that the files belong to.
         */
        vector_store_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
        /**
         * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
         */
        filter?: "in_progress" | "completed" | "failed" | "cancelled";
    };
    request?: RequestInit;
}

function listVectorStoreFiles(client: OpenAPIClient, init: ListVectorStoreFilesInit): Promise<ListVectorStoreFilesResponse> {
    return client[internal_fetch]("GET", "/vector_stores/{vector_store_id}/files", init);
}

export function listVectorStoreFilesQueryKey(init: ListVectorStoreFilesInit): QueryKey {
    return ["get", "/vector_stores/{vector_store_id}/files", init];
}

export interface PrefetchListVectorStoreFilesOptions extends ListVectorStoreFilesInit {
    queryOptions?: Partial<FetchQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

export function prefetchListVectorStoreFiles(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListVectorStoreFilesOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listVectorStoreFilesQueryKey(init),
        queryFn: () => listVectorStoreFiles(client, init),
        ...queryOptions
    });
}

export interface UseListVectorStoreFilesQueryOptions extends ListVectorStoreFilesInit {
    queryOptions?: Partial<UseQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/files`
 *
 * Returns a list of vector store files.
 */
export function useListVectorStoreFilesQuery(options: UseListVectorStoreFilesQueryOptions): UseQueryResult<ListVectorStoreFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listVectorStoreFilesQueryKey(init),
        queryFn: () => listVectorStoreFiles(client, init),
        ...queryOptions
    });
}

export interface UseListVectorStoreFilesSuspenseQueryOptions extends ListVectorStoreFilesInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/files`
 *
 * Returns a list of vector store files.
 */
export function useListVectorStoreFilesSuspenseQuery(options: UseListVectorStoreFilesSuspenseQueryOptions): UseSuspenseQueryResult<ListVectorStoreFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listVectorStoreFilesQueryKey(init),
        queryFn: () => listVectorStoreFiles(client, init),
        ...queryOptions
    });
}

export interface CreateVectorStoreFileInit {
    body: CreateVectorStoreFileRequest;
    path: {
        /**
         * The ID of the vector store for which to create a File.
         *
         */
        vector_store_id: string;
    };
    request?: RequestInit;
}

function createVectorStoreFile(client: OpenAPIClient, init: CreateVectorStoreFileInit): Promise<VectorStoreFileObject> {
    return client[internal_fetch]("POST", "/vector_stores/{vector_store_id}/files", init, "application/json");
}

export function createVectorStoreFileQueryKey(init: CreateVectorStoreFileInit): QueryKey {
    return ["post", "/vector_stores/{vector_store_id}/files", init];
}

export interface PrefetchCreateVectorStoreFileOptions extends CreateVectorStoreFileInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreFileObject, unknown>>;
}

export function prefetchCreateVectorStoreFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateVectorStoreFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createVectorStoreFileQueryKey(init),
        queryFn: () => createVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreFileQueryOptions extends CreateVectorStoreFileInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreFileObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/files`
 *
 * Create a vector store file by attaching a [File](/docs/api-reference/files) to a [vector store](/docs/api-reference/vector-stores/object).
 */
export function useCreateVectorStoreFileQuery(options: UseCreateVectorStoreFileQueryOptions): UseQueryResult<VectorStoreFileObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createVectorStoreFileQueryKey(init),
        queryFn: () => createVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreFileSuspenseQueryOptions extends CreateVectorStoreFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreFileObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/files`
 *
 * Create a vector store file by attaching a [File](/docs/api-reference/files) to a [vector store](/docs/api-reference/vector-stores/object).
 */
export function useCreateVectorStoreFileSuspenseQuery(options: UseCreateVectorStoreFileSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreFileObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createVectorStoreFileQueryKey(init),
        queryFn: () => createVectorStoreFile(client, init),
        ...queryOptions
    });
}

/**
 * `POST /vector_stores/{vector_store_id}/files`
 *
 * Create a vector store file by attaching a [File](/docs/api-reference/files) to a [vector store](/docs/api-reference/vector-stores/object).
 */
export function useCreateVectorStoreFileMutation(options: Partial<UseMutationOptions<VectorStoreFileObject, unknown, CreateVectorStoreFileInit>> = {}): UseMutationResult<VectorStoreFileObject, unknown, CreateVectorStoreFileInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateVectorStoreFileInit) => createVectorStoreFile(client, init),
        ...options
    });
}

export interface GetVectorStoreFileInit {
    path: {
        /**
         * The ID of the vector store that the file belongs to.
         */
        vector_store_id: string;
        /**
         * The ID of the file being retrieved.
         */
        file_id: string;
    };
    request?: RequestInit;
}

function getVectorStoreFile(client: OpenAPIClient, init: GetVectorStoreFileInit): Promise<VectorStoreFileObject> {
    return client[internal_fetch]("GET", "/vector_stores/{vector_store_id}/files/{file_id}", init);
}

export function getVectorStoreFileQueryKey(init: GetVectorStoreFileInit): QueryKey {
    return ["get", "/vector_stores/{vector_store_id}/files/{file_id}", init];
}

export interface PrefetchGetVectorStoreFileOptions extends GetVectorStoreFileInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreFileObject, unknown>>;
}

export function prefetchGetVectorStoreFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetVectorStoreFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getVectorStoreFileQueryKey(init),
        queryFn: () => getVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreFileQueryOptions extends GetVectorStoreFileInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreFileObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/files/{file_id}`
 *
 * Retrieves a vector store file.
 */
export function useGetVectorStoreFileQuery(options: UseGetVectorStoreFileQueryOptions): UseQueryResult<VectorStoreFileObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getVectorStoreFileQueryKey(init),
        queryFn: () => getVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreFileSuspenseQueryOptions extends GetVectorStoreFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreFileObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/files/{file_id}`
 *
 * Retrieves a vector store file.
 */
export function useGetVectorStoreFileSuspenseQuery(options: UseGetVectorStoreFileSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreFileObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getVectorStoreFileQueryKey(init),
        queryFn: () => getVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface DeleteVectorStoreFileInit {
    path: {
        /**
         * The ID of the vector store that the file belongs to.
         */
        vector_store_id: string;
        /**
         * The ID of the file to delete.
         */
        file_id: string;
    };
    request?: RequestInit;
}

function deleteVectorStoreFile(client: OpenAPIClient, init: DeleteVectorStoreFileInit): Promise<DeleteVectorStoreFileResponse> {
    return client[internal_fetch]("DELETE", "/vector_stores/{vector_store_id}/files/{file_id}", init);
}

export function deleteVectorStoreFileQueryKey(init: DeleteVectorStoreFileInit): QueryKey {
    return ["delete", "/vector_stores/{vector_store_id}/files/{file_id}", init];
}

export interface PrefetchDeleteVectorStoreFileOptions extends DeleteVectorStoreFileInit {
    queryOptions?: Partial<FetchQueryOptions<DeleteVectorStoreFileResponse, unknown>>;
}

export function prefetchDeleteVectorStoreFile(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteVectorStoreFileOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteVectorStoreFileQueryKey(init),
        queryFn: () => deleteVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseDeleteVectorStoreFileQueryOptions extends DeleteVectorStoreFileInit {
    queryOptions?: Partial<UseQueryOptions<DeleteVectorStoreFileResponse, unknown>>;
}

/**
 * `DELETE /vector_stores/{vector_store_id}/files/{file_id}`
 *
 * Delete a vector store file. This will remove the file from the vector store but the file itself will not be deleted. To delete the file, use the [delete file](/docs/api-reference/files/delete) endpoint.
 */
export function useDeleteVectorStoreFileQuery(options: UseDeleteVectorStoreFileQueryOptions): UseQueryResult<DeleteVectorStoreFileResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteVectorStoreFileQueryKey(init),
        queryFn: () => deleteVectorStoreFile(client, init),
        ...queryOptions
    });
}

export interface UseDeleteVectorStoreFileSuspenseQueryOptions extends DeleteVectorStoreFileInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<DeleteVectorStoreFileResponse, unknown>>;
}

/**
 * `DELETE /vector_stores/{vector_store_id}/files/{file_id}`
 *
 * Delete a vector store file. This will remove the file from the vector store but the file itself will not be deleted. To delete the file, use the [delete file](/docs/api-reference/files/delete) endpoint.
 */
export function useDeleteVectorStoreFileSuspenseQuery(options: UseDeleteVectorStoreFileSuspenseQueryOptions): UseSuspenseQueryResult<DeleteVectorStoreFileResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteVectorStoreFileQueryKey(init),
        queryFn: () => deleteVectorStoreFile(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /vector_stores/{vector_store_id}/files/{file_id}`
 *
 * Delete a vector store file. This will remove the file from the vector store but the file itself will not be deleted. To delete the file, use the [delete file](/docs/api-reference/files/delete) endpoint.
 */
export function useDeleteVectorStoreFileMutation(options: Partial<UseMutationOptions<DeleteVectorStoreFileResponse, unknown, DeleteVectorStoreFileInit>> = {}): UseMutationResult<DeleteVectorStoreFileResponse, unknown, DeleteVectorStoreFileInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteVectorStoreFileInit) => deleteVectorStoreFile(client, init),
        ...options
    });
}

export interface CreateVectorStoreFileBatchInit {
    body: CreateVectorStoreFileBatchRequest;
    path: {
        /**
         * The ID of the vector store for which to create a File Batch.
         *
         */
        vector_store_id: string;
    };
    request?: RequestInit;
}

function createVectorStoreFileBatch(client: OpenAPIClient, init: CreateVectorStoreFileBatchInit): Promise<VectorStoreFileBatchObject> {
    return client[internal_fetch]("POST", "/vector_stores/{vector_store_id}/file_batches", init, "application/json");
}

export function createVectorStoreFileBatchQueryKey(init: CreateVectorStoreFileBatchInit): QueryKey {
    return ["post", "/vector_stores/{vector_store_id}/file_batches", init];
}

export interface PrefetchCreateVectorStoreFileBatchOptions extends CreateVectorStoreFileBatchInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

export function prefetchCreateVectorStoreFileBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateVectorStoreFileBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createVectorStoreFileBatchQueryKey(init),
        queryFn: () => createVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreFileBatchQueryOptions extends CreateVectorStoreFileBatchInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches`
 *
 * Create a vector store file batch.
 */
export function useCreateVectorStoreFileBatchQuery(options: UseCreateVectorStoreFileBatchQueryOptions): UseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createVectorStoreFileBatchQueryKey(init),
        queryFn: () => createVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseCreateVectorStoreFileBatchSuspenseQueryOptions extends CreateVectorStoreFileBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches`
 *
 * Create a vector store file batch.
 */
export function useCreateVectorStoreFileBatchSuspenseQuery(options: UseCreateVectorStoreFileBatchSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createVectorStoreFileBatchQueryKey(init),
        queryFn: () => createVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches`
 *
 * Create a vector store file batch.
 */
export function useCreateVectorStoreFileBatchMutation(options: Partial<UseMutationOptions<VectorStoreFileBatchObject, unknown, CreateVectorStoreFileBatchInit>> = {}): UseMutationResult<VectorStoreFileBatchObject, unknown, CreateVectorStoreFileBatchInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateVectorStoreFileBatchInit) => createVectorStoreFileBatch(client, init),
        ...options
    });
}

export interface GetVectorStoreFileBatchInit {
    path: {
        /**
         * The ID of the vector store that the file batch belongs to.
         */
        vector_store_id: string;
        /**
         * The ID of the file batch being retrieved.
         */
        batch_id: string;
    };
    request?: RequestInit;
}

function getVectorStoreFileBatch(client: OpenAPIClient, init: GetVectorStoreFileBatchInit): Promise<VectorStoreFileBatchObject> {
    return client[internal_fetch]("GET", "/vector_stores/{vector_store_id}/file_batches/{batch_id}", init);
}

export function getVectorStoreFileBatchQueryKey(init: GetVectorStoreFileBatchInit): QueryKey {
    return ["get", "/vector_stores/{vector_store_id}/file_batches/{batch_id}", init];
}

export interface PrefetchGetVectorStoreFileBatchOptions extends GetVectorStoreFileBatchInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

export function prefetchGetVectorStoreFileBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchGetVectorStoreFileBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: getVectorStoreFileBatchQueryKey(init),
        queryFn: () => getVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreFileBatchQueryOptions extends GetVectorStoreFileBatchInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/file_batches/{batch_id}`
 *
 * Retrieves a vector store file batch.
 */
export function useGetVectorStoreFileBatchQuery(options: UseGetVectorStoreFileBatchQueryOptions): UseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: getVectorStoreFileBatchQueryKey(init),
        queryFn: () => getVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseGetVectorStoreFileBatchSuspenseQueryOptions extends GetVectorStoreFileBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/file_batches/{batch_id}`
 *
 * Retrieves a vector store file batch.
 */
export function useGetVectorStoreFileBatchSuspenseQuery(options: UseGetVectorStoreFileBatchSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: getVectorStoreFileBatchQueryKey(init),
        queryFn: () => getVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface CancelVectorStoreFileBatchInit {
    path: {
        /**
         * The ID of the vector store that the file batch belongs to.
         */
        vector_store_id: string;
        /**
         * The ID of the file batch to cancel.
         */
        batch_id: string;
    };
    request?: RequestInit;
}

function cancelVectorStoreFileBatch(client: OpenAPIClient, init: CancelVectorStoreFileBatchInit): Promise<VectorStoreFileBatchObject> {
    return client[internal_fetch]("POST", "/vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel", init);
}

export function cancelVectorStoreFileBatchQueryKey(init: CancelVectorStoreFileBatchInit): QueryKey {
    return ["post", "/vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel", init];
}

export interface PrefetchCancelVectorStoreFileBatchOptions extends CancelVectorStoreFileBatchInit {
    queryOptions?: Partial<FetchQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

export function prefetchCancelVectorStoreFileBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCancelVectorStoreFileBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: cancelVectorStoreFileBatchQueryKey(init),
        queryFn: () => cancelVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseCancelVectorStoreFileBatchQueryOptions extends CancelVectorStoreFileBatchInit {
    queryOptions?: Partial<UseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel`
 *
 * Cancel a vector store file batch. This attempts to cancel the processing of files in this batch as soon as possible.
 */
export function useCancelVectorStoreFileBatchQuery(options: UseCancelVectorStoreFileBatchQueryOptions): UseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: cancelVectorStoreFileBatchQueryKey(init),
        queryFn: () => cancelVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

export interface UseCancelVectorStoreFileBatchSuspenseQueryOptions extends CancelVectorStoreFileBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<VectorStoreFileBatchObject, unknown>>;
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel`
 *
 * Cancel a vector store file batch. This attempts to cancel the processing of files in this batch as soon as possible.
 */
export function useCancelVectorStoreFileBatchSuspenseQuery(options: UseCancelVectorStoreFileBatchSuspenseQueryOptions): UseSuspenseQueryResult<VectorStoreFileBatchObject, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: cancelVectorStoreFileBatchQueryKey(init),
        queryFn: () => cancelVectorStoreFileBatch(client, init),
        ...queryOptions
    });
}

/**
 * `POST /vector_stores/{vector_store_id}/file_batches/{batch_id}/cancel`
 *
 * Cancel a vector store file batch. This attempts to cancel the processing of files in this batch as soon as possible.
 */
export function useCancelVectorStoreFileBatchMutation(options: Partial<UseMutationOptions<VectorStoreFileBatchObject, unknown, CancelVectorStoreFileBatchInit>> = {}): UseMutationResult<VectorStoreFileBatchObject, unknown, CancelVectorStoreFileBatchInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CancelVectorStoreFileBatchInit) => cancelVectorStoreFileBatch(client, init),
        ...options
    });
}

export interface ListFilesInVectorStoreBatchInit {
    path: {
        /**
         * The ID of the vector store that the files belong to.
         */
        vector_store_id: string;
        /**
         * The ID of the file batch that the files belong to.
         */
        batch_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * Sort order by the `created_at` timestamp of the objects. `asc` for ascending order and `desc` for descending order.
         *
         * @defaultValue `desc`
         */
        order?: "asc" | "desc";
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
        /**
         * Filter by file status. One of `in_progress`, `completed`, `failed`, `cancelled`.
         */
        filter?: "in_progress" | "completed" | "failed" | "cancelled";
    };
    request?: RequestInit;
}

function listFilesInVectorStoreBatch(client: OpenAPIClient, init: ListFilesInVectorStoreBatchInit): Promise<ListVectorStoreFilesResponse> {
    return client[internal_fetch]("GET", "/vector_stores/{vector_store_id}/file_batches/{batch_id}/files", init);
}

export function listFilesInVectorStoreBatchQueryKey(init: ListFilesInVectorStoreBatchInit): QueryKey {
    return ["get", "/vector_stores/{vector_store_id}/file_batches/{batch_id}/files", init];
}

export interface PrefetchListFilesInVectorStoreBatchOptions extends ListFilesInVectorStoreBatchInit {
    queryOptions?: Partial<FetchQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

export function prefetchListFilesInVectorStoreBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListFilesInVectorStoreBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listFilesInVectorStoreBatchQueryKey(init),
        queryFn: () => listFilesInVectorStoreBatch(client, init),
        ...queryOptions
    });
}

export interface UseListFilesInVectorStoreBatchQueryOptions extends ListFilesInVectorStoreBatchInit {
    queryOptions?: Partial<UseQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/file_batches/{batch_id}/files`
 *
 * Returns a list of vector store files in a batch.
 */
export function useListFilesInVectorStoreBatchQuery(options: UseListFilesInVectorStoreBatchQueryOptions): UseQueryResult<ListVectorStoreFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listFilesInVectorStoreBatchQueryKey(init),
        queryFn: () => listFilesInVectorStoreBatch(client, init),
        ...queryOptions
    });
}

export interface UseListFilesInVectorStoreBatchSuspenseQueryOptions extends ListFilesInVectorStoreBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListVectorStoreFilesResponse, unknown>>;
}

/**
 * `GET /vector_stores/{vector_store_id}/file_batches/{batch_id}/files`
 *
 * Returns a list of vector store files in a batch.
 */
export function useListFilesInVectorStoreBatchSuspenseQuery(options: UseListFilesInVectorStoreBatchSuspenseQueryOptions): UseSuspenseQueryResult<ListVectorStoreFilesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listFilesInVectorStoreBatchQueryKey(init),
        queryFn: () => listFilesInVectorStoreBatch(client, init),
        ...queryOptions
    });
}

export interface ListBatchesInit {
    query?: {
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
    };
    request?: RequestInit;
}

function listBatches(client: OpenAPIClient, init: ListBatchesInit = {}): Promise<ListBatchesResponse> {
    return client[internal_fetch]("GET", "/batches", init);
}

export function listBatchesQueryKey(init: ListBatchesInit = {}): QueryKey {
    return ["get", "/batches", init];
}

export interface PrefetchListBatchesOptions extends ListBatchesInit {
    queryOptions?: Partial<FetchQueryOptions<ListBatchesResponse, unknown>>;
}

export function prefetchListBatches(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListBatchesOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listBatchesQueryKey(init),
        queryFn: () => listBatches(client, init),
        ...queryOptions
    });
}

export interface UseListBatchesQueryOptions extends ListBatchesInit {
    queryOptions?: Partial<UseQueryOptions<ListBatchesResponse, unknown>>;
}

/**
 * `GET /batches`
 *
 * List your organization's batches.
 */
export function useListBatchesQuery(options: UseListBatchesQueryOptions = {}): UseQueryResult<ListBatchesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listBatchesQueryKey(init),
        queryFn: () => listBatches(client, init),
        ...queryOptions
    });
}

export interface UseListBatchesSuspenseQueryOptions extends ListBatchesInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListBatchesResponse, unknown>>;
}

/**
 * `GET /batches`
 *
 * List your organization's batches.
 */
export function useListBatchesSuspenseQuery(options: UseListBatchesSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListBatchesResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listBatchesQueryKey(init),
        queryFn: () => listBatches(client, init),
        ...queryOptions
    });
}

export interface CreateBatchInit {
    body: {
        /**
         * The ID of an uploaded file that contains requests for the new batch.
         *
         * See [upload file](/docs/api-reference/files/create) for how to upload a file.
         *
         * Your input file must be formatted as a [JSONL file](/docs/api-reference/batch/request-input), and must be uploaded with the purpose `batch`. The file can contain up to 50,000 requests, and can be up to 100 MB in size.
         *
         */
        input_file_id: string;
        /**
         * The endpoint to be used for all requests in the batch. Currently `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are supported. Note that `/v1/embeddings` batches are also restricted to a maximum of 50,000 embedding inputs across all requests in the batch.
         */
        endpoint: "/v1/chat/completions" | "/v1/embeddings" | "/v1/completions";
        /**
         * The time frame within which the batch should be processed. Currently only `24h` is supported.
         */
        completion_window: "24h";
        /**
         * Optional custom metadata for the batch.
         */
        metadata?: Record<string, string> | null;
    };
    request?: RequestInit;
}

function createBatch(client: OpenAPIClient, init: CreateBatchInit): Promise<Batch> {
    return client[internal_fetch]("POST", "/batches", init, "application/json");
}

export function createBatchQueryKey(init: CreateBatchInit): QueryKey {
    return ["post", "/batches", init];
}

export interface PrefetchCreateBatchOptions extends CreateBatchInit {
    queryOptions?: Partial<FetchQueryOptions<Batch, unknown>>;
}

export function prefetchCreateBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createBatchQueryKey(init),
        queryFn: () => createBatch(client, init),
        ...queryOptions
    });
}

export interface UseCreateBatchQueryOptions extends CreateBatchInit {
    queryOptions?: Partial<UseQueryOptions<Batch, unknown>>;
}

/**
 * `POST /batches`
 *
 * Creates and executes a batch from an uploaded file of requests
 */
export function useCreateBatchQuery(options: UseCreateBatchQueryOptions): UseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createBatchQueryKey(init),
        queryFn: () => createBatch(client, init),
        ...queryOptions
    });
}

export interface UseCreateBatchSuspenseQueryOptions extends CreateBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Batch, unknown>>;
}

/**
 * `POST /batches`
 *
 * Creates and executes a batch from an uploaded file of requests
 */
export function useCreateBatchSuspenseQuery(options: UseCreateBatchSuspenseQueryOptions): UseSuspenseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createBatchQueryKey(init),
        queryFn: () => createBatch(client, init),
        ...queryOptions
    });
}

/**
 * `POST /batches`
 *
 * Creates and executes a batch from an uploaded file of requests
 */
export function useCreateBatchMutation(options: Partial<UseMutationOptions<Batch, unknown, CreateBatchInit>> = {}): UseMutationResult<Batch, unknown, CreateBatchInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateBatchInit) => createBatch(client, init),
        ...options
    });
}

export interface RetrieveBatchInit {
    path: {
        /**
         * The ID of the batch to retrieve.
         */
        batch_id: string;
    };
    request?: RequestInit;
}

function retrieveBatch(client: OpenAPIClient, init: RetrieveBatchInit): Promise<Batch> {
    return client[internal_fetch]("GET", "/batches/{batch_id}", init);
}

export function retrieveBatchQueryKey(init: RetrieveBatchInit): QueryKey {
    return ["get", "/batches/{batch_id}", init];
}

export interface PrefetchRetrieveBatchOptions extends RetrieveBatchInit {
    queryOptions?: Partial<FetchQueryOptions<Batch, unknown>>;
}

export function prefetchRetrieveBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveBatchQueryKey(init),
        queryFn: () => retrieveBatch(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveBatchQueryOptions extends RetrieveBatchInit {
    queryOptions?: Partial<UseQueryOptions<Batch, unknown>>;
}

/**
 * `GET /batches/{batch_id}`
 *
 * Retrieves a batch.
 */
export function useRetrieveBatchQuery(options: UseRetrieveBatchQueryOptions): UseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveBatchQueryKey(init),
        queryFn: () => retrieveBatch(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveBatchSuspenseQueryOptions extends RetrieveBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Batch, unknown>>;
}

/**
 * `GET /batches/{batch_id}`
 *
 * Retrieves a batch.
 */
export function useRetrieveBatchSuspenseQuery(options: UseRetrieveBatchSuspenseQueryOptions): UseSuspenseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveBatchQueryKey(init),
        queryFn: () => retrieveBatch(client, init),
        ...queryOptions
    });
}

export interface CancelBatchInit {
    path: {
        /**
         * The ID of the batch to cancel.
         */
        batch_id: string;
    };
    request?: RequestInit;
}

function cancelBatch(client: OpenAPIClient, init: CancelBatchInit): Promise<Batch> {
    return client[internal_fetch]("POST", "/batches/{batch_id}/cancel", init);
}

export function cancelBatchQueryKey(init: CancelBatchInit): QueryKey {
    return ["post", "/batches/{batch_id}/cancel", init];
}

export interface PrefetchCancelBatchOptions extends CancelBatchInit {
    queryOptions?: Partial<FetchQueryOptions<Batch, unknown>>;
}

export function prefetchCancelBatch(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCancelBatchOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: cancelBatchQueryKey(init),
        queryFn: () => cancelBatch(client, init),
        ...queryOptions
    });
}

export interface UseCancelBatchQueryOptions extends CancelBatchInit {
    queryOptions?: Partial<UseQueryOptions<Batch, unknown>>;
}

/**
 * `POST /batches/{batch_id}/cancel`
 *
 * Cancels an in-progress batch. The batch will be in status `cancelling` for up to 10 minutes, before changing to `cancelled`, where it will have partial results (if any) available in the output file.
 */
export function useCancelBatchQuery(options: UseCancelBatchQueryOptions): UseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: cancelBatchQueryKey(init),
        queryFn: () => cancelBatch(client, init),
        ...queryOptions
    });
}

export interface UseCancelBatchSuspenseQueryOptions extends CancelBatchInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Batch, unknown>>;
}

/**
 * `POST /batches/{batch_id}/cancel`
 *
 * Cancels an in-progress batch. The batch will be in status `cancelling` for up to 10 minutes, before changing to `cancelled`, where it will have partial results (if any) available in the output file.
 */
export function useCancelBatchSuspenseQuery(options: UseCancelBatchSuspenseQueryOptions): UseSuspenseQueryResult<Batch, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: cancelBatchQueryKey(init),
        queryFn: () => cancelBatch(client, init),
        ...queryOptions
    });
}

/**
 * `POST /batches/{batch_id}/cancel`
 *
 * Cancels an in-progress batch. The batch will be in status `cancelling` for up to 10 minutes, before changing to `cancelled`, where it will have partial results (if any) available in the output file.
 */
export function useCancelBatchMutation(options: Partial<UseMutationOptions<Batch, unknown, CancelBatchInit>> = {}): UseMutationResult<Batch, unknown, CancelBatchInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CancelBatchInit) => cancelBatch(client, init),
        ...options
    });
}

export interface ListauditlogsInit {
    query?: {
        /**
         * Return only events whose `effective_at` (Unix seconds) is in this range.
         */
        effective_at?: {
            /**
             * Return only events whose `effective_at` (Unix seconds) is greater than this value.
             */
            gt?: number;
            /**
             * Return only events whose `effective_at` (Unix seconds) is greater than or equal to this value.
             */
            gte?: number;
            /**
             * Return only events whose `effective_at` (Unix seconds) is less than this value.
             */
            lt?: number;
            /**
             * Return only events whose `effective_at` (Unix seconds) is less than or equal to this value.
             */
            lte?: number;
        };
        /**
         * Return only events for these projects.
         */
        "project_ids[]"?: string[];
        /**
         * Return only events with a `type` in one of these values. For example, `project.created`. For all options, see the documentation for the [audit log object](/docs/api-reference/audit-logs/object).
         */
        "event_types[]"?: AuditLogEventType[];
        /**
         * Return only events performed by these actors. Can be a user ID, a service account ID, or an api key tracking ID.
         */
        "actor_ids[]"?: string[];
        /**
         * Return only events performed by users with these emails.
         */
        "actor_emails[]"?: string[];
        /**
         * Return only events performed on these targets. For example, a project ID updated.
         */
        "resource_ids[]"?: string[];
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * A cursor for use in pagination. `before` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include before=obj_foo in order to fetch the previous page of the list.
         *
         */
        before?: string;
    };
    request?: RequestInit;
}

function listauditlogs(client: OpenAPIClient, init: ListauditlogsInit = {}): Promise<ListAuditLogsResponse> {
    return client[internal_fetch]("GET", "/organization/audit_logs", init);
}

export function listauditlogsQueryKey(init: ListauditlogsInit = {}): QueryKey {
    return ["get", "/organization/audit_logs", init];
}

export interface PrefetchListauditlogsOptions extends ListauditlogsInit {
    queryOptions?: Partial<FetchQueryOptions<ListAuditLogsResponse, unknown>>;
}

export function prefetchListauditlogs(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListauditlogsOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listauditlogsQueryKey(init),
        queryFn: () => listauditlogs(client, init),
        ...queryOptions
    });
}

export interface UseListauditlogsQueryOptions extends ListauditlogsInit {
    queryOptions?: Partial<UseQueryOptions<ListAuditLogsResponse, unknown>>;
}

/**
 * `GET /organization/audit_logs`
 *
 * List user actions and configuration changes within this organization.
 */
export function useListauditlogsQuery(options: UseListauditlogsQueryOptions = {}): UseQueryResult<ListAuditLogsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listauditlogsQueryKey(init),
        queryFn: () => listauditlogs(client, init),
        ...queryOptions
    });
}

export interface UseListauditlogsSuspenseQueryOptions extends ListauditlogsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ListAuditLogsResponse, unknown>>;
}

/**
 * `GET /organization/audit_logs`
 *
 * List user actions and configuration changes within this organization.
 */
export function useListauditlogsSuspenseQuery(options: UseListauditlogsSuspenseQueryOptions = {}): UseSuspenseQueryResult<ListAuditLogsResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listauditlogsQueryKey(init),
        queryFn: () => listauditlogs(client, init),
        ...queryOptions
    });
}

export interface ListinvitesInit {
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
    };
    request?: RequestInit;
}

function listinvites(client: OpenAPIClient, init: ListinvitesInit = {}): Promise<InviteListResponse> {
    return client[internal_fetch]("GET", "/organization/invites", init);
}

export function listinvitesQueryKey(init: ListinvitesInit = {}): QueryKey {
    return ["get", "/organization/invites", init];
}

export interface PrefetchListinvitesOptions extends ListinvitesInit {
    queryOptions?: Partial<FetchQueryOptions<InviteListResponse, unknown>>;
}

export function prefetchListinvites(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListinvitesOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listinvitesQueryKey(init),
        queryFn: () => listinvites(client, init),
        ...queryOptions
    });
}

export interface UseListinvitesQueryOptions extends ListinvitesInit {
    queryOptions?: Partial<UseQueryOptions<InviteListResponse, unknown>>;
}

/**
 * `GET /organization/invites`
 *
 * Returns a list of invites in the organization.
 */
export function useListinvitesQuery(options: UseListinvitesQueryOptions = {}): UseQueryResult<InviteListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listinvitesQueryKey(init),
        queryFn: () => listinvites(client, init),
        ...queryOptions
    });
}

export interface UseListinvitesSuspenseQueryOptions extends ListinvitesInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<InviteListResponse, unknown>>;
}

/**
 * `GET /organization/invites`
 *
 * Returns a list of invites in the organization.
 */
export function useListinvitesSuspenseQuery(options: UseListinvitesSuspenseQueryOptions = {}): UseSuspenseQueryResult<InviteListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listinvitesQueryKey(init),
        queryFn: () => listinvites(client, init),
        ...queryOptions
    });
}

export interface InviteUserInit {
    body: InviteRequest;
    request?: RequestInit;
}

function inviteUser(client: OpenAPIClient, init: InviteUserInit): Promise<Invite> {
    return client[internal_fetch]("POST", "/organization/invites", init, "application/json");
}

export function inviteUserQueryKey(init: InviteUserInit): QueryKey {
    return ["post", "/organization/invites", init];
}

export interface PrefetchInviteUserOptions extends InviteUserInit {
    queryOptions?: Partial<FetchQueryOptions<Invite, unknown>>;
}

export function prefetchInviteUser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchInviteUserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: inviteUserQueryKey(init),
        queryFn: () => inviteUser(client, init),
        ...queryOptions
    });
}

export interface UseInviteUserQueryOptions extends InviteUserInit {
    queryOptions?: Partial<UseQueryOptions<Invite, unknown>>;
}

/**
 * `POST /organization/invites`
 *
 * Create an invite for a user to the organization. The invite must be accepted by the user before they have access to the organization.
 */
export function useInviteUserQuery(options: UseInviteUserQueryOptions): UseQueryResult<Invite, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: inviteUserQueryKey(init),
        queryFn: () => inviteUser(client, init),
        ...queryOptions
    });
}

export interface UseInviteUserSuspenseQueryOptions extends InviteUserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Invite, unknown>>;
}

/**
 * `POST /organization/invites`
 *
 * Create an invite for a user to the organization. The invite must be accepted by the user before they have access to the organization.
 */
export function useInviteUserSuspenseQuery(options: UseInviteUserSuspenseQueryOptions): UseSuspenseQueryResult<Invite, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: inviteUserQueryKey(init),
        queryFn: () => inviteUser(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/invites`
 *
 * Create an invite for a user to the organization. The invite must be accepted by the user before they have access to the organization.
 */
export function useInviteUserMutation(options: Partial<UseMutationOptions<Invite, unknown, InviteUserInit>> = {}): UseMutationResult<Invite, unknown, InviteUserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: InviteUserInit) => inviteUser(client, init),
        ...options
    });
}

export interface RetrieveinviteInit {
    path: {
        /**
         * The ID of the invite to retrieve.
         */
        invite_id: string;
    };
    request?: RequestInit;
}

function retrieveinvite(client: OpenAPIClient, init: RetrieveinviteInit): Promise<Invite> {
    return client[internal_fetch]("GET", "/organization/invites/{invite_id}", init);
}

export function retrieveinviteQueryKey(init: RetrieveinviteInit): QueryKey {
    return ["get", "/organization/invites/{invite_id}", init];
}

export interface PrefetchRetrieveinviteOptions extends RetrieveinviteInit {
    queryOptions?: Partial<FetchQueryOptions<Invite, unknown>>;
}

export function prefetchRetrieveinvite(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveinviteOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveinviteQueryKey(init),
        queryFn: () => retrieveinvite(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveinviteQueryOptions extends RetrieveinviteInit {
    queryOptions?: Partial<UseQueryOptions<Invite, unknown>>;
}

/**
 * `GET /organization/invites/{invite_id}`
 *
 * Retrieves an invite.
 */
export function useRetrieveinviteQuery(options: UseRetrieveinviteQueryOptions): UseQueryResult<Invite, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveinviteQueryKey(init),
        queryFn: () => retrieveinvite(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveinviteSuspenseQueryOptions extends RetrieveinviteInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Invite, unknown>>;
}

/**
 * `GET /organization/invites/{invite_id}`
 *
 * Retrieves an invite.
 */
export function useRetrieveinviteSuspenseQuery(options: UseRetrieveinviteSuspenseQueryOptions): UseSuspenseQueryResult<Invite, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveinviteQueryKey(init),
        queryFn: () => retrieveinvite(client, init),
        ...queryOptions
    });
}

export interface DeleteinviteInit {
    path: {
        /**
         * The ID of the invite to delete.
         */
        invite_id: string;
    };
    request?: RequestInit;
}

function deleteinvite(client: OpenAPIClient, init: DeleteinviteInit): Promise<InviteDeleteResponse> {
    return client[internal_fetch]("DELETE", "/organization/invites/{invite_id}", init);
}

export function deleteinviteQueryKey(init: DeleteinviteInit): QueryKey {
    return ["delete", "/organization/invites/{invite_id}", init];
}

export interface PrefetchDeleteinviteOptions extends DeleteinviteInit {
    queryOptions?: Partial<FetchQueryOptions<InviteDeleteResponse, unknown>>;
}

export function prefetchDeleteinvite(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteinviteOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteinviteQueryKey(init),
        queryFn: () => deleteinvite(client, init),
        ...queryOptions
    });
}

export interface UseDeleteinviteQueryOptions extends DeleteinviteInit {
    queryOptions?: Partial<UseQueryOptions<InviteDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/invites/{invite_id}`
 *
 * Delete an invite. If the invite has already been accepted, it cannot be deleted.
 */
export function useDeleteinviteQuery(options: UseDeleteinviteQueryOptions): UseQueryResult<InviteDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteinviteQueryKey(init),
        queryFn: () => deleteinvite(client, init),
        ...queryOptions
    });
}

export interface UseDeleteinviteSuspenseQueryOptions extends DeleteinviteInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<InviteDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/invites/{invite_id}`
 *
 * Delete an invite. If the invite has already been accepted, it cannot be deleted.
 */
export function useDeleteinviteSuspenseQuery(options: UseDeleteinviteSuspenseQueryOptions): UseSuspenseQueryResult<InviteDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteinviteQueryKey(init),
        queryFn: () => deleteinvite(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /organization/invites/{invite_id}`
 *
 * Delete an invite. If the invite has already been accepted, it cannot be deleted.
 */
export function useDeleteinviteMutation(options: Partial<UseMutationOptions<InviteDeleteResponse, unknown, DeleteinviteInit>> = {}): UseMutationResult<InviteDeleteResponse, unknown, DeleteinviteInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteinviteInit) => deleteinvite(client, init),
        ...options
    });
}

export interface ListusersInit {
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
    };
    request?: RequestInit;
}

function listusers(client: OpenAPIClient, init: ListusersInit = {}): Promise<UserListResponse> {
    return client[internal_fetch]("GET", "/organization/users", init);
}

export function listusersQueryKey(init: ListusersInit = {}): QueryKey {
    return ["get", "/organization/users", init];
}

export interface PrefetchListusersOptions extends ListusersInit {
    queryOptions?: Partial<FetchQueryOptions<UserListResponse, unknown>>;
}

export function prefetchListusers(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListusersOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listusersQueryKey(init),
        queryFn: () => listusers(client, init),
        ...queryOptions
    });
}

export interface UseListusersQueryOptions extends ListusersInit {
    queryOptions?: Partial<UseQueryOptions<UserListResponse, unknown>>;
}

/**
 * `GET /organization/users`
 *
 * Lists all of the users in the organization.
 */
export function useListusersQuery(options: UseListusersQueryOptions = {}): UseQueryResult<UserListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listusersQueryKey(init),
        queryFn: () => listusers(client, init),
        ...queryOptions
    });
}

export interface UseListusersSuspenseQueryOptions extends ListusersInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<UserListResponse, unknown>>;
}

/**
 * `GET /organization/users`
 *
 * Lists all of the users in the organization.
 */
export function useListusersSuspenseQuery(options: UseListusersSuspenseQueryOptions = {}): UseSuspenseQueryResult<UserListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listusersQueryKey(init),
        queryFn: () => listusers(client, init),
        ...queryOptions
    });
}

export interface RetrieveuserInit {
    path: {
        /**
         * The ID of the user.
         */
        user_id: string;
    };
    request?: RequestInit;
}

function retrieveuser(client: OpenAPIClient, init: RetrieveuserInit): Promise<User> {
    return client[internal_fetch]("GET", "/organization/users/{user_id}", init);
}

export function retrieveuserQueryKey(init: RetrieveuserInit): QueryKey {
    return ["get", "/organization/users/{user_id}", init];
}

export interface PrefetchRetrieveuserOptions extends RetrieveuserInit {
    queryOptions?: Partial<FetchQueryOptions<User, unknown>>;
}

export function prefetchRetrieveuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveuserQueryKey(init),
        queryFn: () => retrieveuser(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveuserQueryOptions extends RetrieveuserInit {
    queryOptions?: Partial<UseQueryOptions<User, unknown>>;
}

/**
 * `GET /organization/users/{user_id}`
 *
 * Retrieves a user by their identifier.
 */
export function useRetrieveuserQuery(options: UseRetrieveuserQueryOptions): UseQueryResult<User, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveuserQueryKey(init),
        queryFn: () => retrieveuser(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveuserSuspenseQueryOptions extends RetrieveuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<User, unknown>>;
}

/**
 * `GET /organization/users/{user_id}`
 *
 * Retrieves a user by their identifier.
 */
export function useRetrieveuserSuspenseQuery(options: UseRetrieveuserSuspenseQueryOptions): UseSuspenseQueryResult<User, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveuserQueryKey(init),
        queryFn: () => retrieveuser(client, init),
        ...queryOptions
    });
}

export interface ModifyuserInit {
    body: UserRoleUpdateRequest;
    request?: RequestInit;
}

function modifyuser(client: OpenAPIClient, init: ModifyuserInit): Promise<User> {
    return client[internal_fetch]("POST", "/organization/users/{user_id}", init, "application/json");
}

export function modifyuserQueryKey(init: ModifyuserInit): QueryKey {
    return ["post", "/organization/users/{user_id}", init];
}

export interface PrefetchModifyuserOptions extends ModifyuserInit {
    queryOptions?: Partial<FetchQueryOptions<User, unknown>>;
}

export function prefetchModifyuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyuserQueryKey(init),
        queryFn: () => modifyuser(client, init),
        ...queryOptions
    });
}

export interface UseModifyuserQueryOptions extends ModifyuserInit {
    queryOptions?: Partial<UseQueryOptions<User, unknown>>;
}

/**
 * `POST /organization/users/{user_id}`
 *
 * Modifies a user's role in the organization.
 */
export function useModifyuserQuery(options: UseModifyuserQueryOptions): UseQueryResult<User, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyuserQueryKey(init),
        queryFn: () => modifyuser(client, init),
        ...queryOptions
    });
}

export interface UseModifyuserSuspenseQueryOptions extends ModifyuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<User, unknown>>;
}

/**
 * `POST /organization/users/{user_id}`
 *
 * Modifies a user's role in the organization.
 */
export function useModifyuserSuspenseQuery(options: UseModifyuserSuspenseQueryOptions): UseSuspenseQueryResult<User, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyuserQueryKey(init),
        queryFn: () => modifyuser(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/users/{user_id}`
 *
 * Modifies a user's role in the organization.
 */
export function useModifyuserMutation(options: Partial<UseMutationOptions<User, unknown, ModifyuserInit>> = {}): UseMutationResult<User, unknown, ModifyuserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyuserInit) => modifyuser(client, init),
        ...options
    });
}

export interface DeleteuserInit {
    path: {
        /**
         * The ID of the user.
         */
        user_id: string;
    };
    request?: RequestInit;
}

function deleteuser(client: OpenAPIClient, init: DeleteuserInit): Promise<UserDeleteResponse> {
    return client[internal_fetch]("DELETE", "/organization/users/{user_id}", init);
}

export function deleteuserQueryKey(init: DeleteuserInit): QueryKey {
    return ["delete", "/organization/users/{user_id}", init];
}

export interface PrefetchDeleteuserOptions extends DeleteuserInit {
    queryOptions?: Partial<FetchQueryOptions<UserDeleteResponse, unknown>>;
}

export function prefetchDeleteuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteuserQueryKey(init),
        queryFn: () => deleteuser(client, init),
        ...queryOptions
    });
}

export interface UseDeleteuserQueryOptions extends DeleteuserInit {
    queryOptions?: Partial<UseQueryOptions<UserDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/users/{user_id}`
 *
 * Deletes a user from the organization.
 */
export function useDeleteuserQuery(options: UseDeleteuserQueryOptions): UseQueryResult<UserDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteuserQueryKey(init),
        queryFn: () => deleteuser(client, init),
        ...queryOptions
    });
}

export interface UseDeleteuserSuspenseQueryOptions extends DeleteuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<UserDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/users/{user_id}`
 *
 * Deletes a user from the organization.
 */
export function useDeleteuserSuspenseQuery(options: UseDeleteuserSuspenseQueryOptions): UseSuspenseQueryResult<UserDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteuserQueryKey(init),
        queryFn: () => deleteuser(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /organization/users/{user_id}`
 *
 * Deletes a user from the organization.
 */
export function useDeleteuserMutation(options: Partial<UseMutationOptions<UserDeleteResponse, unknown, DeleteuserInit>> = {}): UseMutationResult<UserDeleteResponse, unknown, DeleteuserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteuserInit) => deleteuser(client, init),
        ...options
    });
}

export interface ListprojectsInit {
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
        /**
         * If `true` returns all projects including those that have been `archived`. Archived projects are not included by default.
         * @defaultValue `false`
         */
        include_archived?: boolean;
    };
    request?: RequestInit;
}

function listprojects(client: OpenAPIClient, init: ListprojectsInit = {}): Promise<ProjectListResponse> {
    return client[internal_fetch]("GET", "/organization/projects", init);
}

export function listprojectsQueryKey(init: ListprojectsInit = {}): QueryKey {
    return ["get", "/organization/projects", init];
}

export interface PrefetchListprojectsOptions extends ListprojectsInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectListResponse, unknown>>;
}

export function prefetchListprojects(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListprojectsOptions = {}): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listprojectsQueryKey(init),
        queryFn: () => listprojects(client, init),
        ...queryOptions
    });
}

export interface UseListprojectsQueryOptions extends ListprojectsInit {
    queryOptions?: Partial<UseQueryOptions<ProjectListResponse, unknown>>;
}

/**
 * `GET /organization/projects`
 *
 * Returns a list of projects.
 */
export function useListprojectsQuery(options: UseListprojectsQueryOptions = {}): UseQueryResult<ProjectListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listprojectsQueryKey(init),
        queryFn: () => listprojects(client, init),
        ...queryOptions
    });
}

export interface UseListprojectsSuspenseQueryOptions extends ListprojectsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectListResponse, unknown>>;
}

/**
 * `GET /organization/projects`
 *
 * Returns a list of projects.
 */
export function useListprojectsSuspenseQuery(options: UseListprojectsSuspenseQueryOptions = {}): UseSuspenseQueryResult<ProjectListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listprojectsQueryKey(init),
        queryFn: () => listprojects(client, init),
        ...queryOptions
    });
}

export interface CreateprojectInit {
    body: ProjectCreateRequest;
    request?: RequestInit;
}

function createproject(client: OpenAPIClient, init: CreateprojectInit): Promise<Project> {
    return client[internal_fetch]("POST", "/organization/projects", init, "application/json");
}

export function createprojectQueryKey(init: CreateprojectInit): QueryKey {
    return ["post", "/organization/projects", init];
}

export interface PrefetchCreateprojectOptions extends CreateprojectInit {
    queryOptions?: Partial<FetchQueryOptions<Project, unknown>>;
}

export function prefetchCreateproject(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateprojectOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createprojectQueryKey(init),
        queryFn: () => createproject(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectQueryOptions extends CreateprojectInit {
    queryOptions?: Partial<UseQueryOptions<Project, unknown>>;
}

/**
 * `POST /organization/projects`
 *
 * Create a new project in the organization. Projects can be created and archived, but cannot be deleted.
 */
export function useCreateprojectQuery(options: UseCreateprojectQueryOptions): UseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createprojectQueryKey(init),
        queryFn: () => createproject(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectSuspenseQueryOptions extends CreateprojectInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Project, unknown>>;
}

/**
 * `POST /organization/projects`
 *
 * Create a new project in the organization. Projects can be created and archived, but cannot be deleted.
 */
export function useCreateprojectSuspenseQuery(options: UseCreateprojectSuspenseQueryOptions): UseSuspenseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createprojectQueryKey(init),
        queryFn: () => createproject(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects`
 *
 * Create a new project in the organization. Projects can be created and archived, but cannot be deleted.
 */
export function useCreateprojectMutation(options: Partial<UseMutationOptions<Project, unknown, CreateprojectInit>> = {}): UseMutationResult<Project, unknown, CreateprojectInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateprojectInit) => createproject(client, init),
        ...options
    });
}

export interface RetrieveprojectInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    request?: RequestInit;
}

function retrieveproject(client: OpenAPIClient, init: RetrieveprojectInit): Promise<Project> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}", init);
}

export function retrieveprojectQueryKey(init: RetrieveprojectInit): QueryKey {
    return ["get", "/organization/projects/{project_id}", init];
}

export interface PrefetchRetrieveprojectOptions extends RetrieveprojectInit {
    queryOptions?: Partial<FetchQueryOptions<Project, unknown>>;
}

export function prefetchRetrieveproject(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveprojectOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveprojectQueryKey(init),
        queryFn: () => retrieveproject(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectQueryOptions extends RetrieveprojectInit {
    queryOptions?: Partial<UseQueryOptions<Project, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}`
 *
 * Retrieves a project.
 */
export function useRetrieveprojectQuery(options: UseRetrieveprojectQueryOptions): UseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveprojectQueryKey(init),
        queryFn: () => retrieveproject(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectSuspenseQueryOptions extends RetrieveprojectInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Project, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}`
 *
 * Retrieves a project.
 */
export function useRetrieveprojectSuspenseQuery(options: UseRetrieveprojectSuspenseQueryOptions): UseSuspenseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveprojectQueryKey(init),
        queryFn: () => retrieveproject(client, init),
        ...queryOptions
    });
}

export interface ModifyprojectInit {
    body: ProjectUpdateRequest;
    request?: RequestInit;
}

function modifyproject(client: OpenAPIClient, init: ModifyprojectInit): Promise<Project> {
    return client[internal_fetch]("POST", "/organization/projects/{project_id}", init, "application/json");
}

export function modifyprojectQueryKey(init: ModifyprojectInit): QueryKey {
    return ["post", "/organization/projects/{project_id}", init];
}

export interface PrefetchModifyprojectOptions extends ModifyprojectInit {
    queryOptions?: Partial<FetchQueryOptions<Project, ErrorResponse>>;
}

export function prefetchModifyproject(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyprojectOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyprojectQueryKey(init),
        queryFn: () => modifyproject(client, init),
        ...queryOptions
    });
}

export interface UseModifyprojectQueryOptions extends ModifyprojectInit {
    queryOptions?: Partial<UseQueryOptions<Project, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}`
 *
 * Modifies a project in the organization.
 */
export function useModifyprojectQuery(options: UseModifyprojectQueryOptions): UseQueryResult<Project, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyprojectQueryKey(init),
        queryFn: () => modifyproject(client, init),
        ...queryOptions
    });
}

export interface UseModifyprojectSuspenseQueryOptions extends ModifyprojectInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Project, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}`
 *
 * Modifies a project in the organization.
 */
export function useModifyprojectSuspenseQuery(options: UseModifyprojectSuspenseQueryOptions): UseSuspenseQueryResult<Project, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyprojectQueryKey(init),
        queryFn: () => modifyproject(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects/{project_id}`
 *
 * Modifies a project in the organization.
 */
export function useModifyprojectMutation(options: Partial<UseMutationOptions<Project, ErrorResponse, ModifyprojectInit>> = {}): UseMutationResult<Project, ErrorResponse, ModifyprojectInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyprojectInit) => modifyproject(client, init),
        ...options
    });
}

export interface ArchiveprojectInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    request?: RequestInit;
}

function archiveproject(client: OpenAPIClient, init: ArchiveprojectInit): Promise<Project> {
    return client[internal_fetch]("POST", "/organization/projects/{project_id}/archive", init);
}

export function archiveprojectQueryKey(init: ArchiveprojectInit): QueryKey {
    return ["post", "/organization/projects/{project_id}/archive", init];
}

export interface PrefetchArchiveprojectOptions extends ArchiveprojectInit {
    queryOptions?: Partial<FetchQueryOptions<Project, unknown>>;
}

export function prefetchArchiveproject(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchArchiveprojectOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: archiveprojectQueryKey(init),
        queryFn: () => archiveproject(client, init),
        ...queryOptions
    });
}

export interface UseArchiveprojectQueryOptions extends ArchiveprojectInit {
    queryOptions?: Partial<UseQueryOptions<Project, unknown>>;
}

/**
 * `POST /organization/projects/{project_id}/archive`
 *
 * Archives a project in the organization. Archived projects cannot be used or updated.
 */
export function useArchiveprojectQuery(options: UseArchiveprojectQueryOptions): UseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: archiveprojectQueryKey(init),
        queryFn: () => archiveproject(client, init),
        ...queryOptions
    });
}

export interface UseArchiveprojectSuspenseQueryOptions extends ArchiveprojectInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<Project, unknown>>;
}

/**
 * `POST /organization/projects/{project_id}/archive`
 *
 * Archives a project in the organization. Archived projects cannot be used or updated.
 */
export function useArchiveprojectSuspenseQuery(options: UseArchiveprojectSuspenseQueryOptions): UseSuspenseQueryResult<Project, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: archiveprojectQueryKey(init),
        queryFn: () => archiveproject(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects/{project_id}/archive`
 *
 * Archives a project in the organization. Archived projects cannot be used or updated.
 */
export function useArchiveprojectMutation(options: Partial<UseMutationOptions<Project, unknown, ArchiveprojectInit>> = {}): UseMutationResult<Project, unknown, ArchiveprojectInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ArchiveprojectInit) => archiveproject(client, init),
        ...options
    });
}

export interface ListprojectusersInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
    };
    request?: RequestInit;
}

function listprojectusers(client: OpenAPIClient, init: ListprojectusersInit): Promise<ProjectUserListResponse> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/users", init);
}

export function listprojectusersQueryKey(init: ListprojectusersInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/users", init];
}

export interface PrefetchListprojectusersOptions extends ListprojectusersInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectUserListResponse, ErrorResponse>>;
}

export function prefetchListprojectusers(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListprojectusersOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listprojectusersQueryKey(init),
        queryFn: () => listprojectusers(client, init),
        ...queryOptions
    });
}

export interface UseListprojectusersQueryOptions extends ListprojectusersInit {
    queryOptions?: Partial<UseQueryOptions<ProjectUserListResponse, ErrorResponse>>;
}

/**
 * `GET /organization/projects/{project_id}/users`
 *
 * Returns a list of users in the project.
 */
export function useListprojectusersQuery(options: UseListprojectusersQueryOptions): UseQueryResult<ProjectUserListResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listprojectusersQueryKey(init),
        queryFn: () => listprojectusers(client, init),
        ...queryOptions
    });
}

export interface UseListprojectusersSuspenseQueryOptions extends ListprojectusersInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectUserListResponse, ErrorResponse>>;
}

/**
 * `GET /organization/projects/{project_id}/users`
 *
 * Returns a list of users in the project.
 */
export function useListprojectusersSuspenseQuery(options: UseListprojectusersSuspenseQueryOptions): UseSuspenseQueryResult<ProjectUserListResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listprojectusersQueryKey(init),
        queryFn: () => listprojectusers(client, init),
        ...queryOptions
    });
}

export interface CreateprojectuserInit {
    body: ProjectUserCreateRequest;
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    request?: RequestInit;
}

function createprojectuser(client: OpenAPIClient, init: CreateprojectuserInit): Promise<ProjectUser> {
    return client[internal_fetch]("POST", "/organization/projects/{project_id}/users", init, "application/json");
}

export function createprojectuserQueryKey(init: CreateprojectuserInit): QueryKey {
    return ["post", "/organization/projects/{project_id}/users", init];
}

export interface PrefetchCreateprojectuserOptions extends CreateprojectuserInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectUser, ErrorResponse>>;
}

export function prefetchCreateprojectuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateprojectuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createprojectuserQueryKey(init),
        queryFn: () => createprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectuserQueryOptions extends CreateprojectuserInit {
    queryOptions?: Partial<UseQueryOptions<ProjectUser, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/users`
 *
 * Adds a user to the project. Users must already be members of the organization to be added to a project.
 */
export function useCreateprojectuserQuery(options: UseCreateprojectuserQueryOptions): UseQueryResult<ProjectUser, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createprojectuserQueryKey(init),
        queryFn: () => createprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectuserSuspenseQueryOptions extends CreateprojectuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectUser, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/users`
 *
 * Adds a user to the project. Users must already be members of the organization to be added to a project.
 */
export function useCreateprojectuserSuspenseQuery(options: UseCreateprojectuserSuspenseQueryOptions): UseSuspenseQueryResult<ProjectUser, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createprojectuserQueryKey(init),
        queryFn: () => createprojectuser(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects/{project_id}/users`
 *
 * Adds a user to the project. Users must already be members of the organization to be added to a project.
 */
export function useCreateprojectuserMutation(options: Partial<UseMutationOptions<ProjectUser, ErrorResponse, CreateprojectuserInit>> = {}): UseMutationResult<ProjectUser, ErrorResponse, CreateprojectuserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateprojectuserInit) => createprojectuser(client, init),
        ...options
    });
}

export interface RetrieveprojectuserInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the user.
         */
        user_id: string;
    };
    request?: RequestInit;
}

function retrieveprojectuser(client: OpenAPIClient, init: RetrieveprojectuserInit): Promise<ProjectUser> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/users/{user_id}", init);
}

export function retrieveprojectuserQueryKey(init: RetrieveprojectuserInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/users/{user_id}", init];
}

export interface PrefetchRetrieveprojectuserOptions extends RetrieveprojectuserInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectUser, unknown>>;
}

export function prefetchRetrieveprojectuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveprojectuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveprojectuserQueryKey(init),
        queryFn: () => retrieveprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectuserQueryOptions extends RetrieveprojectuserInit {
    queryOptions?: Partial<UseQueryOptions<ProjectUser, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/users/{user_id}`
 *
 * Retrieves a user in the project.
 */
export function useRetrieveprojectuserQuery(options: UseRetrieveprojectuserQueryOptions): UseQueryResult<ProjectUser, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveprojectuserQueryKey(init),
        queryFn: () => retrieveprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectuserSuspenseQueryOptions extends RetrieveprojectuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectUser, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/users/{user_id}`
 *
 * Retrieves a user in the project.
 */
export function useRetrieveprojectuserSuspenseQuery(options: UseRetrieveprojectuserSuspenseQueryOptions): UseSuspenseQueryResult<ProjectUser, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveprojectuserQueryKey(init),
        queryFn: () => retrieveprojectuser(client, init),
        ...queryOptions
    });
}

export interface ModifyprojectuserInit {
    body: ProjectUserUpdateRequest;
    request?: RequestInit;
}

function modifyprojectuser(client: OpenAPIClient, init: ModifyprojectuserInit): Promise<ProjectUser> {
    return client[internal_fetch]("POST", "/organization/projects/{project_id}/users/{user_id}", init, "application/json");
}

export function modifyprojectuserQueryKey(init: ModifyprojectuserInit): QueryKey {
    return ["post", "/organization/projects/{project_id}/users/{user_id}", init];
}

export interface PrefetchModifyprojectuserOptions extends ModifyprojectuserInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectUser, ErrorResponse>>;
}

export function prefetchModifyprojectuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchModifyprojectuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: modifyprojectuserQueryKey(init),
        queryFn: () => modifyprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseModifyprojectuserQueryOptions extends ModifyprojectuserInit {
    queryOptions?: Partial<UseQueryOptions<ProjectUser, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/users/{user_id}`
 *
 * Modifies a user's role in the project.
 */
export function useModifyprojectuserQuery(options: UseModifyprojectuserQueryOptions): UseQueryResult<ProjectUser, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: modifyprojectuserQueryKey(init),
        queryFn: () => modifyprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseModifyprojectuserSuspenseQueryOptions extends ModifyprojectuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectUser, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/users/{user_id}`
 *
 * Modifies a user's role in the project.
 */
export function useModifyprojectuserSuspenseQuery(options: UseModifyprojectuserSuspenseQueryOptions): UseSuspenseQueryResult<ProjectUser, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: modifyprojectuserQueryKey(init),
        queryFn: () => modifyprojectuser(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects/{project_id}/users/{user_id}`
 *
 * Modifies a user's role in the project.
 */
export function useModifyprojectuserMutation(options: Partial<UseMutationOptions<ProjectUser, ErrorResponse, ModifyprojectuserInit>> = {}): UseMutationResult<ProjectUser, ErrorResponse, ModifyprojectuserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: ModifyprojectuserInit) => modifyprojectuser(client, init),
        ...options
    });
}

export interface DeleteprojectuserInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the user.
         */
        user_id: string;
    };
    request?: RequestInit;
}

function deleteprojectuser(client: OpenAPIClient, init: DeleteprojectuserInit): Promise<ProjectUserDeleteResponse> {
    return client[internal_fetch]("DELETE", "/organization/projects/{project_id}/users/{user_id}", init);
}

export function deleteprojectuserQueryKey(init: DeleteprojectuserInit): QueryKey {
    return ["delete", "/organization/projects/{project_id}/users/{user_id}", init];
}

export interface PrefetchDeleteprojectuserOptions extends DeleteprojectuserInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectUserDeleteResponse, ErrorResponse>>;
}

export function prefetchDeleteprojectuser(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteprojectuserOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteprojectuserQueryKey(init),
        queryFn: () => deleteprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectuserQueryOptions extends DeleteprojectuserInit {
    queryOptions?: Partial<UseQueryOptions<ProjectUserDeleteResponse, ErrorResponse>>;
}

/**
 * `DELETE /organization/projects/{project_id}/users/{user_id}`
 *
 * Deletes a user from the project.
 */
export function useDeleteprojectuserQuery(options: UseDeleteprojectuserQueryOptions): UseQueryResult<ProjectUserDeleteResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteprojectuserQueryKey(init),
        queryFn: () => deleteprojectuser(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectuserSuspenseQueryOptions extends DeleteprojectuserInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectUserDeleteResponse, ErrorResponse>>;
}

/**
 * `DELETE /organization/projects/{project_id}/users/{user_id}`
 *
 * Deletes a user from the project.
 */
export function useDeleteprojectuserSuspenseQuery(options: UseDeleteprojectuserSuspenseQueryOptions): UseSuspenseQueryResult<ProjectUserDeleteResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteprojectuserQueryKey(init),
        queryFn: () => deleteprojectuser(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /organization/projects/{project_id}/users/{user_id}`
 *
 * Deletes a user from the project.
 */
export function useDeleteprojectuserMutation(options: Partial<UseMutationOptions<ProjectUserDeleteResponse, ErrorResponse, DeleteprojectuserInit>> = {}): UseMutationResult<ProjectUserDeleteResponse, ErrorResponse, DeleteprojectuserInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteprojectuserInit) => deleteprojectuser(client, init),
        ...options
    });
}

export interface ListprojectserviceaccountsInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
    };
    request?: RequestInit;
}

function listprojectserviceaccounts(client: OpenAPIClient, init: ListprojectserviceaccountsInit): Promise<ProjectServiceAccountListResponse> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/service_accounts", init);
}

export function listprojectserviceaccountsQueryKey(init: ListprojectserviceaccountsInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/service_accounts", init];
}

export interface PrefetchListprojectserviceaccountsOptions extends ListprojectserviceaccountsInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectServiceAccountListResponse, ErrorResponse>>;
}

export function prefetchListprojectserviceaccounts(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListprojectserviceaccountsOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listprojectserviceaccountsQueryKey(init),
        queryFn: () => listprojectserviceaccounts(client, init),
        ...queryOptions
    });
}

export interface UseListprojectserviceaccountsQueryOptions extends ListprojectserviceaccountsInit {
    queryOptions?: Partial<UseQueryOptions<ProjectServiceAccountListResponse, ErrorResponse>>;
}

/**
 * `GET /organization/projects/{project_id}/service_accounts`
 *
 * Returns a list of service accounts in the project.
 */
export function useListprojectserviceaccountsQuery(options: UseListprojectserviceaccountsQueryOptions): UseQueryResult<ProjectServiceAccountListResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listprojectserviceaccountsQueryKey(init),
        queryFn: () => listprojectserviceaccounts(client, init),
        ...queryOptions
    });
}

export interface UseListprojectserviceaccountsSuspenseQueryOptions extends ListprojectserviceaccountsInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectServiceAccountListResponse, ErrorResponse>>;
}

/**
 * `GET /organization/projects/{project_id}/service_accounts`
 *
 * Returns a list of service accounts in the project.
 */
export function useListprojectserviceaccountsSuspenseQuery(options: UseListprojectserviceaccountsSuspenseQueryOptions): UseSuspenseQueryResult<ProjectServiceAccountListResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listprojectserviceaccountsQueryKey(init),
        queryFn: () => listprojectserviceaccounts(client, init),
        ...queryOptions
    });
}

export interface CreateprojectserviceaccountInit {
    body: ProjectServiceAccountCreateRequest;
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    request?: RequestInit;
}

function createprojectserviceaccount(client: OpenAPIClient, init: CreateprojectserviceaccountInit): Promise<ProjectServiceAccountCreateResponse> {
    return client[internal_fetch]("POST", "/organization/projects/{project_id}/service_accounts", init, "application/json");
}

export function createprojectserviceaccountQueryKey(init: CreateprojectserviceaccountInit): QueryKey {
    return ["post", "/organization/projects/{project_id}/service_accounts", init];
}

export interface PrefetchCreateprojectserviceaccountOptions extends CreateprojectserviceaccountInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectServiceAccountCreateResponse, ErrorResponse>>;
}

export function prefetchCreateprojectserviceaccount(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchCreateprojectserviceaccountOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: createprojectserviceaccountQueryKey(init),
        queryFn: () => createprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectserviceaccountQueryOptions extends CreateprojectserviceaccountInit {
    queryOptions?: Partial<UseQueryOptions<ProjectServiceAccountCreateResponse, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/service_accounts`
 *
 * Creates a new service account in the project. This also returns an unredacted API key for the service account.
 */
export function useCreateprojectserviceaccountQuery(options: UseCreateprojectserviceaccountQueryOptions): UseQueryResult<ProjectServiceAccountCreateResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: createprojectserviceaccountQueryKey(init),
        queryFn: () => createprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseCreateprojectserviceaccountSuspenseQueryOptions extends CreateprojectserviceaccountInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectServiceAccountCreateResponse, ErrorResponse>>;
}

/**
 * `POST /organization/projects/{project_id}/service_accounts`
 *
 * Creates a new service account in the project. This also returns an unredacted API key for the service account.
 */
export function useCreateprojectserviceaccountSuspenseQuery(options: UseCreateprojectserviceaccountSuspenseQueryOptions): UseSuspenseQueryResult<ProjectServiceAccountCreateResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: createprojectserviceaccountQueryKey(init),
        queryFn: () => createprojectserviceaccount(client, init),
        ...queryOptions
    });
}

/**
 * `POST /organization/projects/{project_id}/service_accounts`
 *
 * Creates a new service account in the project. This also returns an unredacted API key for the service account.
 */
export function useCreateprojectserviceaccountMutation(options: Partial<UseMutationOptions<ProjectServiceAccountCreateResponse, ErrorResponse, CreateprojectserviceaccountInit>> = {}): UseMutationResult<ProjectServiceAccountCreateResponse, ErrorResponse, CreateprojectserviceaccountInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: CreateprojectserviceaccountInit) => createprojectserviceaccount(client, init),
        ...options
    });
}

export interface RetrieveprojectserviceaccountInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the service account.
         */
        service_account_id: string;
    };
    request?: RequestInit;
}

function retrieveprojectserviceaccount(client: OpenAPIClient, init: RetrieveprojectserviceaccountInit): Promise<ProjectServiceAccount> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/service_accounts/{service_account_id}", init);
}

export function retrieveprojectserviceaccountQueryKey(init: RetrieveprojectserviceaccountInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/service_accounts/{service_account_id}", init];
}

export interface PrefetchRetrieveprojectserviceaccountOptions extends RetrieveprojectserviceaccountInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectServiceAccount, unknown>>;
}

export function prefetchRetrieveprojectserviceaccount(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveprojectserviceaccountOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveprojectserviceaccountQueryKey(init),
        queryFn: () => retrieveprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectserviceaccountQueryOptions extends RetrieveprojectserviceaccountInit {
    queryOptions?: Partial<UseQueryOptions<ProjectServiceAccount, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/service_accounts/{service_account_id}`
 *
 * Retrieves a service account in the project.
 */
export function useRetrieveprojectserviceaccountQuery(options: UseRetrieveprojectserviceaccountQueryOptions): UseQueryResult<ProjectServiceAccount, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveprojectserviceaccountQueryKey(init),
        queryFn: () => retrieveprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectserviceaccountSuspenseQueryOptions extends RetrieveprojectserviceaccountInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectServiceAccount, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/service_accounts/{service_account_id}`
 *
 * Retrieves a service account in the project.
 */
export function useRetrieveprojectserviceaccountSuspenseQuery(options: UseRetrieveprojectserviceaccountSuspenseQueryOptions): UseSuspenseQueryResult<ProjectServiceAccount, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveprojectserviceaccountQueryKey(init),
        queryFn: () => retrieveprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface DeleteprojectserviceaccountInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the service account.
         */
        service_account_id: string;
    };
    request?: RequestInit;
}

function deleteprojectserviceaccount(client: OpenAPIClient, init: DeleteprojectserviceaccountInit): Promise<ProjectServiceAccountDeleteResponse> {
    return client[internal_fetch]("DELETE", "/organization/projects/{project_id}/service_accounts/{service_account_id}", init);
}

export function deleteprojectserviceaccountQueryKey(init: DeleteprojectserviceaccountInit): QueryKey {
    return ["delete", "/organization/projects/{project_id}/service_accounts/{service_account_id}", init];
}

export interface PrefetchDeleteprojectserviceaccountOptions extends DeleteprojectserviceaccountInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectServiceAccountDeleteResponse, unknown>>;
}

export function prefetchDeleteprojectserviceaccount(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteprojectserviceaccountOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteprojectserviceaccountQueryKey(init),
        queryFn: () => deleteprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectserviceaccountQueryOptions extends DeleteprojectserviceaccountInit {
    queryOptions?: Partial<UseQueryOptions<ProjectServiceAccountDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/projects/{project_id}/service_accounts/{service_account_id}`
 *
 * Deletes a service account from the project.
 */
export function useDeleteprojectserviceaccountQuery(options: UseDeleteprojectserviceaccountQueryOptions): UseQueryResult<ProjectServiceAccountDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteprojectserviceaccountQueryKey(init),
        queryFn: () => deleteprojectserviceaccount(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectserviceaccountSuspenseQueryOptions extends DeleteprojectserviceaccountInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectServiceAccountDeleteResponse, unknown>>;
}

/**
 * `DELETE /organization/projects/{project_id}/service_accounts/{service_account_id}`
 *
 * Deletes a service account from the project.
 */
export function useDeleteprojectserviceaccountSuspenseQuery(options: UseDeleteprojectserviceaccountSuspenseQueryOptions): UseSuspenseQueryResult<ProjectServiceAccountDeleteResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteprojectserviceaccountQueryKey(init),
        queryFn: () => deleteprojectserviceaccount(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /organization/projects/{project_id}/service_accounts/{service_account_id}`
 *
 * Deletes a service account from the project.
 */
export function useDeleteprojectserviceaccountMutation(options: Partial<UseMutationOptions<ProjectServiceAccountDeleteResponse, unknown, DeleteprojectserviceaccountInit>> = {}): UseMutationResult<ProjectServiceAccountDeleteResponse, unknown, DeleteprojectserviceaccountInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteprojectserviceaccountInit) => deleteprojectserviceaccount(client, init),
        ...options
    });
}

export interface ListprojectapikeysInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
    };
    query?: {
        /**
         * A limit on the number of objects to be returned. Limit can range between 1 and 100, and the default is 20.
         *
         * @defaultValue `20`
         */
        limit?: number;
        /**
         * A cursor for use in pagination. `after` is an object ID that defines your place in the list. For instance, if you make a list request and receive 100 objects, ending with obj_foo, your subsequent call can include after=obj_foo in order to fetch the next page of the list.
         *
         */
        after?: string;
    };
    request?: RequestInit;
}

function listprojectapikeys(client: OpenAPIClient, init: ListprojectapikeysInit): Promise<ProjectApiKeyListResponse> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/api_keys", init);
}

export function listprojectapikeysQueryKey(init: ListprojectapikeysInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/api_keys", init];
}

export interface PrefetchListprojectapikeysOptions extends ListprojectapikeysInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectApiKeyListResponse, unknown>>;
}

export function prefetchListprojectapikeys(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchListprojectapikeysOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: listprojectapikeysQueryKey(init),
        queryFn: () => listprojectapikeys(client, init),
        ...queryOptions
    });
}

export interface UseListprojectapikeysQueryOptions extends ListprojectapikeysInit {
    queryOptions?: Partial<UseQueryOptions<ProjectApiKeyListResponse, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/api_keys`
 *
 * Returns a list of API keys in the project.
 */
export function useListprojectapikeysQuery(options: UseListprojectapikeysQueryOptions): UseQueryResult<ProjectApiKeyListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: listprojectapikeysQueryKey(init),
        queryFn: () => listprojectapikeys(client, init),
        ...queryOptions
    });
}

export interface UseListprojectapikeysSuspenseQueryOptions extends ListprojectapikeysInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectApiKeyListResponse, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/api_keys`
 *
 * Returns a list of API keys in the project.
 */
export function useListprojectapikeysSuspenseQuery(options: UseListprojectapikeysSuspenseQueryOptions): UseSuspenseQueryResult<ProjectApiKeyListResponse, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: listprojectapikeysQueryKey(init),
        queryFn: () => listprojectapikeys(client, init),
        ...queryOptions
    });
}

export interface RetrieveprojectapikeyInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the API key.
         */
        key_id: string;
    };
    request?: RequestInit;
}

function retrieveprojectapikey(client: OpenAPIClient, init: RetrieveprojectapikeyInit): Promise<ProjectApiKey> {
    return client[internal_fetch]("GET", "/organization/projects/{project_id}/api_keys/{key_id}", init);
}

export function retrieveprojectapikeyQueryKey(init: RetrieveprojectapikeyInit): QueryKey {
    return ["get", "/organization/projects/{project_id}/api_keys/{key_id}", init];
}

export interface PrefetchRetrieveprojectapikeyOptions extends RetrieveprojectapikeyInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectApiKey, unknown>>;
}

export function prefetchRetrieveprojectapikey(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchRetrieveprojectapikeyOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: retrieveprojectapikeyQueryKey(init),
        queryFn: () => retrieveprojectapikey(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectapikeyQueryOptions extends RetrieveprojectapikeyInit {
    queryOptions?: Partial<UseQueryOptions<ProjectApiKey, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/api_keys/{key_id}`
 *
 * Retrieves an API key in the project.
 */
export function useRetrieveprojectapikeyQuery(options: UseRetrieveprojectapikeyQueryOptions): UseQueryResult<ProjectApiKey, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: retrieveprojectapikeyQueryKey(init),
        queryFn: () => retrieveprojectapikey(client, init),
        ...queryOptions
    });
}

export interface UseRetrieveprojectapikeySuspenseQueryOptions extends RetrieveprojectapikeyInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectApiKey, unknown>>;
}

/**
 * `GET /organization/projects/{project_id}/api_keys/{key_id}`
 *
 * Retrieves an API key in the project.
 */
export function useRetrieveprojectapikeySuspenseQuery(options: UseRetrieveprojectapikeySuspenseQueryOptions): UseSuspenseQueryResult<ProjectApiKey, unknown> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: retrieveprojectapikeyQueryKey(init),
        queryFn: () => retrieveprojectapikey(client, init),
        ...queryOptions
    });
}

export interface DeleteprojectapikeyInit {
    path: {
        /**
         * The ID of the project.
         */
        project_id: string;
        /**
         * The ID of the API key.
         */
        key_id: string;
    };
    request?: RequestInit;
}

function deleteprojectapikey(client: OpenAPIClient, init: DeleteprojectapikeyInit): Promise<ProjectApiKeyDeleteResponse> {
    return client[internal_fetch]("DELETE", "/organization/projects/{project_id}/api_keys/{key_id}", init);
}

export function deleteprojectapikeyQueryKey(init: DeleteprojectapikeyInit): QueryKey {
    return ["delete", "/organization/projects/{project_id}/api_keys/{key_id}", init];
}

export interface PrefetchDeleteprojectapikeyOptions extends DeleteprojectapikeyInit {
    queryOptions?: Partial<FetchQueryOptions<ProjectApiKeyDeleteResponse, ErrorResponse>>;
}

export function prefetchDeleteprojectapikey(client: OpenAPIClient, queryClient: QueryClient, options: PrefetchDeleteprojectapikeyOptions): Promise<void> {
    const { queryOptions, ...init } = options;

    return queryClient.prefetchQuery({
        queryKey: deleteprojectapikeyQueryKey(init),
        queryFn: () => deleteprojectapikey(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectapikeyQueryOptions extends DeleteprojectapikeyInit {
    queryOptions?: Partial<UseQueryOptions<ProjectApiKeyDeleteResponse, ErrorResponse>>;
}

/**
 * `DELETE /organization/projects/{project_id}/api_keys/{key_id}`
 *
 * Deletes an API key from the project.
 */
export function useDeleteprojectapikeyQuery(options: UseDeleteprojectapikeyQueryOptions): UseQueryResult<ProjectApiKeyDeleteResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useQuery({
        queryKey: deleteprojectapikeyQueryKey(init),
        queryFn: () => deleteprojectapikey(client, init),
        ...queryOptions
    });
}

export interface UseDeleteprojectapikeySuspenseQueryOptions extends DeleteprojectapikeyInit {
    queryOptions?: Partial<UseSuspenseQueryOptions<ProjectApiKeyDeleteResponse, ErrorResponse>>;
}

/**
 * `DELETE /organization/projects/{project_id}/api_keys/{key_id}`
 *
 * Deletes an API key from the project.
 */
export function useDeleteprojectapikeySuspenseQuery(options: UseDeleteprojectapikeySuspenseQueryOptions): UseSuspenseQueryResult<ProjectApiKeyDeleteResponse, ErrorResponse> {
    const { queryOptions, ...init } = options;
    const client = useContext(OpenAIAPIContext);

    return useSuspenseQuery({
        queryKey: deleteprojectapikeyQueryKey(init),
        queryFn: () => deleteprojectapikey(client, init),
        ...queryOptions
    });
}

/**
 * `DELETE /organization/projects/{project_id}/api_keys/{key_id}`
 *
 * Deletes an API key from the project.
 */
export function useDeleteprojectapikeyMutation(options: Partial<UseMutationOptions<ProjectApiKeyDeleteResponse, ErrorResponse, DeleteprojectapikeyInit>> = {}): UseMutationResult<ProjectApiKeyDeleteResponse, ErrorResponse, DeleteprojectapikeyInit> {
    const client = useContext(OpenAIAPIContext);

    return useMutation({
        mutationFn: (init: DeleteprojectapikeyInit) => deleteprojectapikey(client, init),
        ...options
    });
}

export interface Error {
    code: string | null;
    message: string;
    param: string | null;
    type: string;
}

export interface ErrorResponse {
    error: Error;
}

export interface ListModelsResponse {
    object: "list";
    data: Model[];
}

export interface DeleteModelResponse {
    id: string;
    deleted: boolean;
    object: string;
}

export interface CreateCompletionRequest {
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     *
     */
    model: string | ("gpt-3.5-turbo-instruct" | "davinci-002" | "babbage-002") | (string & ("gpt-3.5-turbo-instruct" | "davinci-002" | "babbage-002"));
    /**
     * The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
     *
     * Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
     *
     * @defaultValue `<|endoftext|>`
     */
    prompt: string | string[] | number[] | number[][];
    /**
     * Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.
     *
     * When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return  `best_of` must be greater than `n`.
     *
     * **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     *
     * @maximum `20`
     * @minimum `0`
     * @defaultValue `1`
     */
    best_of?: number | null;
    /**
     * Echo back the prompt in addition to the completion
     *
     * @defaultValue `false`
     */
    echo?: boolean | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     *
     * @maximum `2`
     * @minimum `-2`
     * @defaultValue `0`
     */
    frequency_penalty?: number | null;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     *
     * As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
     *
     * @defaultValue `null`
     */
    logit_bias?: Record<string, number> | null;
    /**
     * Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.
     *
     * The maximum value for `logprobs` is 5.
     *
     * @maximum `5`
     * @minimum `0`
     * @defaultValue `null`
     */
    logprobs?: number | null;
    /**
     * The maximum number of [tokens](/tokenizer) that can be generated in the completion.
     *
     * The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     *
     * @minimum `0`
     * @defaultValue `16`
     */
    max_tokens?: number | null;
    /**
     * How many completions to generate for each prompt.
     *
     * **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
     *
     * @maximum `128`
     * @minimum `1`
     * @defaultValue `1`
     */
    n?: number | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     *
     * @maximum `2`
     * @minimum `-2`
     * @defaultValue `0`
     */
    presence_penalty?: number | null;
    /**
     * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
     *
     * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     *
     * @maximum `9223372036854776000`
     * @minimum `-9223372036854776000`
     */
    seed?: number | null;
    /**
     * Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
     *
     * @defaultValue `null`
     */
    stop?: (string | null) | string[];
    /**
     * Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
     *
     * @defaultValue `false`
     */
    stream?: boolean | null;
    stream_options?: ChatCompletionStreamOptions;
    /**
     * The suffix that comes after a completion of inserted text.
     *
     * This parameter is only supported for `gpt-3.5-turbo-instruct`.
     *
     * @defaultValue `null`
     */
    suffix?: string | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
}

/**
 * Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
 *
 */
export interface CreateCompletionResponse {
    /**
     * A unique identifier for the completion.
     */
    id: string;
    /**
     * The list of completion choices the model generated for the input prompt.
     */
    choices: {
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
         * `length` if the maximum number of tokens specified in the request was reached,
         * or `content_filter` if content was omitted due to a flag from our content filters.
         *
         */
        finish_reason: "stop" | "length" | "content_filter";
        index: number;
        logprobs: {
            text_offset?: number[];
            token_logprobs?: number[];
            tokens?: string[];
            top_logprobs?: Record<string, number>[];
        } | null;
        text: string;
    }[];
    /**
     * The Unix timestamp (in seconds) of when the completion was created.
     */
    created: number;
    /**
     * The model used for completion.
     */
    model: string;
    /**
     * This fingerprint represents the backend configuration that the model runs with.
     *
     * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
     *
     */
    system_fingerprint?: string;
    /**
     * The object type, which is always "text_completion"
     */
    object: "text_completion";
    usage?: CompletionUsage;
}

export interface ChatCompletionRequestMessageContentPartText {
    /**
     * The type of the content part.
     */
    type: "text";
    /**
     * The text content.
     */
    text: string;
}

export interface ChatCompletionRequestMessageContentPartImage {
    /**
     * The type of the content part.
     */
    type: "image_url";
    image_url: {
        /**
         * Either a URL of the image or the base64 encoded image data.
         * @format `uri`
         */
        url: string;
        /**
         * Specifies the detail level of the image. Learn more in the [Vision guide](/docs/guides/vision/low-or-high-fidelity-image-understanding).
         * @defaultValue `auto`
         */
        detail?: "auto" | "low" | "high";
    };
}

export interface ChatCompletionRequestMessageContentPartRefusal {
    /**
     * The type of the content part.
     */
    type: "refusal";
    /**
     * The refusal message generated by the model.
     */
    refusal: string;
}

export type ChatCompletionRequestMessage = ChatCompletionRequestSystemMessage | ChatCompletionRequestUserMessage | ChatCompletionRequestAssistantMessage | ChatCompletionRequestToolMessage | ChatCompletionRequestFunctionMessage;

export type ChatCompletionRequestSystemMessageContentPart = ChatCompletionRequestMessageContentPartText;

export type ChatCompletionRequestUserMessageContentPart = ChatCompletionRequestMessageContentPartText | ChatCompletionRequestMessageContentPartImage;

export type ChatCompletionRequestAssistantMessageContentPart = ChatCompletionRequestMessageContentPartText | ChatCompletionRequestMessageContentPartRefusal;

export type ChatCompletionRequestToolMessageContentPart = ChatCompletionRequestMessageContentPartText;

export interface ChatCompletionRequestSystemMessage {
    /**
     * The contents of the system message.
     */
    content: string | ChatCompletionRequestSystemMessageContentPart[];
    /**
     * The role of the messages author, in this case `system`.
     */
    role: "system";
    /**
     * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
     */
    name?: string;
}

export interface ChatCompletionRequestUserMessage {
    /**
     * The contents of the user message.
     *
     */
    content: string | ChatCompletionRequestUserMessageContentPart[];
    /**
     * The role of the messages author, in this case `user`.
     */
    role: "user";
    /**
     * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
     */
    name?: string;
}

export interface ChatCompletionRequestAssistantMessage {
    /**
     * The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified.
     *
     */
    content?: string | ChatCompletionRequestAssistantMessageContentPart[];
    /**
     * The refusal message by the assistant.
     */
    refusal?: string | null;
    /**
     * The role of the messages author, in this case `assistant`.
     */
    role: "assistant";
    /**
     * An optional name for the participant. Provides the model information to differentiate between participants of the same role.
     */
    name?: string;
    tool_calls?: ChatCompletionMessageToolCalls;
    /**
     * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
     * @deprecated
     */
    function_call?: {
        /**
         * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
         */
        arguments: string;
        /**
         * The name of the function to call.
         */
        name: string;
    } | null;
}

export type FineTuneChatCompletionRequestAssistantMessage = {
    /**
     * Controls whether the assistant message is trained against (0 or 1)
     */
    weight?: 0 | 1;
} & ChatCompletionRequestAssistantMessage;

export interface ChatCompletionRequestToolMessage {
    /**
     * The role of the messages author, in this case `tool`.
     */
    role: "tool";
    /**
     * The contents of the tool message.
     */
    content: string | ChatCompletionRequestToolMessageContentPart[];
    /**
     * Tool call that this message is responding to.
     */
    tool_call_id: string;
}

/**
 * @deprecated
 */
export interface ChatCompletionRequestFunctionMessage {
    /**
     * The role of the messages author, in this case `function`.
     */
    role: "function";
    /**
     * The contents of the function message.
     */
    content: string | null;
    /**
     * The name of the function to call.
     */
    name: string;
}

/**
 * The parameters the functions accepts, described as a JSON Schema object. See the [guide](/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.
 *
 * Omitting `parameters` defines a function with an empty parameter list.
 */
export type FunctionParameters = Record<string, unknown>;

/**
 * @deprecated
 */
export interface ChatCompletionFunctions {
    /**
     * A description of what the function does, used by the model to choose when and how to call the function.
     */
    description?: string;
    /**
     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
     */
    name: string;
    parameters?: FunctionParameters;
}

/**
 * Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
 *
 */
export interface ChatCompletionFunctionCallOption {
    /**
     * The name of the function to call.
     */
    name: string;
}

export interface ChatCompletionTool {
    /**
     * The type of the tool. Currently, only `function` is supported.
     */
    type: "function";
    function: FunctionObject;
}

export interface FunctionObject {
    /**
     * A description of what the function does, used by the model to choose when and how to call the function.
     */
    description?: string;
    /**
     * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
     */
    name: string;
    parameters?: FunctionParameters;
    /**
     * Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).
     * @defaultValue `false`
     */
    strict?: boolean | null;
}

export interface ResponseFormatText {
    /**
     * The type of response format being defined: `text`
     */
    type: "text";
}

export interface ResponseFormatJsonObject {
    /**
     * The type of response format being defined: `json_object`
     */
    type: "json_object";
}

/**
 * The schema for the response format, described as a JSON Schema object.
 */
export type ResponseFormatJsonSchemaSchema = Record<string, unknown>;

export interface ResponseFormatJsonSchema {
    /**
     * The type of response format being defined: `json_schema`
     */
    type: "json_schema";
    json_schema: {
        /**
         * A description of what the response format is for, used by the model to determine how to respond in the format.
         */
        description?: string;
        /**
         * The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
         */
        name: string;
        schema?: ResponseFormatJsonSchemaSchema;
        /**
         * Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. To learn more, read the [Structured Outputs guide](/docs/guides/structured-outputs).
         * @defaultValue `false`
         */
        strict?: boolean | null;
    };
}

/**
 * Controls which (if any) tool is called by the model.
 * `none` means the model will not call any tool and instead generates a message.
 * `auto` means the model can pick between generating a message or calling one or more tools.
 * `required` means the model must call one or more tools.
 * Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.
 *
 * `none` is the default when no tools are present. `auto` is the default if tools are present.
 *
 */
export type ChatCompletionToolChoiceOption = ("none" | "auto" | "required") | ChatCompletionNamedToolChoice;

/**
 * Specifies a tool the model should use. Use to force the model to call a specific function.
 */
export interface ChatCompletionNamedToolChoice {
    /**
     * The type of the tool. Currently, only `function` is supported.
     */
    type: "function";
    function: {
        /**
         * The name of the function to call.
         */
        name: string;
    };
}

/**
 * Whether to enable [parallel function calling](/docs/guides/function-calling/parallel-function-calling) during tool use.
 * @defaultValue `true`
 */
export type ParallelToolCalls = boolean;

/**
 * The tool calls generated by the model, such as function calls.
 */
export type ChatCompletionMessageToolCalls = ChatCompletionMessageToolCall[];

export interface ChatCompletionMessageToolCall {
    /**
     * The ID of the tool call.
     */
    id: string;
    /**
     * The type of the tool. Currently, only `function` is supported.
     */
    type: "function";
    /**
     * The function that the model called.
     */
    function: {
        /**
         * The name of the function to call.
         */
        name: string;
        /**
         * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
         */
        arguments: string;
    };
}

export interface ChatCompletionMessageToolCallChunk {
    index: number;
    /**
     * The ID of the tool call.
     */
    id?: string;
    /**
     * The type of the tool. Currently, only `function` is supported.
     */
    type?: "function";
    function?: {
        /**
         * The name of the function to call.
         */
        name?: string;
        /**
         * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
         */
        arguments?: string;
    };
}

/**
 * The role of the author of a message
 */
export type ChatCompletionRole = "system" | "user" | "assistant" | "tool" | "function";

/**
 * Options for streaming response. Only set this when you set `stream: true`.
 *
 * @defaultValue `null`
 */
export type ChatCompletionStreamOptions = {
    /**
     * If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.
     *
     */
    include_usage?: boolean;
} | null;

/**
 * A chat completion message generated by the model.
 */
export interface ChatCompletionResponseMessage {
    /**
     * The contents of the message.
     */
    content: string | null;
    /**
     * The refusal message generated by the model.
     */
    refusal: string | null;
    tool_calls?: ChatCompletionMessageToolCalls;
    /**
     * The role of the author of this message.
     */
    role: "assistant";
    /**
     * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
     * @deprecated
     */
    function_call?: {
        /**
         * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
         */
        arguments: string;
        /**
         * The name of the function to call.
         */
        name: string;
    };
}

/**
 * A chat completion delta generated by streamed model responses.
 */
export interface ChatCompletionStreamResponseDelta {
    /**
     * The contents of the chunk message.
     */
    content?: string | null;
    /**
     * Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.
     * @deprecated
     */
    function_call?: {
        /**
         * The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
         */
        arguments?: string;
        /**
         * The name of the function to call.
         */
        name?: string;
    };
    tool_calls?: ChatCompletionMessageToolCallChunk[];
    /**
     * The role of the author of this message.
     */
    role?: "system" | "user" | "assistant" | "tool";
    /**
     * The refusal message generated by the model.
     */
    refusal?: string | null;
}

export interface CreateChatCompletionRequest {
    /**
     * A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
     * @minimum `1`
     */
    messages: ChatCompletionRequestMessage[];
    /**
     * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
     */
    model: string | ("gpt-4o" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "chatgpt-4o-latest" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0301" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613") | (string & ("gpt-4o" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "chatgpt-4o-latest" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0301" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613"));
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     *
     * @maximum `2`
     * @minimum `-2`
     * @defaultValue `0`
     */
    frequency_penalty?: number | null;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
     *
     * @defaultValue `null`
     */
    logit_bias?: Record<string, number> | null;
    /**
     * Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
     * @defaultValue `false`
     */
    logprobs?: boolean | null;
    /**
     * An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
     * @maximum `20`
     * @minimum `0`
     */
    top_logprobs?: number | null;
    /**
     * The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.
     *
     * The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     *
     */
    max_tokens?: number | null;
    /**
     * How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
     * @maximum `128`
     * @minimum `1`
     * @defaultValue `1`
     */
    n?: number | null;
    /**
     * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
     *
     * [See more information about frequency and presence penalties.](/docs/guides/text-generation/parameter-details)
     *
     * @maximum `2`
     * @minimum `-2`
     * @defaultValue `0`
     */
    presence_penalty?: number | null;
    /**
     * An object specifying the format that the model must output. Compatible with [GPT-4o](/docs/models/gpt-4o), [GPT-4o mini](/docs/models/gpt-4o-mini), [GPT-4 Turbo](/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
     *
     * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which guarantees the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).
     *
     * Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
     *
     * **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
     *
     */
    response_format?: ResponseFormatText | ResponseFormatJsonObject | ResponseFormatJsonSchema;
    /**
     * This feature is in Beta.
     * If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
     * Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
     *
     * @maximum `9223372036854776000`
     * @minimum `-9223372036854776000`
     */
    seed?: number | null;
    /**
     * Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
     *   - If set to 'auto', the system will utilize scale tier credits until they are exhausted.
     *   - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
     *   - When not set, the default behavior is 'auto'.
     *
     *   When this parameter is set, the response body will include the `service_tier` utilized.
     *
     * @defaultValue `null`
     */
    service_tier?: ("auto" | "default") | null;
    /**
     * Up to 4 sequences where the API will stop generating further tokens.
     *
     * @defaultValue `null`
     */
    stop?: (string | null) | string[];
    /**
     * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
     *
     * @defaultValue `false`
     */
    stream?: boolean | null;
    stream_options?: ChatCompletionStreamOptions;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * We generally recommend altering this or `top_p` but not both.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or `temperature` but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    /**
     * A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
     *
     */
    tools?: ChatCompletionTool[];
    tool_choice?: ChatCompletionToolChoiceOption;
    parallel_tool_calls?: ParallelToolCalls;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
    /**
     * Deprecated in favor of `tool_choice`.
     *
     * Controls which (if any) function is called by the model.
     * `none` means the model will not call a function and instead generates a message.
     * `auto` means the model can pick between generating a message or calling a function.
     * Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
     *
     * `none` is the default when no functions are present. `auto` is the default if functions are present.
     *
     * @deprecated
     */
    function_call?: ("none" | "auto") | ChatCompletionFunctionCallOption;
    /**
     * Deprecated in favor of `tools`.
     *
     * A list of functions the model may generate JSON inputs for.
     *
     * @maximum `128`
     * @minimum `1`
     * @deprecated
     */
    functions?: ChatCompletionFunctions[];
}

/**
 * Represents a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionResponse {
    /**
     * A unique identifier for the chat completion.
     */
    id: string;
    /**
     * A list of chat completion choices. Can be more than one if `n` is greater than 1.
     */
    choices: {
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
         * `length` if the maximum number of tokens specified in the request was reached,
         * `content_filter` if content was omitted due to a flag from our content filters,
         * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
         *
         */
        finish_reason: "stop" | "length" | "tool_calls" | "content_filter" | "function_call";
        /**
         * The index of the choice in the list of choices.
         */
        index: number;
        message: ChatCompletionResponseMessage;
        /**
         * Log probability information for the choice.
         */
        logprobs: {
            /**
             * A list of message content tokens with log probability information.
             */
            content: ChatCompletionTokenLogprob[] | null;
            /**
             * A list of message refusal tokens with log probability information.
             */
            refusal: ChatCompletionTokenLogprob[] | null;
        } | null;
    }[];
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created.
     */
    created: number;
    /**
     * The model used for the chat completion.
     */
    model: string;
    /**
     * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
     */
    service_tier?: ("scale" | "default") | null;
    /**
     * This fingerprint represents the backend configuration that the model runs with.
     *
     * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
     *
     */
    system_fingerprint?: string;
    /**
     * The object type, which is always `chat.completion`.
     */
    object: "chat.completion";
    usage?: CompletionUsage;
}

/**
 * Represents a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionFunctionResponse {
    /**
     * A unique identifier for the chat completion.
     */
    id: string;
    /**
     * A list of chat completion choices. Can be more than one if `n` is greater than 1.
     */
    choices: {
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, or `function_call` if the model called a function.
         *
         */
        finish_reason: "stop" | "length" | "function_call" | "content_filter";
        /**
         * The index of the choice in the list of choices.
         */
        index: number;
        message: ChatCompletionResponseMessage;
    }[];
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created.
     */
    created: number;
    /**
     * The model used for the chat completion.
     */
    model: string;
    /**
     * This fingerprint represents the backend configuration that the model runs with.
     *
     * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
     *
     */
    system_fingerprint?: string;
    /**
     * The object type, which is always `chat.completion`.
     */
    object: "chat.completion";
    usage?: CompletionUsage;
}

export interface ChatCompletionTokenLogprob {
    /**
     * The token.
     */
    token: string;
    /**
     * The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
     */
    logprob: number;
    /**
     * A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
     */
    bytes: number[] | null;
    /**
     * List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
     */
    top_logprobs: {
        /**
         * The token.
         */
        token: string;
        /**
         * The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
         */
        logprob: number;
        /**
         * A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
         */
        bytes: number[] | null;
    }[];
}

export interface ListPaginatedFineTuningJobsResponse {
    data: FineTuningJob[];
    has_more: boolean;
    object: "list";
}

/**
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 */
export interface CreateChatCompletionStreamResponse {
    /**
     * A unique identifier for the chat completion. Each chunk has the same ID.
     */
    id: string;
    /**
     * A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
     * last chunk if you set `stream_options: {"include_usage": true}`.
     *
     */
    choices: {
        delta: ChatCompletionStreamResponseDelta;
        /**
         * Log probability information for the choice.
         */
        logprobs?: {
            /**
             * A list of message content tokens with log probability information.
             */
            content: ChatCompletionTokenLogprob[] | null;
            /**
             * A list of message refusal tokens with log probability information.
             */
            refusal: ChatCompletionTokenLogprob[] | null;
        } | null;
        /**
         * The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
         * `length` if the maximum number of tokens specified in the request was reached,
         * `content_filter` if content was omitted due to a flag from our content filters,
         * `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
         *
         */
        finish_reason: ("stop" | "length" | "tool_calls" | "content_filter" | "function_call") | null;
        /**
         * The index of the choice in the list of choices.
         */
        index: number;
    }[];
    /**
     * The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
     */
    created: number;
    /**
     * The model to generate the completion.
     */
    model: string;
    /**
     * The service tier used for processing the request. This field is only included if the `service_tier` parameter is specified in the request.
     */
    service_tier?: ("scale" | "default") | null;
    /**
     * This fingerprint represents the backend configuration that the model runs with.
     * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
     *
     */
    system_fingerprint?: string;
    /**
     * The object type, which is always `chat.completion.chunk`.
     */
    object: "chat.completion.chunk";
    /**
     * An optional field that will only be present when you set `stream_options: {"include_usage": true}` in your request.
     * When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
     *
     */
    usage?: {
        /**
         * Number of tokens in the generated completion.
         */
        completion_tokens: number;
        /**
         * Number of tokens in the prompt.
         */
        prompt_tokens: number;
        /**
         * Total number of tokens used in the request (prompt + completion).
         */
        total_tokens: number;
    };
}

/**
 * Represents a streamed chunk of a chat completion response returned by model, based on the provided input.
 */
export type CreateChatCompletionImageResponse = unknown;

export interface CreateImageRequest {
    /**
     * A text description of the desired image(s). The maximum length is 1000 characters for `dall-e-2` and 4000 characters for `dall-e-3`.
     */
    prompt: string;
    /**
     * The model to use for image generation.
     * @defaultValue `dall-e-2`
     */
    model?: string | ("dall-e-2" | "dall-e-3") | (string & ("dall-e-2" | "dall-e-3"));
    /**
     * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported.
     * @maximum `10`
     * @minimum `1`
     * @defaultValue `1`
     */
    n?: number | null;
    /**
     * The quality of the image that will be generated. `hd` creates images with finer details and greater consistency across the image. This param is only supported for `dall-e-3`.
     * @defaultValue `standard`
     */
    quality?: "standard" | "hd";
    /**
     * The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
     * @defaultValue `url`
     */
    response_format?: ("url" | "b64_json") | null;
    /**
     * The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024` for `dall-e-2`. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for `dall-e-3` models.
     * @defaultValue `1024x1024`
     */
    size?: ("256x256" | "512x512" | "1024x1024" | "1792x1024" | "1024x1792") | null;
    /**
     * The style of the generated images. Must be one of `vivid` or `natural`. Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images. This param is only supported for `dall-e-3`.
     * @defaultValue `vivid`
     */
    style?: ("vivid" | "natural") | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
}

export type ImagesResponse = unknown;

/**
 * Represents the url or the content of an image generated by the OpenAI API.
 */
export interface Image {
    /**
     * The base64-encoded JSON of the generated image, if `response_format` is `b64_json`.
     */
    b64_json?: string;
    /**
     * The URL of the generated image, if `response_format` is `url` (default).
     */
    url?: string;
    /**
     * The prompt that was used to generate the image, if there was any revision to the prompt.
     */
    revised_prompt?: string;
}

export interface CreateImageEditRequest {
    /**
     * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not provided, image must have transparency, which will be used as the mask.
     * @format `binary`
     */
    image: Blob;
    /**
     * A text description of the desired image(s). The maximum length is 1000 characters.
     */
    prompt: string;
    /**
     * An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions as `image`.
     * @format `binary`
     */
    mask?: Blob;
    /**
     * The model to use for image generation. Only `dall-e-2` is supported at this time.
     * @defaultValue `dall-e-2`
     */
    model?: string | "dall-e-2" | (string & "dall-e-2");
    /**
     * The number of images to generate. Must be between 1 and 10.
     * @maximum `10`
     * @minimum `1`
     * @defaultValue `1`
     */
    n?: number | null;
    /**
     * The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
     * @defaultValue `1024x1024`
     */
    size?: ("256x256" | "512x512" | "1024x1024") | null;
    /**
     * The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
     * @defaultValue `url`
     */
    response_format?: ("url" | "b64_json") | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
}

export interface CreateImageVariationRequest {
    /**
     * The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB, and square.
     * @format `binary`
     */
    image: Blob;
    /**
     * The model to use for image generation. Only `dall-e-2` is supported at this time.
     * @defaultValue `dall-e-2`
     */
    model?: string | "dall-e-2" | (string & "dall-e-2");
    /**
     * The number of images to generate. Must be between 1 and 10. For `dall-e-3`, only `n=1` is supported.
     * @maximum `10`
     * @minimum `1`
     * @defaultValue `1`
     */
    n?: number | null;
    /**
     * The format in which the generated images are returned. Must be one of `url` or `b64_json`. URLs are only valid for 60 minutes after the image has been generated.
     * @defaultValue `url`
     */
    response_format?: ("url" | "b64_json") | null;
    /**
     * The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`.
     * @defaultValue `1024x1024`
     */
    size?: ("256x256" | "512x512" | "1024x1024") | null;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
}

export interface CreateModerationRequest {
    /**
     * The input text to classify
     */
    input: string | string[];
    /**
     * Two content moderations models are available: `text-moderation-stable` and `text-moderation-latest`.
     *
     * The default is `text-moderation-latest` which will be automatically upgraded over time. This ensures you are always using our most accurate model. If you use `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.
     *
     * @defaultValue `text-moderation-latest`
     */
    model?: string | ("text-moderation-latest" | "text-moderation-stable") | (string & ("text-moderation-latest" | "text-moderation-stable"));
}

/**
 * Represents if a given text input is potentially harmful.
 */
export interface CreateModerationResponse {
    /**
     * The unique identifier for the moderation request.
     */
    id: string;
    /**
     * The model used to generate the moderation results.
     */
    model: string;
    /**
     * A list of moderation objects.
     */
    results: {
        /**
         * Whether any of the below categories are flagged.
         */
        flagged: boolean;
        /**
         * A list of the categories, and whether they are flagged or not.
         */
        categories: {
            /**
             * Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups (e.g., chess players) is harassment.
             */
            hate: boolean;
            /**
             * Hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.
             */
            "hate/threatening": boolean;
            /**
             * Content that expresses, incites, or promotes harassing language towards any target.
             */
            harassment: boolean;
            /**
             * Harassment content that also includes violence or serious harm towards any target.
             */
            "harassment/threatening": boolean;
            /**
             * Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.
             */
            "self-harm": boolean;
            /**
             * Content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.
             */
            "self-harm/intent": boolean;
            /**
             * Content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.
             */
            "self-harm/instructions": boolean;
            /**
             * Content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).
             */
            sexual: boolean;
            /**
             * Sexual content that includes an individual who is under 18 years old.
             */
            "sexual/minors": boolean;
            /**
             * Content that depicts death, violence, or physical injury.
             */
            violence: boolean;
            /**
             * Content that depicts death, violence, or physical injury in graphic detail.
             */
            "violence/graphic": boolean;
        };
        /**
         * A list of the categories along with their scores as predicted by model.
         */
        category_scores: {
            /**
             * The score for the category 'hate'.
             */
            hate: number;
            /**
             * The score for the category 'hate/threatening'.
             */
            "hate/threatening": number;
            /**
             * The score for the category 'harassment'.
             */
            harassment: number;
            /**
             * The score for the category 'harassment/threatening'.
             */
            "harassment/threatening": number;
            /**
             * The score for the category 'self-harm'.
             */
            "self-harm": number;
            /**
             * The score for the category 'self-harm/intent'.
             */
            "self-harm/intent": number;
            /**
             * The score for the category 'self-harm/instructions'.
             */
            "self-harm/instructions": number;
            /**
             * The score for the category 'sexual'.
             */
            sexual: number;
            /**
             * The score for the category 'sexual/minors'.
             */
            "sexual/minors": number;
            /**
             * The score for the category 'violence'.
             */
            violence: number;
            /**
             * The score for the category 'violence/graphic'.
             */
            "violence/graphic": number;
        };
    }[];
}

export interface ListFilesResponse {
    data: OpenAIFile[];
    object: "list";
}

export interface CreateFileRequest {
    /**
     * The File object (not file name) to be uploaded.
     *
     * @format `binary`
     */
    file: Blob;
    /**
     * The intended purpose of the uploaded file.
     *
     * Use "assistants" for [Assistants](/docs/api-reference/assistants) and [Message](/docs/api-reference/messages) files, "vision" for Assistants image file inputs, "batch" for [Batch API](/docs/guides/batch), and "fine-tune" for [Fine-tuning](/docs/api-reference/fine-tuning).
     *
     */
    purpose: "assistants" | "batch" | "fine-tune" | "vision";
}

export interface DeleteFileResponse {
    id: string;
    object: "file";
    deleted: boolean;
}

export interface CreateUploadRequest {
    /**
     * The name of the file to upload.
     *
     */
    filename: string;
    /**
     * The intended purpose of the uploaded file.
     *
     * See the [documentation on File purposes](/docs/api-reference/files/create#files-create-purpose).
     *
     */
    purpose: "assistants" | "batch" | "fine-tune" | "vision";
    /**
     * The number of bytes in the file you are uploading.
     *
     */
    bytes: number;
    /**
     * The MIME type of the file.
     *
     * This must fall within the supported MIME types for your file purpose. See the supported MIME types for assistants and vision.
     *
     */
    mime_type: string;
}

export interface AddUploadPartRequest {
    /**
     * The chunk of bytes for this Part.
     *
     * @format `binary`
     */
    data: Blob;
}

export interface CompleteUploadRequest {
    /**
     * The ordered list of Part IDs.
     *
     */
    part_ids: string[];
    /**
     * The optional md5 checksum for the file contents to verify if the bytes uploaded matches what you expect.
     *
     */
    md5?: string;
}

export type CancelUploadRequest = unknown;

export interface CreateFineTuningJobRequest {
    /**
     * The name of the model to fine-tune. You can select one of the
     * [supported models](/docs/guides/fine-tuning/which-models-can-be-fine-tuned).
     *
     */
    model: string | ("babbage-002" | "davinci-002" | "gpt-3.5-turbo" | "gpt-4o-mini") | (string & ("babbage-002" | "davinci-002" | "gpt-3.5-turbo" | "gpt-4o-mini"));
    /**
     * The ID of an uploaded file that contains training data.
     *
     * See [upload file](/docs/api-reference/files/create) for how to upload a file.
     *
     * Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with the purpose `fine-tune`.
     *
     * The contents of the file should differ depending on if the model uses the [chat](/docs/api-reference/fine-tuning/chat-input) or [completions](/docs/api-reference/fine-tuning/completions-input) format.
     *
     * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     *
     */
    training_file: string;
    /**
     * The hyperparameters used for the fine-tuning job.
     */
    hyperparameters?: {
        /**
         * Number of examples in each batch. A larger batch size means that model parameters
         * are updated less frequently, but with lower variance.
         *
         * @defaultValue `auto`
         */
        batch_size?: "auto" | number;
        /**
         * Scaling factor for the learning rate. A smaller learning rate may be useful to avoid
         * overfitting.
         *
         * @defaultValue `auto`
         */
        learning_rate_multiplier?: "auto" | number;
        /**
         * The number of epochs to train the model for. An epoch refers to one full cycle
         * through the training dataset.
         *
         * @defaultValue `auto`
         */
        n_epochs?: "auto" | number;
    };
    /**
     * A string of up to 18 characters that will be added to your fine-tuned model name.
     *
     * For example, a `suffix` of "custom-model-name" would produce a model name like `ft:gpt-4o-mini:openai:custom-model-name:7p4lURel`.
     *
     * @defaultValue `null`
     */
    suffix?: string | null;
    /**
     * The ID of an uploaded file that contains validation data.
     *
     * If you provide this file, the data is used to generate validation
     * metrics periodically during fine-tuning. These metrics can be viewed in
     * the fine-tuning results file.
     * The same data should not be present in both train and validation files.
     *
     * Your dataset must be formatted as a JSONL file. You must upload your file with the purpose `fine-tune`.
     *
     * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     *
     */
    validation_file?: string | null;
    /**
     * A list of integrations to enable for your fine-tuning job.
     */
    integrations?: {
        /**
         * The type of integration to enable. Currently, only "wandb" (Weights and Biases) is supported.
         *
         */
        type: "wandb";
        /**
         * The settings for your integration with Weights and Biases. This payload specifies the project that
         * metrics will be sent to. Optionally, you can set an explicit display name for your run, add tags
         * to your run, and set a default entity (team, username, etc) to be associated with your run.
         *
         */
        wandb: {
            /**
             * The name of the project that the new run will be created under.
             *
             */
            project: string;
            /**
             * A display name to set for the run. If not set, we will use the Job ID as the name.
             *
             */
            name?: string | null;
            /**
             * The entity to use for the run. This allows you to set the team or username of the WandB user that you would
             * like associated with the run. If not set, the default entity for the registered WandB API key is used.
             *
             */
            entity?: string | null;
            /**
             * A list of tags to be attached to the newly created run. These tags are passed through directly to WandB. Some
             * default tags are generated by OpenAI: "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
             *
             */
            tags?: string[];
        };
    }[] | null;
    /**
     * The seed controls the reproducibility of the job. Passing in the same seed and job parameters should produce the same results, but may differ in rare cases.
     * If a seed is not specified, one will be generated for you.
     *
     * @maximum `2147483647`
     * @minimum `0`
     */
    seed?: number | null;
}

export interface ListFineTuningJobEventsResponse {
    data: FineTuningJobEvent[];
    object: "list";
}

export interface ListFineTuningJobCheckpointsResponse {
    data: FineTuningJobCheckpoint[];
    object: "list";
    first_id?: string | null;
    last_id?: string | null;
    has_more: boolean;
}

export interface CreateEmbeddingRequest {
    /**
     * Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a single request, pass an array of strings or array of token arrays. The input must not exceed the max input tokens for the model (8192 tokens for `text-embedding-ada-002`), cannot be an empty string, and any array must be 2048 dimensions or less. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
     *
     */
    input: string | string[] | number[] | number[][];
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     *
     */
    model: string | ("text-embedding-ada-002" | "text-embedding-3-small" | "text-embedding-3-large") | (string & ("text-embedding-ada-002" | "text-embedding-3-small" | "text-embedding-3-large"));
    /**
     * The format to return the embeddings in. Can be either `float` or [`base64`](https://pypi.org/project/pybase64/).
     * @defaultValue `float`
     */
    encoding_format?: "float" | "base64";
    /**
     * The number of dimensions the resulting output embeddings should have. Only supported in `text-embedding-3` and later models.
     *
     * @minimum `1`
     */
    dimensions?: number;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).
     *
     */
    user?: string;
}

export interface CreateEmbeddingResponse {
    /**
     * The list of embeddings generated by the model.
     */
    data: Embedding[];
    /**
     * The name of the model used to generate the embedding.
     */
    model: string;
    /**
     * The object type, which is always "list".
     */
    object: "list";
    /**
     * The usage information for the request.
     */
    usage: {
        /**
         * The number of tokens used by the prompt.
         */
        prompt_tokens: number;
        /**
         * The total number of tokens used by the request.
         */
        total_tokens: number;
    };
}

export interface CreateTranscriptionRequest {
    /**
     * The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
     *
     * @format `binary`
     */
    file: Blob;
    /**
     * ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available.
     *
     */
    model: string | "whisper-1" | (string & "whisper-1");
    /**
     * The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency.
     *
     */
    language?: string;
    /**
     * An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.
     *
     */
    prompt?: string;
    /**
     * The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.
     *
     * @defaultValue `json`
     */
    response_format?: "json" | "text" | "srt" | "verbose_json" | "vtt";
    /**
     * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
     *
     * @defaultValue `0`
     */
    temperature?: number;
    /**
     * The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
     *
     * @defaultValue `segment`
     */
    "timestamp_granularities[]"?: ("word" | "segment")[];
}

/**
 * Represents a transcription response returned by model, based on the provided input.
 */
export interface CreateTranscriptionResponseJson {
    /**
     * The transcribed text.
     */
    text: string;
}

export interface TranscriptionSegment {
    /**
     * Unique identifier of the segment.
     */
    id: number;
    /**
     * Seek offset of the segment.
     */
    seek: number;
    /**
     * Start time of the segment in seconds.
     * @format `float`
     */
    start: number;
    /**
     * End time of the segment in seconds.
     * @format `float`
     */
    end: number;
    /**
     * Text content of the segment.
     */
    text: string;
    /**
     * Array of token IDs for the text content.
     */
    tokens: number[];
    /**
     * Temperature parameter used for generating the segment.
     * @format `float`
     */
    temperature: number;
    /**
     * Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.
     * @format `float`
     */
    avg_logprob: number;
    /**
     * Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.
     * @format `float`
     */
    compression_ratio: number;
    /**
     * Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.
     * @format `float`
     */
    no_speech_prob: number;
}

export interface TranscriptionWord {
    /**
     * The text content of the word.
     */
    word: string;
    /**
     * Start time of the word in seconds.
     * @format `float`
     */
    start: number;
    /**
     * End time of the word in seconds.
     * @format `float`
     */
    end: number;
}

/**
 * Represents a verbose json transcription response returned by model, based on the provided input.
 */
export interface CreateTranscriptionResponseVerboseJson {
    /**
     * The language of the input audio.
     */
    language: string;
    /**
     * The duration of the input audio.
     */
    duration: string;
    /**
     * The transcribed text.
     */
    text: string;
    /**
     * Extracted words and their corresponding timestamps.
     */
    words?: TranscriptionWord[];
    /**
     * Segments of the transcribed text and their corresponding details.
     */
    segments?: TranscriptionSegment[];
}

export interface CreateTranslationRequest {
    /**
     * The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
     *
     * @format `binary`
     */
    file: Blob;
    /**
     * ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available.
     *
     */
    model: string | "whisper-1" | (string & "whisper-1");
    /**
     * An optional text to guide the model's style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English.
     *
     */
    prompt?: string;
    /**
     * The format of the transcript output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.
     *
     * @defaultValue `json`
     */
    response_format?: string;
    /**
     * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
     *
     * @defaultValue `0`
     */
    temperature?: number;
}

export interface CreateTranslationResponseJson {
    text: string;
}

export interface CreateTranslationResponseVerboseJson {
    /**
     * The language of the output translation (always `english`).
     */
    language: string;
    /**
     * The duration of the input audio.
     */
    duration: string;
    /**
     * The translated text.
     */
    text: string;
    /**
     * Segments of the translated text and their corresponding details.
     */
    segments?: TranscriptionSegment[];
}

export interface CreateSpeechRequest {
    /**
     * One of the available [TTS models](/docs/models/tts): `tts-1` or `tts-1-hd`
     *
     */
    model: string | ("tts-1" | "tts-1-hd") | (string & ("tts-1" | "tts-1-hd"));
    /**
     * The text to generate audio for. The maximum length is 4096 characters.
     */
    input: string;
    /**
     * The voice to use when generating the audio. Supported voices are `alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer`. Previews of the voices are available in the [Text to speech guide](/docs/guides/text-to-speech/voice-options).
     */
    voice: "alloy" | "echo" | "fable" | "onyx" | "nova" | "shimmer";
    /**
     * The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.
     * @defaultValue `mp3`
     */
    response_format?: "mp3" | "opus" | "aac" | "flac" | "wav" | "pcm";
    /**
     * The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.
     * @maximum `4`
     * @minimum `0.25`
     * @defaultValue `1`
     */
    speed?: number;
}

/**
 * Describes an OpenAI model offering that can be used with the API.
 */
export type Model = unknown;

/**
 * The `File` object represents a document that has been uploaded to OpenAI.
 */
export type OpenAIFile = unknown;

/**
 * The Upload object can accept byte chunks in the form of Parts.
 *
 */
export interface Upload {
    /**
     * The Upload unique identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The Unix timestamp (in seconds) for when the Upload was created.
     */
    created_at: number;
    /**
     * The name of the file to be uploaded.
     */
    filename: string;
    /**
     * The intended number of bytes to be uploaded.
     */
    bytes: number;
    /**
     * The intended purpose of the file. [Please refer here](/docs/api-reference/files/object#files/object-purpose) for acceptable values.
     */
    purpose: string;
    /**
     * The status of the Upload.
     */
    status: "pending" | "completed" | "cancelled" | "expired";
    /**
     * The Unix timestamp (in seconds) for when the Upload was created.
     */
    expires_at: number;
    /**
     * The object type, which is always "upload".
     */
    object?: "upload";
    file?: OpenAIFile;
}

/**
 * The upload Part represents a chunk of bytes we can add to an Upload object.
 *
 */
export interface UploadPart {
    /**
     * The upload Part unique identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The Unix timestamp (in seconds) for when the Part was created.
     */
    created_at: number;
    /**
     * The ID of the Upload object that this Part was added to.
     */
    upload_id: string;
    /**
     * The object type, which is always `upload.part`.
     */
    object: "upload.part";
}

/**
 * Represents an embedding vector returned by embedding endpoint.
 *
 */
export interface Embedding {
    /**
     * The index of the embedding in the list of embeddings.
     */
    index: number;
    /**
     * The embedding vector, which is a list of floats. The length of vector depends on the model as listed in the [embedding guide](/docs/guides/embeddings).
     *
     */
    embedding: number[];
    /**
     * The object type, which is always "embedding".
     */
    object: "embedding";
}

/**
 * The `fine_tuning.job` object represents a fine-tuning job that has been created through the API.
 *
 */
export interface FineTuningJob {
    /**
     * The object identifier, which can be referenced in the API endpoints.
     */
    id: string;
    /**
     * The Unix timestamp (in seconds) for when the fine-tuning job was created.
     */
    created_at: number;
    /**
     * For fine-tuning jobs that have `failed`, this will contain more information on the cause of the failure.
     */
    error: {
        /**
         * A machine-readable error code.
         */
        code: string;
        /**
         * A human-readable error message.
         */
        message: string;
        /**
         * The parameter that was invalid, usually `training_file` or `validation_file`. This field will be null if the failure was not parameter-specific.
         */
        param: string | null;
    } | null;
    /**
     * The name of the fine-tuned model that is being created. The value will be null if the fine-tuning job is still running.
     */
    fine_tuned_model: string | null;
    /**
     * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be null if the fine-tuning job is still running.
     */
    finished_at: number | null;
    /**
     * The hyperparameters used for the fine-tuning job. See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.
     */
    hyperparameters: {
        /**
         * The number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.
         * "auto" decides the optimal number of epochs based on the size of the dataset. If setting the number manually, we support any number between 1 and 50 epochs.
         * @defaultValue `auto`
         */
        n_epochs: "auto" | number;
    };
    /**
     * The base model that is being fine-tuned.
     */
    model: string;
    /**
     * The object type, which is always "fine_tuning.job".
     */
    object: "fine_tuning.job";
    /**
     * The organization that owns the fine-tuning job.
     */
    organization_id: string;
    /**
     * The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the [Files API](/docs/api-reference/files/retrieve-contents).
     */
    result_files: string[];
    /**
     * The current status of the fine-tuning job, which can be either `validating_files`, `queued`, `running`, `succeeded`, `failed`, or `cancelled`.
     */
    status: "validating_files" | "queued" | "running" | "succeeded" | "failed" | "cancelled";
    /**
     * The total number of billable tokens processed by this fine-tuning job. The value will be null if the fine-tuning job is still running.
     */
    trained_tokens: number | null;
    /**
     * The file ID used for training. You can retrieve the training data with the [Files API](/docs/api-reference/files/retrieve-contents).
     */
    training_file: string;
    /**
     * The file ID used for validation. You can retrieve the validation results with the [Files API](/docs/api-reference/files/retrieve-contents).
     */
    validation_file: string | null;
    /**
     * A list of integrations to enable for this fine-tuning job.
     * @maximum `5`
     */
    integrations?: (FineTuningIntegration)[] | null;
    /**
     * The seed used for the fine-tuning job.
     */
    seed: number;
    /**
     * The Unix timestamp (in seconds) for when the fine-tuning job is estimated to finish. The value will be null if the fine-tuning job is not running.
     */
    estimated_finish?: number | null;
}

export interface FineTuningIntegration {
    /**
     * The type of the integration being enabled for the fine-tuning job
     */
    type: "wandb";
    /**
     * The settings for your integration with Weights and Biases. This payload specifies the project that
     * metrics will be sent to. Optionally, you can set an explicit display name for your run, add tags
     * to your run, and set a default entity (team, username, etc) to be associated with your run.
     *
     */
    wandb: {
        /**
         * The name of the project that the new run will be created under.
         *
         */
        project: string;
        /**
         * A display name to set for the run. If not set, we will use the Job ID as the name.
         *
         */
        name?: string | null;
        /**
         * The entity to use for the run. This allows you to set the team or username of the WandB user that you would
         * like associated with the run. If not set, the default entity for the registered WandB API key is used.
         *
         */
        entity?: string | null;
        /**
         * A list of tags to be attached to the newly created run. These tags are passed through directly to WandB. Some
         * default tags are generated by OpenAI: "openai/finetune", "openai/{base-model}", "openai/{ftjob-abcdef}".
         *
         */
        tags?: string[];
    };
}

/**
 * Fine-tuning job event object
 */
export interface FineTuningJobEvent {
    id: string;
    created_at: number;
    level: "info" | "warn" | "error";
    message: string;
    object: "fine_tuning.job.event";
}

/**
 * The `fine_tuning.job.checkpoint` object represents a model checkpoint for a fine-tuning job that is ready to use.
 *
 */
export interface FineTuningJobCheckpoint {
    /**
     * The checkpoint identifier, which can be referenced in the API endpoints.
     */
    id: string;
    /**
     * The Unix timestamp (in seconds) for when the checkpoint was created.
     */
    created_at: number;
    /**
     * The name of the fine-tuned checkpoint model that is created.
     */
    fine_tuned_model_checkpoint: string;
    /**
     * The step number that the checkpoint was created at.
     */
    step_number: number;
    /**
     * Metrics at the step number during the fine-tuning job.
     */
    metrics: {
        step?: number;
        train_loss?: number;
        train_mean_token_accuracy?: number;
        valid_loss?: number;
        valid_mean_token_accuracy?: number;
        full_valid_loss?: number;
        full_valid_mean_token_accuracy?: number;
    };
    /**
     * The name of the fine-tuning job that this checkpoint was created from.
     */
    fine_tuning_job_id: string;
    /**
     * The object type, which is always "fine_tuning.job.checkpoint".
     */
    object: "fine_tuning.job.checkpoint";
}

/**
 * The per-line training example of a fine-tuning input file for chat models
 */
export interface FinetuneChatRequestInput {
    /**
     * @minimum `1`
     */
    messages?: (ChatCompletionRequestSystemMessage | ChatCompletionRequestUserMessage | FineTuneChatCompletionRequestAssistantMessage | ChatCompletionRequestToolMessage | ChatCompletionRequestFunctionMessage)[];
    /**
     * A list of tools the model may generate JSON inputs for.
     */
    tools?: ChatCompletionTool[];
    parallel_tool_calls?: ParallelToolCalls;
    /**
     * A list of functions the model may generate JSON inputs for.
     * @maximum `128`
     * @minimum `1`
     * @deprecated
     */
    functions?: ChatCompletionFunctions[];
}

/**
 * The per-line training example of a fine-tuning input file for completions models
 */
export interface FinetuneCompletionRequestInput {
    /**
     * The input prompt for this training example.
     */
    prompt?: string;
    /**
     * The desired completion for this training example.
     */
    completion?: string;
}

/**
 * Usage statistics for the completion request.
 */
export interface CompletionUsage {
    /**
     * Number of tokens in the generated completion.
     */
    completion_tokens: number;
    /**
     * Number of tokens in the prompt.
     */
    prompt_tokens: number;
    /**
     * Total number of tokens used in the request (prompt + completion).
     */
    total_tokens: number;
}

/**
 * Usage statistics related to the run. This value will be `null` if the run is not in a terminal state (i.e. `in_progress`, `queued`, etc.).
 */
export type RunCompletionUsage = {
    /**
     * Number of completion tokens used over the course of the run.
     */
    completion_tokens: number;
    /**
     * Number of prompt tokens used over the course of the run.
     */
    prompt_tokens: number;
    /**
     * Total number of tokens used (prompt + completion).
     */
    total_tokens: number;
} | null;

/**
 * Usage statistics related to the run step. This value will be `null` while the run step's status is `in_progress`.
 */
export type RunStepCompletionUsage = {
    /**
     * Number of completion tokens used over the course of the run step.
     */
    completion_tokens: number;
    /**
     * Number of prompt tokens used over the course of the run step.
     */
    prompt_tokens: number;
    /**
     * Total number of tokens used (prompt + completion).
     */
    total_tokens: number;
} | null;

/**
 * Specifies the format that the model must output. Compatible with [GPT-4o](/docs/models/gpt-4o), [GPT-4 Turbo](/docs/models/gpt-4-turbo-and-gpt-4), and all GPT-3.5 Turbo models since `gpt-3.5-turbo-1106`.
 *
 * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which guarantees the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](/docs/guides/structured-outputs).
 *
 * Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
 *
 * **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
 *
 */
export type AssistantsApiResponseFormatOption = "auto" | ResponseFormatText | ResponseFormatJsonObject | ResponseFormatJsonSchema;

/**
 * Represents an `assistant` that can call the model and use tools.
 */
export interface AssistantObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `assistant`.
     */
    object: "assistant";
    /**
     * The Unix timestamp (in seconds) for when the assistant was created.
     */
    created_at: number;
    /**
     * The name of the assistant. The maximum length is 256 characters.
     *
     */
    name: string | null;
    /**
     * The description of the assistant. The maximum length is 512 characters.
     *
     */
    description: string | null;
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     *
     */
    model: string;
    /**
     * The system instructions that the assistant uses. The maximum length is 256,000 characters.
     *
     */
    instructions: string | null;
    /**
     * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`.
     *
     * @maximum `128`
     * @defaultValue ``
     */
    tools: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[];
    /**
     * A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter`` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: {
            /**
             * The ID of the [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant.
             *
             * @maximum `1`
             */
            vector_store_ids?: string[];
        };
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    response_format?: AssistantsApiResponseFormatOption;
}

export interface CreateAssistantRequest {
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     *
     */
    model: string | ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613") | (string & ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613"));
    /**
     * The name of the assistant. The maximum length is 256 characters.
     *
     */
    name?: string | null;
    /**
     * The description of the assistant. The maximum length is 512 characters.
     *
     */
    description?: string | null;
    /**
     * The system instructions that the assistant uses. The maximum length is 256,000 characters.
     *
     */
    instructions?: string | null;
    /**
     * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`.
     *
     * @maximum `128`
     * @defaultValue ``
     */
    tools?: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[];
    /**
     * A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: unknown | unknown;
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    response_format?: AssistantsApiResponseFormatOption;
}

export interface ModifyAssistantRequest {
    /**
     * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.
     *
     */
    model?: string;
    /**
     * The name of the assistant. The maximum length is 256 characters.
     *
     */
    name?: string | null;
    /**
     * The description of the assistant. The maximum length is 512 characters.
     *
     */
    description?: string | null;
    /**
     * The system instructions that the assistant uses. The maximum length is 256,000 characters.
     *
     */
    instructions?: string | null;
    /**
     * A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types `code_interpreter`, `file_search`, or `function`.
     *
     * @maximum `128`
     * @defaultValue ``
     */
    tools?: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[];
    /**
     * A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * Overrides the list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: {
            /**
             * Overrides the [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant.
             *
             * @maximum `1`
             */
            vector_store_ids?: string[];
        };
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    response_format?: AssistantsApiResponseFormatOption;
}

export interface DeleteAssistantResponse {
    id: string;
    deleted: boolean;
    object: "assistant.deleted";
}

export interface ListAssistantsResponse {
    object: string;
    data: AssistantObject[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface AssistantToolsCode {
    /**
     * The type of tool being defined: `code_interpreter`
     */
    type: "code_interpreter";
}

export interface AssistantToolsFileSearch {
    /**
     * The type of tool being defined: `file_search`
     */
    type: "file_search";
    /**
     * Overrides for the file search tool.
     */
    file_search?: {
        /**
         * The maximum number of results the file search tool should output. The default is 20 for `gpt-4*` models and 5 for `gpt-3.5-turbo`. This number should be between 1 and 50 inclusive.
         *
         * Note that the file search tool may output fewer than `max_num_results` results. See the [file search tool documentation](/docs/assistants/tools/file-search/number-of-chunks-returned) for more information.
         *
         * @maximum `50`
         * @minimum `1`
         */
        max_num_results?: number;
    };
}

export interface AssistantToolsFileSearchTypeOnly {
    /**
     * The type of tool being defined: `file_search`
     */
    type: "file_search";
}

export interface AssistantToolsFunction {
    /**
     * The type of tool being defined: `function`
     */
    type: "function";
    function: FunctionObject;
}

/**
 * Controls for how a thread will be truncated prior to the run. Use this to control the intial context window of the run.
 */
export interface TruncationObject {
    /**
     * The truncation strategy to use for the thread. The default is `auto`. If set to `last_messages`, the thread will be truncated to the n most recent messages in the thread. When set to `auto`, messages in the middle of the thread will be dropped to fit the context length of the model, `max_prompt_tokens`.
     */
    type: "auto" | "last_messages";
    /**
     * The number of most recent messages from the thread when constructing the context for the run.
     * @minimum `1`
     */
    last_messages?: number | null;
}

/**
 * Controls which (if any) tool is called by the model.
 * `none` means the model will not call any tools and instead generates a message.
 * `auto` is the default value and means the model can pick between generating a message or calling one or more tools.
 * `required` means the model must call one or more tools before responding to the user.
 * Specifying a particular tool like `{"type": "file_search"}` or `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.
 *
 */
export type AssistantsApiToolChoiceOption = ("none" | "auto" | "required") | AssistantsNamedToolChoice;

/**
 * Specifies a tool the model should use. Use to force the model to call a specific tool.
 */
export interface AssistantsNamedToolChoice {
    /**
     * The type of the tool. If type is `function`, the function name must be set
     */
    type: "function" | "code_interpreter" | "file_search";
    function?: {
        /**
         * The name of the function to call.
         */
        name: string;
    };
}

/**
 * Represents an execution run on a [thread](/docs/api-reference/threads).
 */
export interface RunObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread.run`.
     */
    object: "thread.run";
    /**
     * The Unix timestamp (in seconds) for when the run was created.
     */
    created_at: number;
    /**
     * The ID of the [thread](/docs/api-reference/threads) that was executed on as a part of this run.
     */
    thread_id: string;
    /**
     * The ID of the [assistant](/docs/api-reference/assistants) used for execution of this run.
     */
    assistant_id: string;
    /**
     * The status of the run, which can be either `queued`, `in_progress`, `requires_action`, `cancelling`, `cancelled`, `failed`, `completed`, `incomplete`, or `expired`.
     */
    status: "queued" | "in_progress" | "requires_action" | "cancelling" | "cancelled" | "failed" | "completed" | "incomplete" | "expired";
    /**
     * Details on the action required to continue the run. Will be `null` if no action is required.
     */
    required_action: {
        /**
         * For now, this is always `submit_tool_outputs`.
         */
        type: "submit_tool_outputs";
        /**
         * Details on the tool outputs needed for this run to continue.
         */
        submit_tool_outputs: {
            /**
             * A list of the relevant tool calls.
             */
            tool_calls: RunToolCallObject[];
        };
    } | null;
    /**
     * The last error associated with this run. Will be `null` if there are no errors.
     */
    last_error: {
        /**
         * One of `server_error`, `rate_limit_exceeded`, or `invalid_prompt`.
         */
        code: "server_error" | "rate_limit_exceeded" | "invalid_prompt";
        /**
         * A human-readable description of the error.
         */
        message: string;
    } | null;
    /**
     * The Unix timestamp (in seconds) for when the run will expire.
     */
    expires_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run was started.
     */
    started_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run was cancelled.
     */
    cancelled_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run failed.
     */
    failed_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run was completed.
     */
    completed_at: number | null;
    /**
     * Details on why the run is incomplete. Will be `null` if the run is not incomplete.
     */
    incomplete_details: {
        /**
         * The reason why the run is incomplete. This will point to which specific token limit was reached over the course of the run.
         */
        reason?: "max_completion_tokens" | "max_prompt_tokens";
    } | null;
    /**
     * The model that the [assistant](/docs/api-reference/assistants) used for this run.
     */
    model: string;
    /**
     * The instructions that the [assistant](/docs/api-reference/assistants) used for this run.
     */
    instructions: string;
    /**
     * The list of tools that the [assistant](/docs/api-reference/assistants) used for this run.
     * @maximum `20`
     * @defaultValue ``
     */
    tools: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[];
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
    usage: RunCompletionUsage;
    /**
     * The sampling temperature used for this run. If not set, defaults to 1.
     */
    temperature?: number | null;
    /**
     * The nucleus sampling value used for this run. If not set, defaults to 1.
     */
    top_p?: number | null;
    /**
     * The maximum number of prompt tokens specified to have been used over the course of the run.
     *
     * @minimum `256`
     */
    max_prompt_tokens: number | null;
    /**
     * The maximum number of completion tokens specified to have been used over the course of the run.
     *
     * @minimum `256`
     */
    max_completion_tokens: number | null;
    truncation_strategy: TruncationObject;
    tool_choice: AssistantsApiToolChoiceOption;
    parallel_tool_calls: ParallelToolCalls;
    response_format: AssistantsApiResponseFormatOption;
}

export interface CreateRunRequest {
    /**
     * The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run.
     */
    assistant_id: string;
    /**
     * The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.
     */
    model?: string | ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613") | (string & ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613"));
    /**
     * Overrides the [instructions](/docs/api-reference/assistants/createAssistant) of the assistant. This is useful for modifying the behavior on a per-run basis.
     */
    instructions?: string | null;
    /**
     * Appends additional instructions at the end of the instructions for the run. This is useful for modifying the behavior on a per-run basis without overriding other instructions.
     */
    additional_instructions?: string | null;
    /**
     * Adds additional messages to the thread before creating the run.
     */
    additional_messages?: CreateMessageRequest[] | null;
    /**
     * Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.
     * @maximum `20`
     */
    tools?: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[] | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    /**
     * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message.
     *
     */
    stream?: boolean | null;
    /**
     * The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info.
     *
     * @minimum `256`
     */
    max_prompt_tokens?: number | null;
    /**
     * The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info.
     *
     * @minimum `256`
     */
    max_completion_tokens?: number | null;
    truncation_strategy?: TruncationObject;
    tool_choice?: AssistantsApiToolChoiceOption;
    parallel_tool_calls?: ParallelToolCalls;
    response_format?: AssistantsApiResponseFormatOption;
}

export interface ListRunsResponse {
    object: string;
    data: RunObject[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface ModifyRunRequest {
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface SubmitToolOutputsRunRequest {
    /**
     * A list of tools for which the outputs are being submitted.
     */
    tool_outputs: {
        /**
         * The ID of the tool call in the `required_action` object within the run object the output is being submitted for.
         */
        tool_call_id?: string;
        /**
         * The output of the tool call to be submitted to continue the run.
         */
        output?: string;
    }[];
    /**
     * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message.
     *
     */
    stream?: boolean | null;
}

/**
 * Tool call objects
 */
export interface RunToolCallObject {
    /**
     * The ID of the tool call. This ID must be referenced when you submit the tool outputs in using the [Submit tool outputs to run](/docs/api-reference/runs/submitToolOutputs) endpoint.
     */
    id: string;
    /**
     * The type of tool call the output is required for. For now, this is always `function`.
     */
    type: "function";
    /**
     * The function definition.
     */
    function: {
        /**
         * The name of the function.
         */
        name: string;
        /**
         * The arguments that the model expects you to pass to the function.
         */
        arguments: string;
    };
}

export interface CreateThreadAndRunRequest {
    /**
     * The ID of the [assistant](/docs/api-reference/assistants) to use to execute this run.
     */
    assistant_id: string;
    thread?: CreateThreadRequest;
    /**
     * The ID of the [Model](/docs/api-reference/models) to be used to execute this run. If a value is provided here, it will override the model associated with the assistant. If not, the model associated with the assistant will be used.
     */
    model?: string | ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613") | (string & ("gpt-4o" | "gpt-4o-2024-08-06" | "gpt-4o-2024-05-13" | "gpt-4o-2024-08-06" | "gpt-4o-mini" | "gpt-4o-mini-2024-07-18" | "gpt-4-turbo" | "gpt-4-turbo-2024-04-09" | "gpt-4-0125-preview" | "gpt-4-turbo-preview" | "gpt-4-1106-preview" | "gpt-4-vision-preview" | "gpt-4" | "gpt-4-0314" | "gpt-4-0613" | "gpt-4-32k" | "gpt-4-32k-0314" | "gpt-4-32k-0613" | "gpt-3.5-turbo" | "gpt-3.5-turbo-16k" | "gpt-3.5-turbo-0613" | "gpt-3.5-turbo-1106" | "gpt-3.5-turbo-0125" | "gpt-3.5-turbo-16k-0613"));
    /**
     * Override the default system message of the assistant. This is useful for modifying the behavior on a per-run basis.
     */
    instructions?: string | null;
    /**
     * Override the tools the assistant can use for this run. This is useful for modifying the behavior on a per-run basis.
     * @maximum `20`
     */
    tools?: (AssistantToolsCode | AssistantToolsFileSearch | AssistantToolsFunction)[] | null;
    /**
     * A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: {
            /**
             * The ID of the [vector store](/docs/api-reference/vector-stores/object) attached to this assistant. There can be a maximum of 1 vector store attached to the assistant.
             *
             * @maximum `1`
             */
            vector_store_ids?: string[];
        };
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
    /**
     * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
     *
     * @maximum `2`
     * @minimum `0`
     * @defaultValue `1`
     */
    temperature?: number | null;
    /**
     * An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.
     *
     * We generally recommend altering this or temperature but not both.
     *
     * @maximum `1`
     * @minimum `0`
     * @defaultValue `1`
     */
    top_p?: number | null;
    /**
     * If `true`, returns a stream of events that happen during the Run as server-sent events, terminating when the Run enters a terminal state with a `data: [DONE]` message.
     *
     */
    stream?: boolean | null;
    /**
     * The maximum number of prompt tokens that may be used over the course of the run. The run will make a best effort to use only the number of prompt tokens specified, across multiple turns of the run. If the run exceeds the number of prompt tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info.
     *
     * @minimum `256`
     */
    max_prompt_tokens?: number | null;
    /**
     * The maximum number of completion tokens that may be used over the course of the run. The run will make a best effort to use only the number of completion tokens specified, across multiple turns of the run. If the run exceeds the number of completion tokens specified, the run will end with status `incomplete`. See `incomplete_details` for more info.
     *
     * @minimum `256`
     */
    max_completion_tokens?: number | null;
    truncation_strategy?: TruncationObject;
    tool_choice?: AssistantsApiToolChoiceOption;
    parallel_tool_calls?: ParallelToolCalls;
    response_format?: AssistantsApiResponseFormatOption;
}

/**
 * Represents a thread that contains [messages](/docs/api-reference/messages).
 */
export interface ThreadObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread`.
     */
    object: "thread";
    /**
     * The Unix timestamp (in seconds) for when the thread was created.
     */
    created_at: number;
    /**
     * A set of resources that are made available to the assistant's tools in this thread. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: {
            /**
             * The [vector store](/docs/api-reference/vector-stores/object) attached to this thread. There can be a maximum of 1 vector store attached to the thread.
             *
             * @maximum `1`
             */
            vector_store_ids?: string[];
        };
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
}

export interface CreateThreadRequest {
    /**
     * A list of [messages](/docs/api-reference/messages) to start the thread with.
     */
    messages?: CreateMessageRequest[];
    /**
     * A set of resources that are made available to the assistant's tools in this thread. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: unknown | unknown;
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface ModifyThreadRequest {
    /**
     * A set of resources that are made available to the assistant's tools in this thread. The resources are specific to the type of tool. For example, the `code_interpreter` tool requires a list of file IDs, while the `file_search` tool requires a list of vector store IDs.
     *
     */
    tool_resources?: {
        code_interpreter?: {
            /**
             * A list of [file](/docs/api-reference/files) IDs made available to the `code_interpreter` tool. There can be a maximum of 20 files associated with the tool.
             *
             * @maximum `20`
             * @defaultValue ``
             */
            file_ids?: string[];
        };
        file_search?: {
            /**
             * The [vector store](/docs/api-reference/vector-stores/object) attached to this thread. There can be a maximum of 1 vector store attached to the thread.
             *
             * @maximum `1`
             */
            vector_store_ids?: string[];
        };
    } | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface DeleteThreadResponse {
    id: string;
    deleted: boolean;
    object: "thread.deleted";
}

export type ListThreadsResponse = unknown;

/**
 * Represents a message within a [thread](/docs/api-reference/threads).
 */
export interface MessageObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread.message`.
     */
    object: "thread.message";
    /**
     * The Unix timestamp (in seconds) for when the message was created.
     */
    created_at: number;
    /**
     * The [thread](/docs/api-reference/threads) ID that this message belongs to.
     */
    thread_id: string;
    /**
     * The status of the message, which can be either `in_progress`, `incomplete`, or `completed`.
     */
    status: "in_progress" | "incomplete" | "completed";
    /**
     * On an incomplete message, details about why the message is incomplete.
     */
    incomplete_details: {
        /**
         * The reason the message is incomplete.
         */
        reason: "content_filter" | "max_tokens" | "run_cancelled" | "run_expired" | "run_failed";
    } | null;
    /**
     * The Unix timestamp (in seconds) for when the message was completed.
     */
    completed_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the message was marked as incomplete.
     */
    incomplete_at: number | null;
    /**
     * The entity that produced the message. One of `user` or `assistant`.
     */
    role: "user" | "assistant";
    /**
     * The content of the message in array of text and/or images.
     */
    content: (MessageContentImageFileObject | MessageContentImageUrlObject | MessageContentTextObject | MessageContentRefusalObject)[];
    /**
     * If applicable, the ID of the [assistant](/docs/api-reference/assistants) that authored this message.
     */
    assistant_id: string | null;
    /**
     * The ID of the [run](/docs/api-reference/runs) associated with the creation of this message. Value is `null` when messages are created manually using the create message or create thread endpoints.
     */
    run_id: string | null;
    /**
     * A list of files attached to the message, and the tools they were added to.
     */
    attachments: {
        /**
         * The ID of the file to attach to the message.
         */
        file_id?: string;
        /**
         * The tools to add this file to.
         */
        tools?: (AssistantToolsCode | AssistantToolsFileSearchTypeOnly)[];
    }[] | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
}

/**
 * Represents a message delta i.e. any changed fields on a message during streaming.
 *
 */
export interface MessageDeltaObject {
    /**
     * The identifier of the message, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread.message.delta`.
     */
    object: "thread.message.delta";
    /**
     * The delta containing the fields that have changed on the Message.
     */
    delta: {
        /**
         * The entity that produced the message. One of `user` or `assistant`.
         */
        role?: "user" | "assistant";
        /**
         * The content of the message in array of text and/or images.
         */
        content?: (MessageDeltaContentImageFileObject | MessageDeltaContentTextObject | MessageDeltaContentRefusalObject | MessageDeltaContentImageUrlObject)[];
    };
}

export interface CreateMessageRequest {
    /**
     * The role of the entity that is creating the message. Allowed values include:
     * - `user`: Indicates the message is sent by an actual user and should be used in most cases to represent user-generated messages.
     * - `assistant`: Indicates the message is generated by the assistant. Use this value to insert messages from the assistant into the conversation.
     *
     */
    role: "user" | "assistant";
    content: string | (MessageContentImageFileObject | MessageContentImageUrlObject | MessageRequestContentTextObject)[];
    /**
     * A list of files attached to the message, and the tools they should be added to.
     */
    attachments?: {
        /**
         * The ID of the file to attach to the message.
         */
        file_id?: string;
        /**
         * The tools to add this file to.
         */
        tools?: (AssistantToolsCode | AssistantToolsFileSearchTypeOnly)[];
    }[] | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface ModifyMessageRequest {
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface DeleteMessageResponse {
    id: string;
    deleted: boolean;
    object: "thread.message.deleted";
}

export type ListMessagesResponse = unknown;

/**
 * References an image [File](/docs/api-reference/files) in the content of a message.
 */
export interface MessageContentImageFileObject {
    /**
     * Always `image_file`.
     */
    type: "image_file";
    image_file: {
        /**
         * The [File](/docs/api-reference/files) ID of the image in the message content. Set `purpose="vision"` when uploading the File if you need to later display the file content.
         */
        file_id: string;
        /**
         * Specifies the detail level of the image if specified by the user. `low` uses fewer tokens, you can opt in to high resolution using `high`.
         * @defaultValue `auto`
         */
        detail?: "auto" | "low" | "high";
    };
}

/**
 * References an image [File](/docs/api-reference/files) in the content of a message.
 */
export interface MessageDeltaContentImageFileObject {
    /**
     * The index of the content part in the message.
     */
    index: number;
    /**
     * Always `image_file`.
     */
    type: "image_file";
    image_file?: {
        /**
         * The [File](/docs/api-reference/files) ID of the image in the message content. Set `purpose="vision"` when uploading the File if you need to later display the file content.
         */
        file_id?: string;
        /**
         * Specifies the detail level of the image if specified by the user. `low` uses fewer tokens, you can opt in to high resolution using `high`.
         * @defaultValue `auto`
         */
        detail?: "auto" | "low" | "high";
    };
}

/**
 * References an image URL in the content of a message.
 */
export interface MessageContentImageUrlObject {
    /**
     * The type of the content part.
     */
    type: "image_url";
    image_url: {
        /**
         * The external URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp.
         * @format `uri`
         */
        url: string;
        /**
         * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in to high resolution using `high`. Default value is `auto`
         * @defaultValue `auto`
         */
        detail?: "auto" | "low" | "high";
    };
}

/**
 * References an image URL in the content of a message.
 */
export interface MessageDeltaContentImageUrlObject {
    /**
     * The index of the content part in the message.
     */
    index: number;
    /**
     * Always `image_url`.
     */
    type: "image_url";
    image_url?: {
        /**
         * The URL of the image, must be a supported image types: jpeg, jpg, png, gif, webp.
         */
        url?: string;
        /**
         * Specifies the detail level of the image. `low` uses fewer tokens, you can opt in to high resolution using `high`.
         * @defaultValue `auto`
         */
        detail?: "auto" | "low" | "high";
    };
}

/**
 * The text content that is part of a message.
 */
export interface MessageContentTextObject {
    /**
     * Always `text`.
     */
    type: "text";
    text: {
        /**
         * The data that makes up the text.
         */
        value: string;
        annotations: (MessageContentTextAnnotationsFileCitationObject | MessageContentTextAnnotationsFilePathObject)[];
    };
}

/**
 * The refusal content generated by the assistant.
 */
export interface MessageContentRefusalObject {
    /**
     * Always `refusal`.
     */
    type: "refusal";
    refusal: string;
}

/**
 * The text content that is part of a message.
 */
export interface MessageRequestContentTextObject {
    /**
     * Always `text`.
     */
    type: "text";
    /**
     * Text content to be sent to the model
     */
    text: string;
}

/**
 * A citation within the message that points to a specific quote from a specific File associated with the assistant or the message. Generated when the assistant uses the "file_search" tool to search files.
 */
export interface MessageContentTextAnnotationsFileCitationObject {
    /**
     * Always `file_citation`.
     */
    type: "file_citation";
    /**
     * The text in the message content that needs to be replaced.
     */
    text: string;
    file_citation: {
        /**
         * The ID of the specific File the citation is from.
         */
        file_id: string;
    };
    /**
     * @minimum `0`
     */
    start_index: number;
    /**
     * @minimum `0`
     */
    end_index: number;
}

/**
 * A URL for the file that's generated when the assistant used the `code_interpreter` tool to generate a file.
 */
export interface MessageContentTextAnnotationsFilePathObject {
    /**
     * Always `file_path`.
     */
    type: "file_path";
    /**
     * The text in the message content that needs to be replaced.
     */
    text: string;
    file_path: {
        /**
         * The ID of the file that was generated.
         */
        file_id: string;
    };
    /**
     * @minimum `0`
     */
    start_index: number;
    /**
     * @minimum `0`
     */
    end_index: number;
}

/**
 * The text content that is part of a message.
 */
export interface MessageDeltaContentTextObject {
    /**
     * The index of the content part in the message.
     */
    index: number;
    /**
     * Always `text`.
     */
    type: "text";
    text?: {
        /**
         * The data that makes up the text.
         */
        value?: string;
        annotations?: (MessageDeltaContentTextAnnotationsFileCitationObject | MessageDeltaContentTextAnnotationsFilePathObject)[];
    };
}

/**
 * The refusal content that is part of a message.
 */
export interface MessageDeltaContentRefusalObject {
    /**
     * The index of the refusal part in the message.
     */
    index: number;
    /**
     * Always `refusal`.
     */
    type: "refusal";
    refusal?: string;
}

/**
 * A citation within the message that points to a specific quote from a specific File associated with the assistant or the message. Generated when the assistant uses the "file_search" tool to search files.
 */
export interface MessageDeltaContentTextAnnotationsFileCitationObject {
    /**
     * The index of the annotation in the text content part.
     */
    index: number;
    /**
     * Always `file_citation`.
     */
    type: "file_citation";
    /**
     * The text in the message content that needs to be replaced.
     */
    text?: string;
    file_citation?: {
        /**
         * The ID of the specific File the citation is from.
         */
        file_id?: string;
        /**
         * The specific quote in the file.
         */
        quote?: string;
    };
    /**
     * @minimum `0`
     */
    start_index?: number;
    /**
     * @minimum `0`
     */
    end_index?: number;
}

/**
 * A URL for the file that's generated when the assistant used the `code_interpreter` tool to generate a file.
 */
export interface MessageDeltaContentTextAnnotationsFilePathObject {
    /**
     * The index of the annotation in the text content part.
     */
    index: number;
    /**
     * Always `file_path`.
     */
    type: "file_path";
    /**
     * The text in the message content that needs to be replaced.
     */
    text?: string;
    file_path?: {
        /**
         * The ID of the file that was generated.
         */
        file_id?: string;
    };
    /**
     * @minimum `0`
     */
    start_index?: number;
    /**
     * @minimum `0`
     */
    end_index?: number;
}

/**
 * Represents a step in execution of a run.
 *
 */
export interface RunStepObject {
    /**
     * The identifier of the run step, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread.run.step`.
     */
    object: "thread.run.step";
    /**
     * The Unix timestamp (in seconds) for when the run step was created.
     */
    created_at: number;
    /**
     * The ID of the [assistant](/docs/api-reference/assistants) associated with the run step.
     */
    assistant_id: string;
    /**
     * The ID of the [thread](/docs/api-reference/threads) that was run.
     */
    thread_id: string;
    /**
     * The ID of the [run](/docs/api-reference/runs) that this run step is a part of.
     */
    run_id: string;
    /**
     * The type of run step, which can be either `message_creation` or `tool_calls`.
     */
    type: "message_creation" | "tool_calls";
    /**
     * The status of the run step, which can be either `in_progress`, `cancelled`, `failed`, `completed`, or `expired`.
     */
    status: "in_progress" | "cancelled" | "failed" | "completed" | "expired";
    /**
     * The details of the run step.
     */
    step_details: RunStepDetailsMessageCreationObject | RunStepDetailsToolCallsObject;
    /**
     * The last error associated with this run step. Will be `null` if there are no errors.
     */
    last_error: {
        /**
         * One of `server_error` or `rate_limit_exceeded`.
         */
        code: "server_error" | "rate_limit_exceeded";
        /**
         * A human-readable description of the error.
         */
        message: string;
    } | null;
    /**
     * The Unix timestamp (in seconds) for when the run step expired. A step is considered expired if the parent run is expired.
     */
    expired_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run step was cancelled.
     */
    cancelled_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run step failed.
     */
    failed_at: number | null;
    /**
     * The Unix timestamp (in seconds) for when the run step completed.
     */
    completed_at: number | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
    usage: RunStepCompletionUsage;
}

/**
 * Represents a run step delta i.e. any changed fields on a run step during streaming.
 *
 */
export interface RunStepDeltaObject {
    /**
     * The identifier of the run step, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `thread.run.step.delta`.
     */
    object: "thread.run.step.delta";
    /**
     * The delta containing the fields that have changed on the run step.
     */
    delta: {
        /**
         * The details of the run step.
         */
        step_details?: RunStepDeltaStepDetailsMessageCreationObject | RunStepDeltaStepDetailsToolCallsObject;
    };
}

export type ListRunStepsResponse = unknown;

/**
 * Details of the message creation by the run step.
 */
export interface RunStepDetailsMessageCreationObject {
    /**
     * Always `message_creation`.
     */
    type: "message_creation";
    message_creation: {
        /**
         * The ID of the message that was created by this run step.
         */
        message_id: string;
    };
}

/**
 * Details of the message creation by the run step.
 */
export interface RunStepDeltaStepDetailsMessageCreationObject {
    /**
     * Always `message_creation`.
     */
    type: "message_creation";
    message_creation?: {
        /**
         * The ID of the message that was created by this run step.
         */
        message_id?: string;
    };
}

/**
 * Details of the tool call.
 */
export interface RunStepDetailsToolCallsObject {
    /**
     * Always `tool_calls`.
     */
    type: "tool_calls";
    /**
     * An array of tool calls the run step was involved in. These can be associated with one of three types of tools: `code_interpreter`, `file_search`, or `function`.
     *
     */
    tool_calls: (RunStepDetailsToolCallsCodeObject | RunStepDetailsToolCallsFileSearchObject | RunStepDetailsToolCallsFunctionObject)[];
}

/**
 * Details of the tool call.
 */
export interface RunStepDeltaStepDetailsToolCallsObject {
    /**
     * Always `tool_calls`.
     */
    type: "tool_calls";
    /**
     * An array of tool calls the run step was involved in. These can be associated with one of three types of tools: `code_interpreter`, `file_search`, or `function`.
     *
     */
    tool_calls?: (RunStepDeltaStepDetailsToolCallsCodeObject | RunStepDeltaStepDetailsToolCallsFileSearchObject | RunStepDeltaStepDetailsToolCallsFunctionObject)[];
}

/**
 * Details of the Code Interpreter tool call the run step was involved in.
 */
export interface RunStepDetailsToolCallsCodeObject {
    /**
     * The ID of the tool call.
     */
    id: string;
    /**
     * The type of tool call. This is always going to be `code_interpreter` for this type of tool call.
     */
    type: "code_interpreter";
    /**
     * The Code Interpreter tool call definition.
     */
    code_interpreter: {
        /**
         * The input to the Code Interpreter tool call.
         */
        input: string;
        /**
         * The outputs from the Code Interpreter tool call. Code Interpreter can output one or more items, including text (`logs`) or images (`image`). Each of these are represented by a different object type.
         */
        outputs: (RunStepDetailsToolCallsCodeOutputLogsObject | RunStepDetailsToolCallsCodeOutputImageObject)[];
    };
}

/**
 * Details of the Code Interpreter tool call the run step was involved in.
 */
export interface RunStepDeltaStepDetailsToolCallsCodeObject {
    /**
     * The index of the tool call in the tool calls array.
     */
    index: number;
    /**
     * The ID of the tool call.
     */
    id?: string;
    /**
     * The type of tool call. This is always going to be `code_interpreter` for this type of tool call.
     */
    type: "code_interpreter";
    /**
     * The Code Interpreter tool call definition.
     */
    code_interpreter?: {
        /**
         * The input to the Code Interpreter tool call.
         */
        input?: string;
        /**
         * The outputs from the Code Interpreter tool call. Code Interpreter can output one or more items, including text (`logs`) or images (`image`). Each of these are represented by a different object type.
         */
        outputs?: (RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject | RunStepDeltaStepDetailsToolCallsCodeOutputImageObject)[];
    };
}

/**
 * Text output from the Code Interpreter tool call as part of a run step.
 */
export interface RunStepDetailsToolCallsCodeOutputLogsObject {
    /**
     * Always `logs`.
     */
    type: "logs";
    /**
     * The text output from the Code Interpreter tool call.
     */
    logs: string;
}

/**
 * Text output from the Code Interpreter tool call as part of a run step.
 */
export interface RunStepDeltaStepDetailsToolCallsCodeOutputLogsObject {
    /**
     * The index of the output in the outputs array.
     */
    index: number;
    /**
     * Always `logs`.
     */
    type: "logs";
    /**
     * The text output from the Code Interpreter tool call.
     */
    logs?: string;
}

export interface RunStepDetailsToolCallsCodeOutputImageObject {
    /**
     * Always `image`.
     */
    type: "image";
    image: {
        /**
         * The [file](/docs/api-reference/files) ID of the image.
         */
        file_id: string;
    };
}

export interface RunStepDeltaStepDetailsToolCallsCodeOutputImageObject {
    /**
     * The index of the output in the outputs array.
     */
    index: number;
    /**
     * Always `image`.
     */
    type: "image";
    image?: {
        /**
         * The [file](/docs/api-reference/files) ID of the image.
         */
        file_id?: string;
    };
}

export interface RunStepDetailsToolCallsFileSearchObject {
    /**
     * The ID of the tool call object.
     */
    id: string;
    /**
     * The type of tool call. This is always going to be `file_search` for this type of tool call.
     */
    type: "file_search";
    /**
     * For now, this is always going to be an empty object.
     */
    file_search: unknown;
}

export interface RunStepDeltaStepDetailsToolCallsFileSearchObject {
    /**
     * The index of the tool call in the tool calls array.
     */
    index: number;
    /**
     * The ID of the tool call object.
     */
    id?: string;
    /**
     * The type of tool call. This is always going to be `file_search` for this type of tool call.
     */
    type: "file_search";
    /**
     * For now, this is always going to be an empty object.
     */
    file_search: unknown;
}

export interface RunStepDetailsToolCallsFunctionObject {
    /**
     * The ID of the tool call object.
     */
    id: string;
    /**
     * The type of tool call. This is always going to be `function` for this type of tool call.
     */
    type: "function";
    /**
     * The definition of the function that was called.
     */
    function: {
        /**
         * The name of the function.
         */
        name: string;
        /**
         * The arguments passed to the function.
         */
        arguments: string;
        /**
         * The output of the function. This will be `null` if the outputs have not been [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
         */
        output: string | null;
    };
}

export interface RunStepDeltaStepDetailsToolCallsFunctionObject {
    /**
     * The index of the tool call in the tool calls array.
     */
    index: number;
    /**
     * The ID of the tool call object.
     */
    id?: string;
    /**
     * The type of tool call. This is always going to be `function` for this type of tool call.
     */
    type: "function";
    /**
     * The definition of the function that was called.
     */
    function?: {
        /**
         * The name of the function.
         */
        name?: string;
        /**
         * The arguments passed to the function.
         */
        arguments?: string;
        /**
         * The output of the function. This will be `null` if the outputs have not been [submitted](/docs/api-reference/runs/submitToolOutputs) yet.
         */
        output?: string | null;
    };
}

/**
 * The expiration policy for a vector store.
 */
export interface VectorStoreExpirationAfter {
    /**
     * Anchor timestamp after which the expiration policy applies. Supported anchors: `last_active_at`.
     */
    anchor: "last_active_at";
    /**
     * The number of days after the anchor time that the vector store will expire.
     * @maximum `365`
     * @minimum `1`
     */
    days: number;
}

/**
 * A vector store is a collection of processed files can be used by the `file_search` tool.
 */
export interface VectorStoreObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `vector_store`.
     */
    object: "vector_store";
    /**
     * The Unix timestamp (in seconds) for when the vector store was created.
     */
    created_at: number;
    /**
     * The name of the vector store.
     */
    name: string;
    /**
     * The total number of bytes used by the files in the vector store.
     */
    usage_bytes: number;
    file_counts: {
        /**
         * The number of files that are currently being processed.
         */
        in_progress: number;
        /**
         * The number of files that have been successfully processed.
         */
        completed: number;
        /**
         * The number of files that have failed to process.
         */
        failed: number;
        /**
         * The number of files that were cancelled.
         */
        cancelled: number;
        /**
         * The total number of files.
         */
        total: number;
    };
    /**
     * The status of the vector store, which can be either `expired`, `in_progress`, or `completed`. A status of `completed` indicates that the vector store is ready for use.
     */
    status: "expired" | "in_progress" | "completed";
    expires_after?: VectorStoreExpirationAfter;
    /**
     * The Unix timestamp (in seconds) for when the vector store will expire.
     */
    expires_at?: number | null;
    /**
     * The Unix timestamp (in seconds) for when the vector store was last active.
     */
    last_active_at: number | null;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata: unknown | null;
}

export interface CreateVectorStoreRequest {
    /**
     * A list of [File](/docs/api-reference/files) IDs that the vector store should use. Useful for tools like `file_search` that can access files.
     * @maximum `500`
     */
    file_ids?: string[];
    /**
     * The name of the vector store.
     */
    name?: string;
    expires_after?: VectorStoreExpirationAfter;
    /**
     * The chunking strategy used to chunk the file(s). If not set, will use the `auto` strategy. Only applicable if `file_ids` is non-empty.
     */
    chunking_strategy?: AutoChunkingStrategyRequestParam | StaticChunkingStrategyRequestParam;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export interface UpdateVectorStoreRequest {
    /**
     * The name of the vector store.
     */
    name?: string | null;
    expires_after?: VectorStoreExpirationAfter;
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

export type ListVectorStoresResponse = unknown;

export interface DeleteVectorStoreResponse {
    id: string;
    deleted: boolean;
    object: "vector_store.deleted";
}

/**
 * A list of files attached to a vector store.
 */
export interface VectorStoreFileObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `vector_store.file`.
     */
    object: "vector_store.file";
    /**
     * The total vector store usage in bytes. Note that this may be different from the original file size.
     */
    usage_bytes: number;
    /**
     * The Unix timestamp (in seconds) for when the vector store file was created.
     */
    created_at: number;
    /**
     * The ID of the [vector store](/docs/api-reference/vector-stores/object) that the [File](/docs/api-reference/files) is attached to.
     */
    vector_store_id: string;
    /**
     * The status of the vector store file, which can be either `in_progress`, `completed`, `cancelled`, or `failed`. The status `completed` indicates that the vector store file is ready for use.
     */
    status: "in_progress" | "completed" | "cancelled" | "failed";
    /**
     * The last error associated with this vector store file. Will be `null` if there are no errors.
     */
    last_error: {
        /**
         * One of `server_error` or `rate_limit_exceeded`.
         */
        code: "server_error" | "unsupported_file" | "invalid_file";
        /**
         * A human-readable description of the error.
         */
        message: string;
    } | null;
    /**
     * The strategy used to chunk the file.
     */
    chunking_strategy?: StaticChunkingStrategyResponseParam | OtherChunkingStrategyResponseParam;
}

/**
 * This is returned when the chunking strategy is unknown. Typically, this is because the file was indexed before the `chunking_strategy` concept was introduced in the API.
 */
export interface OtherChunkingStrategyResponseParam {
    /**
     * Always `other`.
     */
    type: "other";
}

export interface StaticChunkingStrategyResponseParam {
    /**
     * Always `static`.
     */
    type: "static";
    static: StaticChunkingStrategy;
}

export interface StaticChunkingStrategy {
    /**
     * The maximum number of tokens in each chunk. The default value is `800`. The minimum value is `100` and the maximum value is `4096`.
     * @maximum `4096`
     * @minimum `100`
     */
    max_chunk_size_tokens: number;
    /**
     * The number of tokens that overlap between chunks. The default value is `400`.
     *
     * Note that the overlap must not exceed half of `max_chunk_size_tokens`.
     *
     */
    chunk_overlap_tokens: number;
}

/**
 * The default strategy. This strategy currently uses a `max_chunk_size_tokens` of `800` and `chunk_overlap_tokens` of `400`.
 */
export interface AutoChunkingStrategyRequestParam {
    /**
     * Always `auto`.
     */
    type: "auto";
}

export interface StaticChunkingStrategyRequestParam {
    /**
     * Always `static`.
     */
    type: "static";
    static: StaticChunkingStrategy;
}

/**
 * The chunking strategy used to chunk the file(s). If not set, will use the `auto` strategy.
 */
export type ChunkingStrategyRequestParam = AutoChunkingStrategyRequestParam | StaticChunkingStrategyRequestParam;

export interface CreateVectorStoreFileRequest {
    /**
     * A [File](/docs/api-reference/files) ID that the vector store should use. Useful for tools like `file_search` that can access files.
     */
    file_id: string;
    chunking_strategy?: ChunkingStrategyRequestParam;
}

export type ListVectorStoreFilesResponse = unknown;

export interface DeleteVectorStoreFileResponse {
    id: string;
    deleted: boolean;
    object: "vector_store.file.deleted";
}

/**
 * A batch of files attached to a vector store.
 */
export interface VectorStoreFileBatchObject {
    /**
     * The identifier, which can be referenced in API endpoints.
     */
    id: string;
    /**
     * The object type, which is always `vector_store.file_batch`.
     */
    object: "vector_store.files_batch";
    /**
     * The Unix timestamp (in seconds) for when the vector store files batch was created.
     */
    created_at: number;
    /**
     * The ID of the [vector store](/docs/api-reference/vector-stores/object) that the [File](/docs/api-reference/files) is attached to.
     */
    vector_store_id: string;
    /**
     * The status of the vector store files batch, which can be either `in_progress`, `completed`, `cancelled` or `failed`.
     */
    status: "in_progress" | "completed" | "cancelled" | "failed";
    file_counts: {
        /**
         * The number of files that are currently being processed.
         */
        in_progress: number;
        /**
         * The number of files that have been processed.
         */
        completed: number;
        /**
         * The number of files that have failed to process.
         */
        failed: number;
        /**
         * The number of files that where cancelled.
         */
        cancelled: number;
        /**
         * The total number of files.
         */
        total: number;
    };
}

export interface CreateVectorStoreFileBatchRequest {
    /**
     * A list of [File](/docs/api-reference/files) IDs that the vector store should use. Useful for tools like `file_search` that can access files.
     * @maximum `500`
     * @minimum `1`
     */
    file_ids: string[];
    chunking_strategy?: ChunkingStrategyRequestParam;
}

/**
 * Represents an event emitted when streaming a Run.
 *
 * Each event in a server-sent events stream has an `event` and `data` property:
 *
 * ```
 * event: thread.created
 * data: {"id": "thread_123", "object": "thread", ...}
 * ```
 *
 * We emit events whenever a new object is created, transitions to a new state, or is being
 * streamed in parts (deltas). For example, we emit `thread.run.created` when a new run
 * is created, `thread.run.completed` when a run completes, and so on. When an Assistant chooses
 * to create a message during a run, we emit a `thread.message.created event`, a
 * `thread.message.in_progress` event, many `thread.message.delta` events, and finally a
 * `thread.message.completed` event.
 *
 * We may add additional events over time, so we recommend handling unknown events gracefully
 * in your code. See the [Assistants API quickstart](/docs/assistants/overview) to learn how to
 * integrate the Assistants API with streaming.
 *
 */
export type AssistantStreamEvent = ThreadStreamEvent | RunStreamEvent | RunStepStreamEvent | MessageStreamEvent | ErrorEvent | DoneEvent;

export interface ThreadStreamEvent {
    event: "thread.created";
    data: ThreadObject;
}

export type RunStreamEvent = {
    event: "thread.run.created";
    data: RunObject;
} | {
    event: "thread.run.queued";
    data: RunObject;
} | {
    event: "thread.run.in_progress";
    data: RunObject;
} | {
    event: "thread.run.requires_action";
    data: RunObject;
} | {
    event: "thread.run.completed";
    data: RunObject;
} | {
    event: "thread.run.incomplete";
    data: RunObject;
} | {
    event: "thread.run.failed";
    data: RunObject;
} | {
    event: "thread.run.cancelling";
    data: RunObject;
} | {
    event: "thread.run.cancelled";
    data: RunObject;
} | {
    event: "thread.run.expired";
    data: RunObject;
};

export type RunStepStreamEvent = {
    event: "thread.run.step.created";
    data: RunStepObject;
} | {
    event: "thread.run.step.in_progress";
    data: RunStepObject;
} | {
    event: "thread.run.step.delta";
    data: RunStepDeltaObject;
} | {
    event: "thread.run.step.completed";
    data: RunStepObject;
} | {
    event: "thread.run.step.failed";
    data: RunStepObject;
} | {
    event: "thread.run.step.cancelled";
    data: RunStepObject;
} | {
    event: "thread.run.step.expired";
    data: RunStepObject;
};

export type MessageStreamEvent = {
    event: "thread.message.created";
    data: MessageObject;
} | {
    event: "thread.message.in_progress";
    data: MessageObject;
} | {
    event: "thread.message.delta";
    data: MessageDeltaObject;
} | {
    event: "thread.message.completed";
    data: MessageObject;
} | {
    event: "thread.message.incomplete";
    data: MessageObject;
};

/**
 * Occurs when an [error](/docs/guides/error-codes/api-errors) occurs. This can happen due to an internal server error or a timeout.
 */
export interface ErrorEvent {
    event: "error";
    data: Error;
}

/**
 * Occurs when a stream ends.
 */
export interface DoneEvent {
    event: "done";
    data: "[DONE]";
}

export interface Batch {
    id: string;
    /**
     * The object type, which is always `batch`.
     */
    object: "batch";
    /**
     * The OpenAI API endpoint used by the batch.
     */
    endpoint: string;
    errors?: {
        /**
         * The object type, which is always `list`.
         */
        object?: string;
        data?: {
            /**
             * An error code identifying the error type.
             */
            code?: string;
            /**
             * A human-readable message providing more details about the error.
             */
            message?: string;
            /**
             * The name of the parameter that caused the error, if applicable.
             */
            param?: string | null;
            /**
             * The line number of the input file where the error occurred, if applicable.
             */
            line?: number | null;
        }[];
    };
    /**
     * The ID of the input file for the batch.
     */
    input_file_id: string;
    /**
     * The time frame within which the batch should be processed.
     */
    completion_window: string;
    /**
     * The current status of the batch.
     */
    status: "validating" | "failed" | "in_progress" | "finalizing" | "completed" | "expired" | "cancelling" | "cancelled";
    /**
     * The ID of the file containing the outputs of successfully executed requests.
     */
    output_file_id?: string;
    /**
     * The ID of the file containing the outputs of requests with errors.
     */
    error_file_id?: string;
    /**
     * The Unix timestamp (in seconds) for when the batch was created.
     */
    created_at: number;
    /**
     * The Unix timestamp (in seconds) for when the batch started processing.
     */
    in_progress_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch will expire.
     */
    expires_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch started finalizing.
     */
    finalizing_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch was completed.
     */
    completed_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch failed.
     */
    failed_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch expired.
     */
    expired_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch started cancelling.
     */
    cancelling_at?: number;
    /**
     * The Unix timestamp (in seconds) for when the batch was cancelled.
     */
    cancelled_at?: number;
    /**
     * The request counts for different statuses within the batch.
     */
    request_counts?: {
        /**
         * Total number of requests in the batch.
         */
        total: number;
        /**
         * Number of requests that have been completed successfully.
         */
        completed: number;
        /**
         * Number of requests that have failed.
         */
        failed: number;
    };
    /**
     * Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.
     *
     */
    metadata?: unknown | null;
}

/**
 * The per-line object of the batch input file
 */
export interface BatchRequestInput {
    /**
     * A developer-provided per-request id that will be used to match outputs to inputs. Must be unique for each request in a batch.
     */
    custom_id?: string;
    /**
     * The HTTP method to be used for the request. Currently only `POST` is supported.
     */
    method?: "POST";
    /**
     * The OpenAI API relative URL to be used for the request. Currently `/v1/chat/completions`, `/v1/embeddings`, and `/v1/completions` are supported.
     */
    url?: string;
}

/**
 * The per-line object of the batch output and error files
 */
export interface BatchRequestOutput {
    id?: string;
    /**
     * A developer-provided per-request id that will be used to match outputs to inputs.
     */
    custom_id?: string;
    response?: {
        /**
         * The HTTP status code of the response
         */
        status_code?: number;
        /**
         * An unique identifier for the OpenAI API request. Please include this request ID when contacting support.
         */
        request_id?: string;
        /**
         * The JSON body of the response
         */
        body?: unknown;
    } | null;
    /**
     * For requests that failed with a non-HTTP error, this will contain more information on the cause of the failure.
     */
    error?: {
        /**
         * A machine-readable error code.
         */
        code?: string;
        /**
         * A human-readable error message.
         */
        message?: string;
    } | null;
}

export interface ListBatchesResponse {
    data: Batch[];
    first_id?: string;
    last_id?: string;
    has_more: boolean;
    object: "list";
}

/**
 * The service account that performed the audit logged action.
 */
export interface AuditLogActorServiceAccount {
    /**
     * The service account id.
     */
    id?: string;
}

/**
 * The user who performed the audit logged action.
 */
export interface AuditLogActorUser {
    /**
     * The user id.
     */
    id?: string;
    /**
     * The user email.
     */
    email?: string;
}

/**
 * The API Key used to perform the audit logged action.
 */
export interface AuditLogActorApiKey {
    /**
     * The tracking id of the API key.
     */
    id?: string;
    /**
     * The type of API key. Can be either `user` or `service_account`.
     */
    type?: "user" | "service_account";
    user?: AuditLogActorUser;
    service_account?: AuditLogActorServiceAccount;
}

/**
 * The session in which the audit logged action was performed.
 */
export interface AuditLogActorSession {
    user?: AuditLogActorUser;
    /**
     * The IP address from which the action was performed.
     */
    ip_address?: string;
}

/**
 * The actor who performed the audit logged action.
 */
export interface AuditLogActor {
    /**
     * The type of actor. Is either `session` or `api_key`.
     */
    type?: "session" | "api_key";
    session?: AuditLogActorSession;
    api_key?: AuditLogActorApiKey;
}

/**
 * The event type.
 */
export type AuditLogEventType = "api_key.created" | "api_key.updated" | "api_key.deleted" | "invite.sent" | "invite.accepted" | "invite.deleted" | "login.succeeded" | "login.failed" | "logout.succeeded" | "logout.failed" | "organization.updated" | "project.created" | "project.updated" | "project.archived" | "service_account.created" | "service_account.updated" | "service_account.deleted" | "user.added" | "user.updated" | "user.deleted";

/**
 * A log of a user action or configuration change within this organization.
 */
export interface AuditLog {
    /**
     * The ID of this log.
     */
    id: string;
    type: AuditLogEventType;
    /**
     * The Unix timestamp (in seconds) of the event.
     */
    effective_at: number;
    /**
     * The project that the action was scoped to. Absent for actions not scoped to projects.
     */
    project?: {
        /**
         * The project ID.
         */
        id?: string;
        /**
         * The project title.
         */
        name?: string;
    };
    actor: AuditLogActor;
    /**
     * The details for events with this `type`.
     */
    "api_key.created"?: {
        /**
         * The tracking ID of the API key.
         */
        id?: string;
        /**
         * The payload used to create the API key.
         */
        data?: {
            /**
             * A list of scopes allowed for the API key, e.g. `["api.model.request"]`
             */
            scopes?: string[];
        };
    };
    /**
     * The details for events with this `type`.
     */
    "api_key.updated"?: {
        /**
         * The tracking ID of the API key.
         */
        id?: string;
        /**
         * The payload used to update the API key.
         */
        changes_requested?: {
            /**
             * A list of scopes allowed for the API key, e.g. `["api.model.request"]`
             */
            scopes?: string[];
        };
    };
    /**
     * The details for events with this `type`.
     */
    "api_key.deleted"?: {
        /**
         * The tracking ID of the API key.
         */
        id?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "invite.sent"?: {
        /**
         * The ID of the invite.
         */
        id?: string;
        /**
         * The payload used to create the invite.
         */
        data?: {
            /**
             * The email invited to the organization.
             */
            email?: string;
            /**
             * The role the email was invited to be. Is either `owner` or `member`.
             */
            role?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "invite.accepted"?: {
        /**
         * The ID of the invite.
         */
        id?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "invite.deleted"?: {
        /**
         * The ID of the invite.
         */
        id?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "login.failed"?: {
        /**
         * The error code of the failure.
         */
        error_code?: string;
        /**
         * The error message of the failure.
         */
        error_message?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "logout.failed"?: {
        /**
         * The error code of the failure.
         */
        error_code?: string;
        /**
         * The error message of the failure.
         */
        error_message?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "organization.updated"?: {
        /**
         * The organization ID.
         */
        id?: string;
        /**
         * The payload used to update the organization settings.
         */
        changes_requested?: {
            /**
             * The organization title.
             */
            title?: string;
            /**
             * The organization description.
             */
            description?: string;
            /**
             * The organization name.
             */
            name?: string;
            settings?: {
                /**
                 * Visibility of the threads page which shows messages created with the Assistants API and Playground. One of `ANY_ROLE`, `OWNERS`, or `NONE`.
                 */
                threads_ui_visibility?: string;
                /**
                 * Visibility of the usage dashboard which shows activity and costs for your organization. One of `ANY_ROLE` or `OWNERS`.
                 */
                usage_dashboard_visibility?: string;
            };
        };
    };
    /**
     * The details for events with this `type`.
     */
    "project.created"?: {
        /**
         * The project ID.
         */
        id?: string;
        /**
         * The payload used to create the project.
         */
        data?: {
            /**
             * The project name.
             */
            name?: string;
            /**
             * The title of the project as seen on the dashboard.
             */
            title?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "project.updated"?: {
        /**
         * The project ID.
         */
        id?: string;
        /**
         * The payload used to update the project.
         */
        changes_requested?: {
            /**
             * The title of the project as seen on the dashboard.
             */
            title?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "project.archived"?: {
        /**
         * The project ID.
         */
        id?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "service_account.created"?: {
        /**
         * The service account ID.
         */
        id?: string;
        /**
         * The payload used to create the service account.
         */
        data?: {
            /**
             * The role of the service account. Is either `owner` or `member`.
             */
            role?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "service_account.updated"?: {
        /**
         * The service account ID.
         */
        id?: string;
        /**
         * The payload used to updated the service account.
         */
        changes_requested?: {
            /**
             * The role of the service account. Is either `owner` or `member`.
             */
            role?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "service_account.deleted"?: {
        /**
         * The service account ID.
         */
        id?: string;
    };
    /**
     * The details for events with this `type`.
     */
    "user.added"?: {
        /**
         * The user ID.
         */
        id?: string;
        /**
         * The payload used to add the user to the project.
         */
        data?: {
            /**
             * The role of the user. Is either `owner` or `member`.
             */
            role?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "user.updated"?: {
        /**
         * The project ID.
         */
        id?: string;
        /**
         * The payload used to update the user.
         */
        changes_requested?: {
            /**
             * The role of the user. Is either `owner` or `member`.
             */
            role?: string;
        };
    };
    /**
     * The details for events with this `type`.
     */
    "user.deleted"?: {
        /**
         * The user ID.
         */
        id?: string;
    };
}

export interface ListAuditLogsResponse {
    object: "list";
    data: AuditLog[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

/**
 * Represents an individual `invite` to the organization.
 */
export interface Invite {
    /**
     * The object type, which is always `organization.invite`
     */
    object: "organization.invite";
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    /**
     * The email address of the individual to whom the invite was sent
     */
    email: string;
    /**
     * `owner` or `reader`
     */
    role: "owner" | "reader";
    /**
     * `accepted`,`expired`, or `pending`
     */
    status: "accepted" | "expired" | "pending";
    /**
     * The Unix timestamp (in seconds) of when the invite was sent.
     */
    invited_at: number;
    /**
     * The Unix timestamp (in seconds) of when the invite expires.
     */
    expires_at: number;
    /**
     * The Unix timestamp (in seconds) of when the invite was accepted.
     */
    accepted_at?: number;
}

export interface InviteListResponse {
    /**
     * The object type, which is always `list`
     */
    object: "list";
    data: Invite[];
    /**
     * The first `invite_id` in the retrieved `list`
     */
    first_id?: string;
    /**
     * The last `invite_id` in the retrieved `list`
     */
    last_id?: string;
    /**
     * The `has_more` property is used for pagination to indicate there are additional results.
     */
    has_more?: boolean;
}

export interface InviteRequest {
    /**
     * Send an email to this address
     */
    email: string;
    /**
     * `owner` or `reader`
     */
    role: "reader" | "owner";
}

export interface InviteDeleteResponse {
    /**
     * The object type, which is always `organization.invite.deleted`
     */
    object: "organization.invite.deleted";
    id: string;
    deleted: boolean;
}

/**
 * Represents an individual `user` within an organization.
 */
export interface User {
    /**
     * The object type, which is always `organization.user`
     */
    object: "organization.user";
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    /**
     * The name of the user
     */
    name: string;
    /**
     * The email address of the user
     */
    email: string;
    /**
     * `owner` or `reader`
     */
    role: "owner" | "reader";
    /**
     * The Unix timestamp (in seconds) of when the user was added.
     */
    added_at: number;
}

export interface UserListResponse {
    object: "list";
    data: User[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface UserRoleUpdateRequest {
    /**
     * `owner` or `reader`
     */
    role: "owner" | "reader";
}

export interface UserDeleteResponse {
    object: "organization.user.deleted";
    id: string;
    deleted: boolean;
}

/**
 * Represents an individual project.
 */
export interface Project {
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    /**
     * The object type, which is always `organization.project`
     */
    object: "organization.project";
    /**
     * The name of the project. This appears in reporting.
     */
    name: string;
    /**
     * The Unix timestamp (in seconds) of when the project was created.
     */
    created_at: number;
    /**
     * The Unix timestamp (in seconds) of when the project was archived or `null`.
     */
    archived_at?: number | null;
    /**
     * `active` or `archived`
     */
    status: "active" | "archived";
}

export interface ProjectListResponse {
    object: "list";
    data: Project[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface ProjectCreateRequest {
    /**
     * The friendly name of the project, this name appears in reports.
     */
    name: string;
}

export interface ProjectUpdateRequest {
    /**
     * The updated name of the project, this name appears in reports.
     */
    name: string;
}

export interface DefaultProjectErrorResponse {
    code: number;
    message: string;
}

/**
 * Represents an individual user in a project.
 */
export interface ProjectUser {
    /**
     * The object type, which is always `organization.project.user`
     */
    object: "organization.project.user";
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    /**
     * The name of the user
     */
    name: string;
    /**
     * The email address of the user
     */
    email: string;
    /**
     * `owner` or `member`
     */
    role: "owner" | "member";
    /**
     * The Unix timestamp (in seconds) of when the project was added.
     */
    added_at: number;
}

export interface ProjectUserListResponse {
    object: string;
    data: ProjectUser[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface ProjectUserCreateRequest {
    /**
     * The ID of the user.
     */
    user_id: string;
    /**
     * `owner` or `member`
     */
    role: "owner" | "member";
}

export interface ProjectUserUpdateRequest {
    /**
     * `owner` or `member`
     */
    role: "owner" | "member";
}

export interface ProjectUserDeleteResponse {
    object: "organization.project.user.deleted";
    id: string;
    deleted: boolean;
}

/**
 * Represents an individual service account in a project.
 */
export interface ProjectServiceAccount {
    /**
     * The object type, which is always `organization.project.service_account`
     */
    object: "organization.project.service_account";
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    /**
     * The name of the service account
     */
    name: string;
    /**
     * `owner` or `member`
     */
    role: "owner" | "member";
    /**
     * The Unix timestamp (in seconds) of when the service account was created
     */
    created_at: number;
}

export interface ProjectServiceAccountListResponse {
    object: "list";
    data: ProjectServiceAccount[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface ProjectServiceAccountCreateRequest {
    /**
     * The name of the service account being created.
     */
    name: string;
}

export interface ProjectServiceAccountCreateResponse {
    object: "organization.project.service_account";
    id: string;
    name: string;
    /**
     * Service accounts can only have one role of type `member`
     */
    role: "member";
    created_at: number;
    api_key: ProjectServiceAccountApiKey;
}

export interface ProjectServiceAccountApiKey {
    /**
     * The object type, which is always `organization.project.service_account.api_key`
     */
    object: "organization.project.service_account.api_key";
    value: string;
    name: string;
    created_at: number;
    id: string;
}

export interface ProjectServiceAccountDeleteResponse {
    object: "organization.project.service_account.deleted";
    id: string;
    deleted: boolean;
}

/**
 * Represents an individual API key in a project.
 */
export interface ProjectApiKey {
    /**
     * The object type, which is always `organization.project.api_key`
     */
    object: "organization.project.api_key";
    /**
     * The redacted value of the API key
     */
    redacted_value: string;
    /**
     * The name of the API key
     */
    name: string;
    /**
     * The Unix timestamp (in seconds) of when the API key was created
     */
    created_at: number;
    /**
     * The identifier, which can be referenced in API endpoints
     */
    id: string;
    owner: {
        /**
         * `user` or `service_account`
         */
        type?: "user" | "service_account";
        user?: ProjectUser;
        service_account?: ProjectServiceAccount;
    };
}

export interface ProjectApiKeyListResponse {
    object: "list";
    data: ProjectApiKey[];
    first_id: string;
    last_id: string;
    has_more: boolean;
}

export interface ProjectApiKeyDeleteResponse {
    object: "organization.project.api_key.deleted";
    id: string;
    deleted: boolean;
}

